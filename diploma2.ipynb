{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "diploma2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3WiexbGSL-Ww"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import pandas\n",
        "import string\n",
        "import warnings\n",
        "import os\n",
        "import re\n",
        "from datetime import datetime\n",
        "from pprint import pprint\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93zdsgkyMIXn",
        "outputId": "a8f50ab9-c5f0-4e59-8a25-df4c5d2fd783"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13wdehfcMJaj",
        "outputId": "5aff153b-50a7-4648-f750-1a9a4714eb0d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.7)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.5.18.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install openpyxl==3.0.9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3z69sUNIMPA_",
        "outputId": "160e2295-7988-4463-d8de-13efba75601e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openpyxl==3.0.9\n",
            "  Downloading openpyxl-3.0.9-py2.py3-none-any.whl (242 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 25.2 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20 kB 31.7 MB/s eta 0:00:01\r\u001b[K     |████                            | 30 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 40 kB 30.0 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 51 kB 23.1 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 81 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 92 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 102 kB 26.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 112 kB 26.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 122 kB 26.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 133 kB 26.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143 kB 26.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 153 kB 26.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 163 kB 26.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 174 kB 26.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 184 kB 26.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 194 kB 26.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 204 kB 26.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 215 kB 26.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 225 kB 26.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 235 kB 26.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 242 kB 26.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl==3.0.9) (1.1.0)\n",
            "Installing collected packages: openpyxl\n",
            "  Attempting uninstall: openpyxl\n",
            "    Found existing installation: openpyxl 3.0.10\n",
            "    Uninstalling openpyxl-3.0.10:\n",
            "      Successfully uninstalled openpyxl-3.0.10\n",
            "Successfully installed openpyxl-3.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pandas.read_excel('drive/MyDrive/Exam2020_Task_7_Essays_mistakes_table.xlsx')"
      ],
      "metadata": {
        "id": "Byx_MQoVMQ2c"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = pandas.read_excel('drive/MyDrive/Exam2020_Task_7_Essays_tokenized_text_table.xlsx')"
      ],
      "metadata": {
        "id": "5n-s1Ug-MSYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = data.loc[data['mistake_type'] == 'punct']"
      ],
      "metadata": {
        "id": "l3oegKp6MTcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation.loc[(punctuation.correction == 'none'), 'correction'] = ''"
      ],
      "metadata": {
        "id": "Zhd7AIsxMUfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text(text_id, sentence_id):\n",
        "    text = sentences[sentences['text_id'] == text_id]\n",
        "    text = text.loc[text['sentence_id'].isin([sentence_id, sentence_id+1])]\n",
        "    \n",
        "    words = list(text.token)\n",
        "    ids = list(text.token_id)\n",
        "    d = {}\n",
        "\n",
        "    for el in range(len(words)):\n",
        "      if isinstance(words[el],datetime):\n",
        "        return [text_id, sentence_id]\n",
        "      else:\n",
        "        d[ids[el]] = str(words[el])\n",
        "\n",
        "    words = [str(i) for i in words]\n",
        "    text  = ' '.join(words)\n",
        "    for k in string.punctuation: \n",
        "      if k != '(':\n",
        "        text = text.replace(f' {k}',k)\n",
        "      elif k == '(':\n",
        "        text = text.replace(f'{k} ',k)\n",
        "\n",
        "    return text, d"
      ],
      "metadata": {
        "id": "9b8MsKv-MWpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_texts = []\n",
        "tokens_ids = []\n",
        "for el in range(len(punctuation.text_id)):\n",
        "  full_texts.append(get_text(list(punctuation.text_id)[el], list(punctuation.sentence_id)[el])[0])\n",
        "  tokens_ids.append(get_text(list(punctuation.text_id)[el], list(punctuation.sentence_id)[el])[1])"
      ],
      "metadata": {
        "id": "6AazIkqnMYEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation['full_text'] = full_texts"
      ],
      "metadata": {
        "id": "Ivi8hQJEMcPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation['tokens_ids'] = tokens_ids"
      ],
      "metadata": {
        "id": "U4cwnNo_MuB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "strange_cases = []\n",
        "for el in punctuation.values:\n",
        "  if el[0] == el[6]:\n",
        "    if el[1] == el[7]:\n",
        "      strange_cases.append(el)"
      ],
      "metadata": {
        "id": "pjUSLUEi2Ml6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "strange_cases = pandas.DataFrame(strange_cases)\n",
        "strange_cases.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids'}, inplace = True)"
      ],
      "metadata": {
        "id": "BNlbub3Y8cWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['full_text'].apply(lambda x: x not in strange_cases['full_text'].values)]"
      ],
      "metadata": {
        "id": "kAMDO3he3SP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.char_classes import ALPHA, ALPHA_LOWER, ALPHA_UPPER, CONCAT_QUOTES, LIST_ELLIPSES, LIST_ICONS\n",
        "from spacy.util import compile_infix_regex\n",
        "\n",
        "def custom_tokenizer(nlp):\n",
        "    infixes = (\n",
        "        LIST_ELLIPSES\n",
        "        + LIST_ICONS\n",
        "        + [\n",
        "            r\"(?<=[0-9])[+\\-\\*^](?=[0-9-])\",\n",
        "            r\"(?<=[{al}{q}])\\.(?=[{au}{q}])\".format(\n",
        "                al=ALPHA_LOWER, au=ALPHA_UPPER, q=CONCAT_QUOTES\n",
        "            ),\n",
        "            r\"(?<=[{a}]),(?=[{a}])\".format(a=ALPHA),\n",
        "            #r\"(?<=[{a}])(?:{h})(?=[{a}])\".format(a=ALPHA, h=HYPHENS),\n",
        "            r\"(?<=[{a}0-9])[:<>=/](?=[{a}])\".format(a=ALPHA),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    infix_re = compile_infix_regex(infixes)\n",
        "\n",
        "    return Tokenizer(nlp.vocab, prefix_search=nlp.tokenizer.prefix_search,\n",
        "                                suffix_search=nlp.tokenizer.suffix_search,\n",
        "                                infix_finditer=infix_re.finditer,\n",
        "                                token_match=nlp.tokenizer.token_match,\n",
        "                                rules=nlp.Defaults.tokenizer_exceptions)\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en\")\n",
        "nlp.tokenizer = custom_tokenizer(nlp)"
      ],
      "metadata": {
        "id": "sIA5QfKQWs3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dependencies = []\n",
        "for el in list(punctuation['tokens_ids']):\n",
        "  el = ' '.join(list(el.values()))\n",
        "  doc = nlp(el)\n",
        "  d = []\n",
        "  for token in doc:\n",
        "    d.append([token.text, token.pos_, token.tag_, token.dep_])\n",
        "  dependencies.append(d)"
      ],
      "metadata": {
        "id": "a-wWZ8SEMwHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  norm_values = []\n",
        "  norm_deps = []\n",
        "  strange_values = []\n",
        "  strange_deps = []\n",
        "  for i in range(len(dependencies)):\n",
        "    if len(dependencies[i]) == len(punctuation.values[i][-1].values()):\n",
        "      norm_values.append(punctuation.values[i])\n",
        "      norm_deps.append(dependencies[i])\n",
        "    else:\n",
        "      strange_values.append(punctuation.values[i])\n",
        "      strange_deps.append(dependencies[i])"
      ],
      "metadata": {
        "id": "ERfTFGX1CsRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_new_dependencies(dependencies, punct):\n",
        "    new_depends = dependencies\n",
        "    for k in range(len(dependencies)):\n",
        "      new_d = []\n",
        "      if dependencies[k][0].isnumeric():\n",
        "        if dependencies[k+1][0] == '-':\n",
        "          if dependencies[k+2][0].isnumeric:\n",
        "            word = ''.join([dependencies[k][0],dependencies[k+1][0],dependencies[k+2][0]])\n",
        "            poses = ' '.join([dependencies[k][1],dependencies[k+1][1],dependencies[k+2][1]])\n",
        "            tags = ' '.join([dependencies[k][2],dependencies[k+1][2],dependencies[k+2][2]])\n",
        "            deps = ' '.join([dependencies[k][3],dependencies[k+1][3],dependencies[k+2][3]])\n",
        "            new_d.append(word)\n",
        "            new_d.append(poses)\n",
        "            new_d.append(tags)\n",
        "            new_d.append(deps)\n",
        "            index = k\n",
        "            break\n",
        "          \n",
        "      \n",
        "            new_depends.insert(index, new_d)\n",
        "            new_depends.pop(index+1)\n",
        "            new_depends.pop(index+1)\n",
        "            new_depends.pop(index+1)\n",
        "      elif dependencies[i][k][0] == 'I':\n",
        "        if dependencies[i][k+1][0] =='\"':\n",
        "          if dependencies[i][k+1][0] =='m':\n",
        "            word = ''.join([dependencies[i][k][0],dependencies[i][k+1][0]])\n",
        "            poses = ' '.join([dependencies[i][k][1],dependencies[i][k+1][1]])\n",
        "            tags = ' '.join([dependencies[i][k][2],dependencies[i][k+1][2]])\n",
        "            deps = ' '.join([dependencies[i][k][3],dependencies[i][k+1][3]])\n",
        "            new_d.append(word)\n",
        "            new_d.append(poses)\n",
        "            new_d.append(tags)\n",
        "            new_d.append(deps)\n",
        "            index = k\n",
        "            break\n",
        "            new_depends.insert(index, new_d)\n",
        "            new_depends.pop(index+1)\n",
        "            new_depends.pop(index+1)\n",
        "\n",
        "      elif dependencies[k][0] == 'should':\n",
        "        if dependencies[k+1][0] =='nt':\n",
        "          word = ''.join([dependencies[k][0],dependencies[i][k+1][0]])\n",
        "          poses = ' '.join([dependencies[i][k][1],dependencies[i][k+1][1]])\n",
        "          tags = ' '.join([dependencies[i][k][2],dependencies[i][k+1][2]])\n",
        "          deps = ' '.join([dependencies[i][k][3],dependencies[i][k+1][3]])\n",
        "          new_d.append(word)\n",
        "          new_d.append(poses)\n",
        "          new_d.append(tags)\n",
        "          new_d.append(deps)\n",
        "          index = k\n",
        "          break\n",
        "          new_depends.insert(index, new_d)\n",
        "          new_depends.pop(index+1)\n",
        "          new_depends.pop(index+1)\n",
        "\n",
        "      elif dependencies[k][0] == 'That':\n",
        "        if dependencies[k+1][0] =='s':\n",
        "          word = ''.join([dependencies[i][k][0],dependencies[i][k+1][0]])\n",
        "          poses = ' '.join([dependencies[i][k][1],dependencies[i][k+1][1]])\n",
        "          tags = ' '.join([dependencies[i][k][2],dependencies[i][k+1][2]])\n",
        "          deps = ' '.join([dependencies[i][k][3],dependencies[i][k+1][3]])\n",
        "          new_d.append(word)\n",
        "          new_d.append(poses)\n",
        "          new_d.append(tags)\n",
        "          new_d.append(deps)\n",
        "          index = k\n",
        "          break\n",
        "          new_depends.insert(index, new_d)\n",
        "          new_depends.pop(index+1)\n",
        "          new_depends.pop(index+1)\n",
        "\n",
        "      elif dependencies[i][k][0] == 'that':\n",
        "        if dependencies[i][k+1][0] == 's':\n",
        "          word = ''.join([dependencies[i][k][0],dependencies[i][k+1][0]])\n",
        "          poses = ' '.join([dependencies[i][k][1],dependencies[i][k+1][1]])\n",
        "          tags = ' '.join([dependencies[i][k][2],dependencies[i][k+1][2]])\n",
        "          deps = ' '.join([dependencies[i][k][3],dependencies[i][k+1][3]])\n",
        "          new_d.append(word)\n",
        "          new_d.append(poses)\n",
        "          new_d.append(tags)\n",
        "          new_d.append(deps)\n",
        "          index = k\n",
        "          break\n",
        "          new_depends.insert(index, new_d)\n",
        "          new_depends.pop(index+1)\n",
        "          new_depends.pop(index+1)\n",
        "\n",
        "      elif dependencies[i][k][0] == 'can':\n",
        "        if dependencies[i][k+1][0] == 't':\n",
        "          word = ''.join([dependencies[i][k][0],dependencies[i][k+1][0]])\n",
        "          poses = ' '.join([dependencies[i][k][1],dependencies[i][k+1][1]])\n",
        "          tags = ' '.join([dependencies[i][k][2],dependencies[i][k+1][2]])\n",
        "          deps = ' '.join([dependencies[i][k][3],dependencies[i][k+1][3]])\n",
        "          new_d.append(word)\n",
        "          new_d.append(poses)\n",
        "          new_d.append(tags)\n",
        "          new_d.append(deps)\n",
        "          index = k\n",
        "          break\n",
        "          new_depends.insert(index, new_d)\n",
        "          new_depends.pop(index+1)\n",
        "          new_depends.pop(index+1)"
      ],
      "metadata": {
        "id": "ItY_WTVgBQ5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "norm_values = pandas.DataFrame(norm_values)\n",
        "norm_values.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids'}, inplace = True)"
      ],
      "metadata": {
        "id": "MCGI3vT5JSel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dicts_of_deps = []\n",
        "for i, b in enumerate(norm_values.values):\n",
        "  d_v = {}\n",
        "  for k in range(len(list(b[7].keys()))):\n",
        "    d_v[list(b[7].keys())[k]] = norm_deps[i][k]\n",
        "  dicts_of_deps.append(d_v)"
      ],
      "metadata": {
        "id": "7hoehsqiNW1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "norm_values['dependencies'] = dicts_of_deps"
      ],
      "metadata": {
        "id": "svuKuFTXJ4PW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "norm_values"
      ],
      "metadata": {
        "id": "8ahPCOdrv2Sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_clean_text(sample):\n",
        "  id = int(sample[2])\n",
        "  change = sample[4]\n",
        "  if isinstance(change, float):\n",
        "    change = str(int(change))\n",
        "  else:\n",
        "    change = change\n",
        "  rechange = sample[5]\n",
        "  change = re.findall(r\"[\\w']+|[.,!?;()%]\", str(change))\n",
        "  rechange2 = re.findall(r\"[\\w']+|[.,!?;()%]\", str(rechange))\n",
        "\n",
        "  change_len = len(change)\n",
        "  text_with_ids = sample[7]\n",
        "  end = id + change_len \n",
        "  \n",
        "  all_words = []\n",
        "  for el in list(text_with_ids.keys()):\n",
        "    try:\n",
        "      if el not in [i for i in range(id,end)]:\n",
        "        all_words.append(str(text_with_ids[el]))\n",
        "      elif all_words[-1] != rechange:\n",
        "        all_words.append(str(rechange))\n",
        "\n",
        "    except IndexError:\n",
        "      if len(all_words) == 0:\n",
        "        all_words.append(str(rechange))\n",
        "  final_text = ' '.join(all_words)\n",
        "  for k in string.punctuation: \n",
        "      if k != '(':\n",
        "        final_text = final_text.replace(f' {k}',k)\n",
        "      elif k == '(':\n",
        "        final_text = final_text.replace(f'{k} ',k)\n",
        "\n",
        "  final_text2 = re.findall(r\"[\\w']+|[.,!?;()%]\", str(final_text))\n",
        "  dict_words_clean = {}\n",
        "  ids_clean = [i for i in range(list((text_with_ids.keys()))[0], len(final_text2) + list(text_with_ids.keys())[0])]\n",
        "\n",
        "  for k in range(len(ids_clean)):\n",
        "    dict_words_clean[ids_clean[k]] = final_text2[k]\n",
        "\n",
        "  return final_text, dict_words_clean"
      ],
      "metadata": {
        "id": "34RCJwWD0eae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corrected_text = []\n",
        "corrected_with_ids = []\n",
        "for element in norm_values.values:\n",
        "  corrected_text.append(get_clean_text(element)[0])\n",
        "  corrected_with_ids.append(get_clean_text(element)[1])"
      ],
      "metadata": {
        "id": "0ilmXiDY5B8R"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "norm_values['corrected_text'] = corrected_text"
      ],
      "metadata": {
        "id": "3FQsp6u65Dmg"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "norm_values['corrected_with_ids'] = corrected_with_ids"
      ],
      "metadata": {
        "id": "VL1IFlp85SPE"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dependencies_heptabot = []\n",
        "\n",
        "for el in norm_values.values:\n",
        "  el = ' '.join(list(el[10].values()))\n",
        "\n",
        "  doc = nlp(el)\n",
        "  d = []\n",
        "  for token in doc:\n",
        "    d.append([token.text, token.pos_, token.tag_, token.dep_])\n",
        "  dependencies_heptabot.append(d)"
      ],
      "metadata": {
        "id": "n0auMQQ995WR"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dicts_of_deps_heptabot = []\n",
        "for i, b in enumerate(norm_values.values):\n",
        "  d_v = {}\n",
        "  for k in range(len(list(b[10].keys()))):\n",
        "    d_v[list(b[10].keys())[k]] = dependencies_heptabot[i][k]\n",
        "  dicts_of_deps_heptabot.append(d_v)"
      ],
      "metadata": {
        "id": "e9InNmsr_Cgd"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "norm_values['dependencies_heptabot'] = dicts_of_deps_heptabot"
      ],
      "metadata": {
        "id": "lfeEz1tp-Nr5"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation_all = punctuation"
      ],
      "metadata": {
        "id": "xIVdtYAQ0fJh"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = norm_values"
      ],
      "metadata": {
        "id": "_9TBXsiflCPG"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation.loc[(punctuation.correction == 'none'), 'correction'] = ''"
      ],
      "metadata": {
        "id": "He-9OCzglIdx"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Articles:"
      ],
      "metadata": {
        "id": "CdUO4BxxkcAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bool_series = punctuation[\"correction\"].str.lower().str.endswith('the', na = False)\n",
        "  \n",
        "articles_mistakes = punctuation[bool_series]"
      ],
      "metadata": {
        "id": "_Nud7uE1k9we"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles_a_the = []\n",
        "for el in articles_mistakes.values:\n",
        "  if 'an' in str(el[4]).split():\n",
        "    if 'the' in str(el[5]).split():\n",
        "      articles_a_the.append(el)\n",
        "  elif 'a' in str(el[4]).split():\n",
        "    if 'the' in str(el[5]).split():\n",
        "      articles_a_the.append(el)\n",
        "  elif 'the' in str(el[4]).split():\n",
        "    if 'a' in str(el[5]).split():\n",
        "      articles_a_the.append(el)\n",
        "  elif 'the' in str(el[4]).split():\n",
        "    if 'an' in str(el[5]).split():\n",
        "      articles_a_the.append(el)"
      ],
      "metadata": {
        "id": "bWxJwRmaYigR"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles_a_the = pandas.DataFrame(articles_a_the)\n",
        "articles_a_the.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "articles_a_the = articles_a_the.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "ouF_rpPJY0DT"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles_a_the"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "DF7cNu6Ydldp",
        "outputId": "4902b5a8-ca56-42fd-8391-14813fcd6e06"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   text_id  sentence_id  token_id mistake_type error_span correction  \\\n",
              "0    440.0         10.0     213.0        punct          a      , the   \n",
              "1    701.0         10.0     172.0        punct         an      , the   \n",
              "\n",
              "                                           full_text  \\\n",
              "0  As an example a quick increase of literacy in ...   \n",
              "1  From the other hand, if talk about university ...   \n",
              "\n",
              "                                          tokens_ids  \\\n",
              "0  {210: 'As', 211: 'an', 212: 'example', 213: 'a...   \n",
              "1  {162: 'From', 163: 'the', 164: 'other', 165: '...   \n",
              "\n",
              "                                        dependencies  \\\n",
              "0  {210: ['As', 'SCONJ', 'IN', 'prep'], 211: ['an...   \n",
              "1  {162: ['From', 'ADP', 'IN', 'prep'], 163: ['th...   \n",
              "\n",
              "                                      corrected_text  \\\n",
              "0  As an example, the quick increase of literacy ...   \n",
              "1  From the other hand, if talk about university ...   \n",
              "\n",
              "                                  corrected_with_ids  \\\n",
              "0  {210: 'As', 211: 'an', 212: 'example', 213: ',...   \n",
              "1  {162: 'From', 163: 'the', 164: 'other', 165: '...   \n",
              "\n",
              "                               dependencies_heptabot  \n",
              "0  {210: ['As', 'SCONJ', 'IN', 'prep'], 211: ['an...  \n",
              "1  {162: ['From', 'ADP', 'IN', 'prep'], 163: ['th...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2abe494c-63eb-46b5-8eeb-ec53e9ae39b4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>sentence_id</th>\n",
              "      <th>token_id</th>\n",
              "      <th>mistake_type</th>\n",
              "      <th>error_span</th>\n",
              "      <th>correction</th>\n",
              "      <th>full_text</th>\n",
              "      <th>tokens_ids</th>\n",
              "      <th>dependencies</th>\n",
              "      <th>corrected_text</th>\n",
              "      <th>corrected_with_ids</th>\n",
              "      <th>dependencies_heptabot</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>440.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>213.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>a</td>\n",
              "      <td>, the</td>\n",
              "      <td>As an example a quick increase of literacy in ...</td>\n",
              "      <td>{210: 'As', 211: 'an', 212: 'example', 213: 'a...</td>\n",
              "      <td>{210: ['As', 'SCONJ', 'IN', 'prep'], 211: ['an...</td>\n",
              "      <td>As an example, the quick increase of literacy ...</td>\n",
              "      <td>{210: 'As', 211: 'an', 212: 'example', 213: ',...</td>\n",
              "      <td>{210: ['As', 'SCONJ', 'IN', 'prep'], 211: ['an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>701.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>172.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>an</td>\n",
              "      <td>, the</td>\n",
              "      <td>From the other hand, if talk about university ...</td>\n",
              "      <td>{162: 'From', 163: 'the', 164: 'other', 165: '...</td>\n",
              "      <td>{162: ['From', 'ADP', 'IN', 'prep'], 163: ['th...</td>\n",
              "      <td>From the other hand, if talk about university ...</td>\n",
              "      <td>{162: 'From', 163: 'the', 164: 'other', 165: '...</td>\n",
              "      <td>{162: ['From', 'ADP', 'IN', 'prep'], 163: ['th...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2abe494c-63eb-46b5-8eeb-ec53e9ae39b4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2abe494c-63eb-46b5-8eeb-ec53e9ae39b4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2abe494c-63eb-46b5-8eeb-ec53e9ae39b4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def change_articles_a_the(sample):\n",
        "  tag = 'punctuation'\n",
        "  pprint(\"USER'S TEXT: \" + sample[6])\n",
        "  pprint('CORRECTED TEXT: ' + str(sample[9]))\n",
        "\n",
        "\n",
        "  i = list(sample[7].keys())[0]\n",
        "  index1 = list(sample[7].keys())[0]\n",
        "  while i != sample[2]:\n",
        "    i += 1\n",
        "    if sample[7][i] == ',':\n",
        "      index1 = i + 1\n",
        "  index2 = int(sample[2]) \n",
        "\n",
        "\n",
        "\n",
        "  indexes = range(index1, index2)\n",
        "  error_area = []\n",
        "  for b in indexes:\n",
        "    error_area.append(str(sample[7][b]))\n",
        "\n",
        "  \n",
        "\n",
        "  pprint('CORRECTION: ' + ' '.join(error_area) + ',')\n",
        "  pprint('ERROR AREA: '+ ' '.join(error_area))\n",
        "  pprint('TAG: ' + tag)\n",
        "\n",
        "  tag = 'articles'\n",
        "  article = re.findall(r'[a-zA-Z]+', str(sample[5]))[0]\n",
        "  correction = [article, str(sample[7][int(sample[2])+1])]\n",
        "  error_area = [sample[7][int(sample[2])], sample[7][int(sample[2])+1]]\n",
        "  pprint('CORRECTION: ' + ' '.join(correction))\n",
        "  pprint('ERROR AREA: '+ ' '.join(error_area))\n",
        "  pprint('TAG: ' + tag)"
      ],
      "metadata": {
        "id": "oWUuHNkGacaq"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in articles_a_the.values:\n",
        "#  change_articles_a_the(el)"
      ],
      "metadata": {
        "id": "4xyVRfYHaesy"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles_mistakes = articles_mistakes[articles_mistakes['corrected_text'].apply(lambda x: x not in articles_a_the['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "NypZCgClZtBI"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in articles_a_the['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "IoQiK3B90bWz"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles_without_punct = []\n",
        "for el in articles_mistakes.values:\n",
        "  for i in string.punctuation:\n",
        "    if i not in str(el[4]):\n",
        "      if i in str(el[5]):\n",
        "        articles_without_punct.append(el)"
      ],
      "metadata": {
        "id": "PpB9LaWDqTUW"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles_without_punct = pandas.DataFrame(articles_without_punct)\n",
        "articles_without_punct.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "articles_without_punct = articles_without_punct.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "RJ4T5ar5qqes"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def articles_correction_commas(sample):\n",
        "\n",
        "  tag = 'punctuation'\n",
        "  pprint(\"USER'S TEXT: \" + sample[6])\n",
        "  pprint('CORRECTED TEXT: ' + str(sample[9]))\n",
        "  i = list(sample[7].keys())[0]\n",
        "  index1 = list(sample[7].keys())[0]\n",
        "  while i != sample[2]:\n",
        "    i += 1\n",
        "    if sample[7][i] == ',':\n",
        "      index1 = i + 1\n",
        "\n",
        "  index2 = int(sample[2]) + 1\n",
        "  indexes = range(index1, index2)\n",
        "\n",
        "  error_area = []\n",
        "  for b in indexes:\n",
        "    error_area.append(str(sample[7][b]))\n",
        "\n",
        "  pprint('CORRECTION: ' + ' '.join(error_area) + ',')\n",
        "  pprint('ERROR AREA: '+ ' '.join(error_area))\n",
        "  pprint('TAG: ' + tag)\n",
        "\n",
        "\n",
        "  error_area2 = []\n",
        "  tag = 'articles'\n",
        "  indexes = []\n",
        "  ind = int(sample[2]+1)\n",
        "  while not (sample[8][ind][1] == 'NOUN' or sample[8][ind][1] == 'PROPN'):\n",
        "    indexes.append(ind)\n",
        "    ind += 1\n",
        "  indexes.append(ind)\n",
        "      \n",
        "\n",
        "  for b in indexes:\n",
        "    error_area2.append(str(sample[7][b]))\n",
        "  pprint('ERROR AREA: '+ ' '.join(error_area2))\n",
        "  pprint('CORRECTION: ' + 'the ' + ' '.join(error_area2))\n",
        "  pprint('TAG: ' + tag) \n",
        "  print('\\n')"
      ],
      "metadata": {
        "id": "2fOyhK-ylOnq"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in articles_without_punct.values:\n",
        "#  articles_correction_commas(el)\n",
        "  "
      ],
      "metadata": {
        "id": "W1oSy0hIrSyc"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles_mistakes = articles_mistakes[articles_mistakes['corrected_text'].apply(lambda x: x not in articles_without_punct['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "4Ups4Mnjt5SO"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in articles_without_punct['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "GhczM54K0omf"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles_with_the = []\n",
        "for el in articles_mistakes.values:\n",
        "  if 'and' in str(el[5]):\n",
        "    if 'the' in str(el[5]):\n",
        "      articles_with_the.append(el)"
      ],
      "metadata": {
        "id": "zy5ygwmwuIQX"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles_with_the = pandas.DataFrame(articles_with_the)\n",
        "articles_with_the.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "articles_with_the = articles_with_the.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "92XH_mB4ufCH"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def articles_correction_dots(sample):\n",
        "  element = str(el[4])\n",
        "  f = re.findall(r'[a-zA-Z]+', element)\n",
        "  a = re.findall(r'\\d+', element)\n",
        "  k = re.findall(r'\\W', element)\n",
        "  all = f+a+k\n",
        "  if ' ' in all:\n",
        "    all.remove(' ')\n",
        "\n",
        "  length = len(all)\n",
        "\n",
        "  indexes = []\n",
        "  ind = int(sample[2]+length)\n",
        "  while not (sample[8][ind][1] == 'NOUN' or sample[8][ind][1] == 'PROPN'):\n",
        "    indexes.append(ind)\n",
        "    ind += 1\n",
        "  indexes.append(ind)\n",
        "\n",
        "  error_area = []\n",
        "  for i in indexes:\n",
        "    error_area.append(sample[7][i])\n",
        "\n",
        "  tag = 'articles'\n",
        "  pprint('CORRECTION: ' + 'the ' + ' '.join(error_area))\n",
        "  pprint('ERROR AREA: '+ str(sample[7][int(sample[2])+length]))\n",
        "  pprint('TAG: ' + tag)\n",
        "\n",
        "  tag = 'Absence of a component in clause or sentence'\n",
        "  pprint('CORRECTION: ' + str(sample[7][int(sample[2])]) + ', and')\n",
        "  pprint('ERROR AREA: '+ str(sample[7][int(sample[2])]))\n",
        "  pprint('TAG: ' + tag)\n",
        "\n"
      ],
      "metadata": {
        "id": "jqk3xUfp09Hk"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in articles_with_the.values:\n",
        "#  articles_correction_dots(el)"
      ],
      "metadata": {
        "id": "74FJW2GVult8"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles_mistakes = articles_mistakes[articles_mistakes['corrected_text'].apply(lambda x: x not in articles_with_the['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "zsEbxA2q00M5"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in articles_with_the['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "TaoT0n_o0tsu"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Commas are changed with dots:"
      ],
      "metadata": {
        "id": "5oS9xi8z2c_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bool_series = punctuation[\"error_span\"].str.lower().str.endswith(',', na = False)\n",
        "commas_dot = punctuation[bool_series]\n",
        "bool_series = commas_dot['correction'].str.endswith('.', na = False)\n",
        "commas_dot = commas_dot[bool_series]"
      ],
      "metadata": {
        "id": "5YLlVaXr2y4n"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# это первое\n",
        "def checking_commas_dots(sample):\n",
        "  tag = 'punctuation'\n",
        "  parts_of_sentences = ['ROOT', 'relcl', 'nsubj', 'expl', 'conj', 'ccomp', 'aux', \n",
        "                      'neg', 'nsubjpass','punct','intj', 'acl']\n",
        "\n",
        "  all_sents = []\n",
        "  first_sent = []\n",
        "  for el in sample[11].values():\n",
        "    if el[-1] in parts_of_sentences:\n",
        "      all_sents.append(el)\n",
        "\n",
        "  for i in all_sents:\n",
        "    first_sent.append(i)\n",
        "    if i[0] == '.':\n",
        "      ind = all_sents.index(i)\n",
        "      break\n",
        "      \n",
        "  second_sent = all_sents[ind::]\n",
        "  del first_sent[-1]\n",
        "\n",
        "  if len(first_sent) >= 4:\n",
        "    pass\n",
        "\n",
        "  else:\n",
        "    parts_of_sentences2 = ['nsubj','ROOT', 'expl']\n",
        "    sent = []\n",
        "    for el in first_sent:\n",
        "      if el in parts_of_sentences2:\n",
        "        sent.append(el)\n",
        "        if len(sent) >= 2:\n",
        "          pass\n",
        "\n",
        "  index1 = int(list(sample[8].keys())[0])\n",
        "  index2 = int(sample[2]) + 1\n",
        "  indexes = range(index1, index2)\n",
        "  error_area = [sample[7][i] for i in indexes]\n",
        "\n",
        "  print(\"USER'S TEXT: \" + sample[6])\n",
        "  print('CORRECTED TEXT: ' + str(sample[9]))\n",
        "\n",
        "  pprint('CORRECTION: ' + ' '.join(error_area[0:-1]) + '.')\n",
        "  pprint('ERROR AREA: '+ ' '.join(error_area))\n",
        "  pprint('TAG: ' + tag)\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "uP1c6eeT3H0D"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in commas_dot.values:\n",
        "#  checking_commas_dots(el)"
      ],
      "metadata": {
        "id": "-2mBX2vj7SZ4"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[~punctuation.isin(commas_dot)].dropna()"
      ],
      "metadata": {
        "id": "GyQccCXI7Vge"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeral_procents = []\n",
        "for el in punctuation.values:\n",
        "  if str(el[4]).isnumeric():\n",
        "    if str(el[5]).endswith('%'):\n",
        "      numeral_procents.append(el)"
      ],
      "metadata": {
        "id": "QHev628Z-NHi"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeral_procents = pandas.DataFrame(numeral_procents)\n",
        "numeral_procents.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "numeral_procents = numeral_procents.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "9jDDZR8i-PIU"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def num_procents(sample): \n",
        "  text = str(sample[5])\n",
        "  find = re.findall(r\"[\\w']+|[.,!?;()%]\", text)\n",
        "\n",
        "\n",
        "  pprint(\"USER'S TEXT: \" + sample[6])\n",
        "  pprint('CORRECTED TEXT: ' + str(sample[9]))\n",
        "\n",
        "  pprint('CORRECTION: ' + ''.join(find))\n",
        "  pprint('ERROR AREA: '+ str(sample[7][sample[2]]))\n",
        "  pprint('TAG: numerals')"
      ],
      "metadata": {
        "id": "MD83kRaJ-U3m"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in numeral_procents.values:\n",
        "#  num_procents(el)"
      ],
      "metadata": {
        "id": "LGn6iKrq-cew"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in numeral_procents['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "OOHitvDc-eLT"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "commas_delete = punctuation[punctuation.correction == '']\n",
        "commas_delete = commas_delete[commas_delete.error_span == ',']\n",
        "commas_delete"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "MCITSspD-kdJ",
        "outputId": "6d401378-e0e6-459e-ced9-88c1ee996d55"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     text_id  sentence_id  token_id mistake_type error_span correction  \\\n",
              "172     14.0          2.0      22.0        punct          ,              \n",
              "173     20.0          1.0      16.0        punct          ,              \n",
              "174     44.0          2.0      44.0        punct          ,              \n",
              "175     44.0         14.0     270.0        punct          ,              \n",
              "176     48.0          8.0     158.0        punct          ,              \n",
              "..       ...          ...       ...          ...        ...        ...   \n",
              "845   1696.0          9.0     205.0        punct          ,              \n",
              "846   1697.0          3.0      55.0        punct          ,              \n",
              "847   1697.0          6.0     126.0        punct          ,              \n",
              "848   1698.0          4.0      79.0        punct          ,              \n",
              "849   1698.0         10.0     231.0        punct          ,              \n",
              "\n",
              "                                             full_text  \\\n",
              "172  Almost from start we already have some life's ...   \n",
              "173  As many people values benefits and opportuniti...   \n",
              "174  However, other think that education is a speci...   \n",
              "175  To conclude, education is an important both fo...   \n",
              "176  Another point of view, that I support, is that...   \n",
              "..                                                 ...   \n",
              "845  However, there are people, who think that stud...   \n",
              "846  It is also clear, that the percentage differen...   \n",
              "847  It is significant, that the percentage differe...   \n",
              "848  First of all, it seems to me, that the ability...   \n",
              "849  However the variety of different governments g...   \n",
              "\n",
              "                                            tokens_ids  \\\n",
              "172  {12: 'Almost', 13: 'from', 14: 'start', 15: 'w...   \n",
              "173  {1: 'As', 2: 'many', 3: 'people', 4: 'values',...   \n",
              "174  {30: 'However', 31: ',', 32: 'other', 33: 'thi...   \n",
              "175  {260: 'To', 261: 'conclude', 262: ',', 263: 'e...   \n",
              "176  {154: 'Another', 155: 'point', 156: 'of', 157:...   \n",
              "..                                                 ...   \n",
              "845  {200: 'However', 201: ',', 202: 'there', 203: ...   \n",
              "846  {51: 'It', 52: 'is', 53: 'also', 54: 'clear', ...   \n",
              "847  {123: 'It', 124: 'is', 125: 'significant', 126...   \n",
              "848  {71: 'First', 72: 'of', 73: 'all', 74: ',', 75...   \n",
              "849  {220: 'However', 221: 'the', 222: 'variety', 2...   \n",
              "\n",
              "                                          dependencies  \\\n",
              "172  {12: ['Almost', 'ADV', 'RB', 'advmod'], 13: ['...   \n",
              "173  {1: ['As', 'SCONJ', 'IN', 'mark'], 2: ['many',...   \n",
              "174  {30: ['However', 'ADV', 'RB', 'advmod'], 31: [...   \n",
              "175  {260: ['To', 'PART', 'TO', 'aux'], 261: ['conc...   \n",
              "176  {154: ['Another', 'DET', 'DT', 'det'], 155: ['...   \n",
              "..                                                 ...   \n",
              "845  {200: ['However', 'ADV', 'RB', 'advmod'], 201:...   \n",
              "846  {51: ['It', 'PRON', 'PRP', 'nsubj'], 52: ['is'...   \n",
              "847  {123: ['It', 'PRON', 'PRP', 'nsubj'], 124: ['i...   \n",
              "848  {71: ['First', 'ADV', 'RB', 'advmod'], 72: ['o...   \n",
              "849  {220: ['However', 'ADV', 'RB', 'advmod'], 221:...   \n",
              "\n",
              "                                        corrected_text  \\\n",
              "172  Almost from start we already have some life's ...   \n",
              "173  As many people values benefits and opportuniti...   \n",
              "174  However, other think that education is a speci...   \n",
              "175  To conclude, education is an important both fo...   \n",
              "176  Another point of view  that I support, is that...   \n",
              "..                                                 ...   \n",
              "845  However, there are people  who think that stud...   \n",
              "846  It is also clear  that the percentage differen...   \n",
              "847  It is significant  that the percentage differe...   \n",
              "848  First of all, it seems to me  that the ability...   \n",
              "849  However the variety of different governments g...   \n",
              "\n",
              "                                    corrected_with_ids  \\\n",
              "172  {12: 'Almost', 13: 'from', 14: 'start', 15: 'w...   \n",
              "173  {1: 'As', 2: 'many', 3: 'people', 4: 'values',...   \n",
              "174  {30: 'However', 31: ',', 32: 'other', 33: 'thi...   \n",
              "175  {260: 'To', 261: 'conclude', 262: ',', 263: 'e...   \n",
              "176  {154: 'Another', 155: 'point', 156: 'of', 157:...   \n",
              "..                                                 ...   \n",
              "845  {200: 'However', 201: ',', 202: 'there', 203: ...   \n",
              "846  {51: 'It', 52: 'is', 53: 'also', 54: 'clear', ...   \n",
              "847  {123: 'It', 124: 'is', 125: 'significant', 126...   \n",
              "848  {71: 'First', 72: 'of', 73: 'all', 74: ',', 75...   \n",
              "849  {220: 'However', 221: 'the', 222: 'variety', 2...   \n",
              "\n",
              "                                 dependencies_heptabot  \n",
              "172  {12: ['Almost', 'ADV', 'RB', 'advmod'], 13: ['...  \n",
              "173  {1: ['As', 'SCONJ', 'IN', 'mark'], 2: ['many',...  \n",
              "174  {30: ['However', 'ADV', 'RB', 'advmod'], 31: [...  \n",
              "175  {260: ['To', 'PART', 'TO', 'aux'], 261: ['conc...  \n",
              "176  {154: ['Another', 'DET', 'DT', 'det'], 155: ['...  \n",
              "..                                                 ...  \n",
              "845  {200: ['However', 'ADV', 'RB', 'advmod'], 201:...  \n",
              "846  {51: ['It', 'PRON', 'PRP', 'nsubj'], 52: ['is'...  \n",
              "847  {123: ['It', 'PRON', 'PRP', 'nsubj'], 124: ['i...  \n",
              "848  {71: ['First', 'ADV', 'RB', 'advmod'], 72: ['o...  \n",
              "849  {220: ['However', 'ADV', 'RB', 'advmod'], 221:...  \n",
              "\n",
              "[642 rows x 12 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-755af361-6253-4dd8-9acb-ceb3bbcdc03e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>sentence_id</th>\n",
              "      <th>token_id</th>\n",
              "      <th>mistake_type</th>\n",
              "      <th>error_span</th>\n",
              "      <th>correction</th>\n",
              "      <th>full_text</th>\n",
              "      <th>tokens_ids</th>\n",
              "      <th>dependencies</th>\n",
              "      <th>corrected_text</th>\n",
              "      <th>corrected_with_ids</th>\n",
              "      <th>dependencies_heptabot</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>14.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>,</td>\n",
              "      <td></td>\n",
              "      <td>Almost from start we already have some life's ...</td>\n",
              "      <td>{12: 'Almost', 13: 'from', 14: 'start', 15: 'w...</td>\n",
              "      <td>{12: ['Almost', 'ADV', 'RB', 'advmod'], 13: ['...</td>\n",
              "      <td>Almost from start we already have some life's ...</td>\n",
              "      <td>{12: 'Almost', 13: 'from', 14: 'start', 15: 'w...</td>\n",
              "      <td>{12: ['Almost', 'ADV', 'RB', 'advmod'], 13: ['...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>20.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>,</td>\n",
              "      <td></td>\n",
              "      <td>As many people values benefits and opportuniti...</td>\n",
              "      <td>{1: 'As', 2: 'many', 3: 'people', 4: 'values',...</td>\n",
              "      <td>{1: ['As', 'SCONJ', 'IN', 'mark'], 2: ['many',...</td>\n",
              "      <td>As many people values benefits and opportuniti...</td>\n",
              "      <td>{1: 'As', 2: 'many', 3: 'people', 4: 'values',...</td>\n",
              "      <td>{1: ['As', 'SCONJ', 'IN', 'mark'], 2: ['many',...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>44.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>,</td>\n",
              "      <td></td>\n",
              "      <td>However, other think that education is a speci...</td>\n",
              "      <td>{30: 'However', 31: ',', 32: 'other', 33: 'thi...</td>\n",
              "      <td>{30: ['However', 'ADV', 'RB', 'advmod'], 31: [...</td>\n",
              "      <td>However, other think that education is a speci...</td>\n",
              "      <td>{30: 'However', 31: ',', 32: 'other', 33: 'thi...</td>\n",
              "      <td>{30: ['However', 'ADV', 'RB', 'advmod'], 31: [...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>44.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>270.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>,</td>\n",
              "      <td></td>\n",
              "      <td>To conclude, education is an important both fo...</td>\n",
              "      <td>{260: 'To', 261: 'conclude', 262: ',', 263: 'e...</td>\n",
              "      <td>{260: ['To', 'PART', 'TO', 'aux'], 261: ['conc...</td>\n",
              "      <td>To conclude, education is an important both fo...</td>\n",
              "      <td>{260: 'To', 261: 'conclude', 262: ',', 263: 'e...</td>\n",
              "      <td>{260: ['To', 'PART', 'TO', 'aux'], 261: ['conc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>48.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>158.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>,</td>\n",
              "      <td></td>\n",
              "      <td>Another point of view, that I support, is that...</td>\n",
              "      <td>{154: 'Another', 155: 'point', 156: 'of', 157:...</td>\n",
              "      <td>{154: ['Another', 'DET', 'DT', 'det'], 155: ['...</td>\n",
              "      <td>Another point of view  that I support, is that...</td>\n",
              "      <td>{154: 'Another', 155: 'point', 156: 'of', 157:...</td>\n",
              "      <td>{154: ['Another', 'DET', 'DT', 'det'], 155: ['...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>845</th>\n",
              "      <td>1696.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>205.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>,</td>\n",
              "      <td></td>\n",
              "      <td>However, there are people, who think that stud...</td>\n",
              "      <td>{200: 'However', 201: ',', 202: 'there', 203: ...</td>\n",
              "      <td>{200: ['However', 'ADV', 'RB', 'advmod'], 201:...</td>\n",
              "      <td>However, there are people  who think that stud...</td>\n",
              "      <td>{200: 'However', 201: ',', 202: 'there', 203: ...</td>\n",
              "      <td>{200: ['However', 'ADV', 'RB', 'advmod'], 201:...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>846</th>\n",
              "      <td>1697.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>55.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>,</td>\n",
              "      <td></td>\n",
              "      <td>It is also clear, that the percentage differen...</td>\n",
              "      <td>{51: 'It', 52: 'is', 53: 'also', 54: 'clear', ...</td>\n",
              "      <td>{51: ['It', 'PRON', 'PRP', 'nsubj'], 52: ['is'...</td>\n",
              "      <td>It is also clear  that the percentage differen...</td>\n",
              "      <td>{51: 'It', 52: 'is', 53: 'also', 54: 'clear', ...</td>\n",
              "      <td>{51: ['It', 'PRON', 'PRP', 'nsubj'], 52: ['is'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>847</th>\n",
              "      <td>1697.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>,</td>\n",
              "      <td></td>\n",
              "      <td>It is significant, that the percentage differe...</td>\n",
              "      <td>{123: 'It', 124: 'is', 125: 'significant', 126...</td>\n",
              "      <td>{123: ['It', 'PRON', 'PRP', 'nsubj'], 124: ['i...</td>\n",
              "      <td>It is significant  that the percentage differe...</td>\n",
              "      <td>{123: 'It', 124: 'is', 125: 'significant', 126...</td>\n",
              "      <td>{123: ['It', 'PRON', 'PRP', 'nsubj'], 124: ['i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>848</th>\n",
              "      <td>1698.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>79.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>,</td>\n",
              "      <td></td>\n",
              "      <td>First of all, it seems to me, that the ability...</td>\n",
              "      <td>{71: 'First', 72: 'of', 73: 'all', 74: ',', 75...</td>\n",
              "      <td>{71: ['First', 'ADV', 'RB', 'advmod'], 72: ['o...</td>\n",
              "      <td>First of all, it seems to me  that the ability...</td>\n",
              "      <td>{71: 'First', 72: 'of', 73: 'all', 74: ',', 75...</td>\n",
              "      <td>{71: ['First', 'ADV', 'RB', 'advmod'], 72: ['o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>849</th>\n",
              "      <td>1698.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>231.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>,</td>\n",
              "      <td></td>\n",
              "      <td>However the variety of different governments g...</td>\n",
              "      <td>{220: 'However', 221: 'the', 222: 'variety', 2...</td>\n",
              "      <td>{220: ['However', 'ADV', 'RB', 'advmod'], 221:...</td>\n",
              "      <td>However the variety of different governments g...</td>\n",
              "      <td>{220: 'However', 221: 'the', 222: 'variety', 2...</td>\n",
              "      <td>{220: ['However', 'ADV', 'RB', 'advmod'], 221:...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>642 rows × 12 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-755af361-6253-4dd8-9acb-ceb3bbcdc03e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-755af361-6253-4dd8-9acb-ceb3bbcdc03e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-755af361-6253-4dd8-9acb-ceb3bbcdc03e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "that_commas_delete = []\n",
        "for el in commas_delete.values:\n",
        "  if str(el[7][int(el[2])+1]) == 'that':\n",
        "    that_commas_delete.append(el)"
      ],
      "metadata": {
        "id": "4Pi4kpaR-r6v"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "that_commas_delete = pandas.DataFrame(that_commas_delete)\n",
        "that_commas_delete.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "that_commas_delete = that_commas_delete.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "8pxB3XG8AdZu"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def change_relative_comma(sample):\n",
        "  tag = 'relative clause'\n",
        "\n",
        "  pprint(\"USER'S TEXT: \" + sample[6])\n",
        "  pprint('CORRECTED TEXT: ' + str(sample[9]))\n",
        "\n",
        "  error_area = [str(el[7][int(el[2])]), str(el[7][int(el[2])+1])]\n",
        "  pprint('ERROR AREA: ' + ' '.join(error_area))\n",
        "  pprint('CORRECTION: '+ str(sample[7][int(sample[2]+1)]))\n",
        "  pprint('TAG: ' + tag)"
      ],
      "metadata": {
        "id": "fmV0vOquAhwL"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in that_commas_delete.values:\n",
        "#  change_relative_comma(el)"
      ],
      "metadata": {
        "id": "kBm2lyYVDR2A"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "commas_delete = commas_delete[commas_delete['corrected_text'].apply(lambda x: x not in that_commas_delete['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "omHQTydMD2Xr"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in that_commas_delete['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "BXy63w1qELXi"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "whos_whichs_commas_delete = []\n",
        "for el in commas_delete.values:\n",
        "  if str(el[7][int(el[2])+1]) == 'who':\n",
        "    whos_whichs_commas_delete.append(el)\n",
        "  if str(el[7][int(el[2])+1]) == 'which':\n",
        "    whos_whichs_commas_delete.append(el)"
      ],
      "metadata": {
        "id": "NEmqC8dFERq0"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "whos_whichs_commas_delete = pandas.DataFrame(whos_whichs_commas_delete)\n",
        "whos_whichs_commas_delete.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "whos_whichs_commas_delete = whos_whichs_commas_delete.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "_nUjByrWEd79"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Уточнить завтра"
      ],
      "metadata": {
        "id": "mTLbgQO0Fb7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in whos_whichs_commas_delete.values:\n",
        "#  change_relative_comma(el)"
      ],
      "metadata": {
        "id": "Usc8c97rEjTY"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "commas_delete = commas_delete[commas_delete['corrected_text'].apply(lambda x: x not in whos_whichs_commas_delete['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "DzJGs1RTEl5i"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in whos_whichs_commas_delete['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "6zbpLCroFh67"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thans_commas_delete = []\n",
        "for el in commas_delete.values:\n",
        "  if str(el[7][int(el[2])+1]) == 'than':\n",
        "    thans_commas_delete.append(el)"
      ],
      "metadata": {
        "id": "LT0xMsQIXGgd"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thans_commas_delete = pandas.DataFrame(thans_commas_delete)\n",
        "thans_commas_delete.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "thans_commas_delete = thans_commas_delete.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "9smdWAzaXLil"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in thans_commas_delete.values:\n",
        "#  change_relative_comma(el)"
      ],
      "metadata": {
        "id": "rpiUt0H-Ftr8"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "commas_delete = commas_delete[commas_delete['corrected_text'].apply(lambda x: x not in thans_commas_delete['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "b4pTWQE_Fy-d"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in thans_commas_delete['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "IwG_WppiXdxi"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conjunctions = []\n",
        "for el in commas_delete.values:\n",
        "  if el[8][int(el[2])+1][3] == 'advmod':\n",
        "    conjunctions.append(el)"
      ],
      "metadata": {
        "id": "GYHWtUrqbkXD"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conjunctions = pandas.DataFrame(conjunctions)\n",
        "conjunctions.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "conjunctions = conjunctions.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "ZN97Xnjlixh5"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in conjunctions.values:\n",
        "#  change_relative_comma(el)"
      ],
      "metadata": {
        "id": "6FrxaaXliyjW"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "commas_delete = commas_delete[commas_delete['corrected_text'].apply(lambda x: x not in conjunctions['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "eCIzjb1si50t"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in conjunctions['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "G6N8Ox-0kLpw"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sconjs = []\n",
        "for el in commas_delete.values:\n",
        "  if el[8][int(el[2])+1][1] == 'SCONJ':\n",
        "    sconjs.append(el)"
      ],
      "metadata": {
        "id": "DOQGVuBRkhpX"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sconjs = pandas.DataFrame(sconjs)\n",
        "sconjs.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "sconjs = sconjs.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "tAab7Naon_Is"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in sconjs.values:\n",
        "#  change_relative_comma(el)"
      ],
      "metadata": {
        "id": "3p3i5B1hoFg-"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "commas_delete = commas_delete[commas_delete['corrected_text'].apply(lambda x: x not in sconjs['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "U8IRp_M9oNdN"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in sconjs['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "upRAMayuoYnL"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def others_delete_commas(sample):\n",
        "\n",
        "  tag = 'punctuation'\n",
        "  pprint(\"USER'S TEXT: \" + sample[6])\n",
        "  pprint('CORRECTED TEXT: ' + str(sample[9]))\n",
        "  i = list(sample[7].keys())[0]\n",
        "  index1 = list(sample[7].keys())[0]\n",
        "  while i != int(sample[2])-1:\n",
        "    i += 1\n",
        "    if sample[7][i] == ',':\n",
        "      index1 = i +1\n",
        "\n",
        "  index2 = int(sample[2]) + 1\n",
        "  indexes = range(index1, index2)\n",
        "\n",
        "  error_area = []\n",
        "  for b in indexes:\n",
        "    error_area.append(str(sample[7][b]))\n",
        "\n",
        "  pprint('CORRECTION: ' + ' '.join(error_area[0:-1]))\n",
        "  pprint('ERROR AREA: '+ ' '.join(error_area))\n",
        "  pprint('TAG: ' + tag)"
      ],
      "metadata": {
        "id": "zIzU5J4morx9"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in commas_delete.values:\n",
        "#  others_delete_commas(el)"
      ],
      "metadata": {
        "id": "reTeThINobNo"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in commas_delete['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "twjZan5mow0J"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adding_commas = []\n",
        "for el in punctuation.values:\n",
        "  if str(el[4])[-1] != ',':\n",
        "    if '.' not in str(el[4]):\n",
        "      if str(el[4])[-1] != '.':\n",
        "        if str(el[5]).endswith(','):\n",
        "          adding_commas.append(el)"
      ],
      "metadata": {
        "id": "L1KmohFbqweD"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Добавление запятых"
      ],
      "metadata": {
        "id": "xY-5aJxquUs5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вводные слова"
      ],
      "metadata": {
        "id": "cAEtjys-v7VW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adding_commas = pandas.DataFrame(adding_commas)\n",
        "adding_commas.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "adding_commas = adding_commas.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "AmYON7Y9tiWk"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_constructions = ['Besides that', 'Besides', 'To my mind', 'On the one hand', 'On the other hand', 'As a result', 'As a result of', 'As a consequence', 'As a consequence of', \n",
        "                       'First of all', 'First and foremost', 'Last but not least', 'Finally', 'For instance', 'For example', 'For this reason', 'Hence', 'Consequently', 'To sum up', \n",
        "                       'To conclude', 'To begin with', 'To start with', 'In addition', 'In addition to that', 'In addition to', 'At the same time', 'In conclusion', 'All in all', 'As can be seen', \n",
        "                       'As we can see', 'According to', 'Due to', 'Thanks to', 'What is more', 'Speaking about', 'Speaking of', 'Moreover', 'Furthermore', 'However', 'Thus', 'Therefore', \n",
        "                       'In contrast to', 'In contrast', 'Taking into consideration', 'Finally', 'To make the long story short', 'To cut the long story short', 'Surprisingly', 'Unsurprisingly', \n",
        "                       'Undoubtedly', 'Overwhelmingly', 'On rare occasions', 'Infrequently', 'In spite of', 'Despite', 'Having said that', 'As if that was', 'As if it was', 'As if this was', \n",
        "                       'To tell the truth', 'To be honest', 'To be fair', 'In truth', 'In all honesty', 'In all fairness', 'In all likelihood', 'In all seriousness', 'Without loss of generality', \n",
        "                       'With all my heart', 'With due respect', 'With all due respect', 'Without any reservation', 'Without further ado', 'Without any ado', 'Without any doubt', 'Without a doubt',\n",
        "                       'Undoubtedly', 'Unanimously', 'Inadvertently', 'By chance', 'By accident', 'Just by accident', 'Just by chance' 'In the nick of time', 'In a jiffy', 'At the last minute', \n",
        "                       'At the last moment', 'At the earliest convenience', 'At the earliest opportunity', 'At the earliest possible time','Firstly','Secondly','Thirdly', 'As an example', 'In fact',\n",
        "                       'This is why', 'In some way', 'By the way', 'In this way', 'So that way', 'So this way', 'From my point of view', 'For this point of view', 'And from this view', 'In my view',\n",
        "                       'For many years', 'Also', 'Meanwhile', 'In our progressive world','So','In other words', 'In that case', 'In this case', 'In these cases', 'In general', 'Further',\n",
        "                       'Of course', 'These days', 'Generally', 'Actually', 'For me', 'Overall', 'Nevertheless', 'Namely', 'Otherwise', 'Clearly','Futhermore','Recently','From my perspective', \n",
        "                       'Personally', 'Probably', 'Afterwards', 'Later', 'Now', 'Then', 'There', 'Inspite of that', 'Nowadays']"
      ],
      "metadata": {
        "id": "HufqHsniuUVu"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "introductory_constructions = []\n",
        "for k in first_constructions:\n",
        "  for el in adding_commas.values:\n",
        "    sample = el[9].split(',')[0]\n",
        "    if k == sample:\n",
        "      if len(el[6].split(k)) == 2:\n",
        "        if el[6].split(k)[1].startswith(','):\n",
        "          pass\n",
        "        else:\n",
        "          introductory_constructions.append(el)\n",
        "    else:\n",
        "      pass"
      ],
      "metadata": {
        "id": "25Ca-0Gcuu7C"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "introductory_constructions = pandas.DataFrame(introductory_constructions)\n",
        "introductory_constructions.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "introductory_constructions = introductory_constructions.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "Vy2uX30qvFlC"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def change_introductory_constructions_1(sample):\n",
        "  tag = 'punctuation'\n",
        "  index = int(sample[2])\n",
        "\n",
        "  pprint(\"USER'S TEXT: \" + sample[6])\n",
        "  pprint('CORRECTED TEXT: ' + str(sample[9]))\n",
        "  \n",
        "  indexes = list(range(int(list(sample[7].keys())[0]),int(sample[2])+1))\n",
        "\n",
        "  pprint('CORRECTION: ' + str(sample[9].split(',')[0]) + ',')\n",
        "\n",
        "  error_area = [] # при внедрении в хептабот стоит пользоваться только этими индексами для выделения области ошибки\n",
        "  for el in indexes:\n",
        "    error_area.append(sample[7][el])\n",
        "  pprint('ERROR AREA: '+ ' '.join(error_area))\n",
        "  pprint('TAG: ' + tag)"
      ],
      "metadata": {
        "id": "Py_pbM-mvR0i"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in introductory_constructions.values:\n",
        "#  change_introductory_constructions_1(el)"
      ],
      "metadata": {
        "id": "vTQVgKaTvY8o"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adding_commas = adding_commas[adding_commas['corrected_text'].apply(lambda x: x not in introductory_constructions['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "jGJL0-_mv1MK"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in introductory_constructions['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "jWT9B62lvfhW"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "introductory2 = [i for i in first_constructions if i != 'There']"
      ],
      "metadata": {
        "id": "_Y7Jrl26vzM_"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "introductory_inside = []\n",
        "for i in introductory2:\n",
        "  for el in adding_commas.values:\n",
        "    if str(el[4]) == i.lower():\n",
        "      if str(el[4]) + ',' == str(el[5]):\n",
        "        introductory_inside.append(el)\n",
        "    elif str(el[7][int(el[2])+1]) == i.lower():\n",
        "      introductory_inside.append(el)"
      ],
      "metadata": {
        "id": "nPKSpBuxNy7d"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "introductory_inside = pandas.DataFrame(introductory_inside)\n",
        "introductory_inside.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "introductory_inside = introductory_inside.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "PIejnvdjN0bU"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def change_introductory_inside(sample):\n",
        "  tag = 'punctuation'\n",
        "  pprint(\"USER'S TEXT: \" + sample[6])\n",
        "  pprint('CORRECTED TEXT: ' + str(sample[9]))\n",
        "\n",
        "  for i in introductory2:\n",
        "    if str(sample[4]) == i.lower():\n",
        "        if str(el[4]) + ',' == str(el[5]):\n",
        "\n",
        "          i = list(sample[7].keys())[0]\n",
        "          index1 = list(sample[7].keys())[0]\n",
        "          while i != el[2]:\n",
        "            i += 1\n",
        "            if el[7][i] == ',':\n",
        "              index1 = i + 1\n",
        "\n",
        "          index2 = int(el[2]) + 1\n",
        "          indexes = range(index1, index2)\n",
        "\n",
        "          error_area = []\n",
        "          for b in indexes:\n",
        "            error_area.append(str(el[7][b]))\n",
        "\n",
        "          pprint('CORRECTION: ' + ' '.join(error_area) + ',')\n",
        "          pprint('ERROR AREA: '+ ' '.join(error_area))\n",
        "          pprint('TAG: ' + tag)\n",
        "\n",
        "    elif str(sample[7][int(sample[2])+1]) == i.lower():\n",
        "      index =int(sample[2])+1\n",
        "      \n",
        "      pprint('CORRECTION: ' + str(sample[7][index]) + ',')\n",
        "      pprint('ERROR AREA: '+ str(sample[7][index]))\n",
        "      pprint('TAG: ' + tag)\n",
        "  "
      ],
      "metadata": {
        "id": "JxiDCy90OF89"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in introductory_inside.values:\n",
        "#  change_introductory_inside(el)"
      ],
      "metadata": {
        "id": "obAl5PoLOOpZ"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adding_commas = adding_commas[adding_commas['corrected_text'].apply(lambda x: x not in introductory_inside['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "DAbs3NAuOQyo"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in introductory_inside['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "U8Rr7OpStXUb"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_modern_world_adding_commas = []\n",
        "for el in adding_commas.values:\n",
        "  sample = el[9].split(',')\n",
        "  try:\n",
        "    if sample[0] == 'In modern world':\n",
        "      if len(el[6].split(',')[0].split()) > 3:\n",
        "        in_modern_world_adding_commas.append(el)\n",
        "    elif sample[0] == 'In the modern world':\n",
        "      if len(el[6].split(',')[0].split()) > 4:\n",
        "        in_modern_world_adding_commas.append(el)\n",
        "    elif sample[0] == 'In a modern world':\n",
        "      if len(el[6].split(',')[0].split()) > 4:\n",
        "        in_modern_world_adding_commas.append(el)\n",
        "    elif sample[1] == ' in a modern world':\n",
        "      if len(el[6].split(',')[1].split()) > 4:\n",
        "          in_modern_world_adding_commas.append(el)\n",
        "    elif sample[1] == ' in the modern world':\n",
        "      if len(el[6].split(',')[1].split()) > 4:\n",
        "          in_modern_world_adding_commas.append(el)\n",
        "    elif sample[1] == ' in modern world':\n",
        "      if len(el[6].split(',')[1].split()) > 3:\n",
        "          in_modern_world_adding_commas.append(el)\n",
        "    elif sample[1] == ' in our modern world':\n",
        "      if len(el[6].split(',')[1].split()) > 4:\n",
        "          in_modern_world_adding_commas.append(el)\n",
        "    elif sample[1] == ' In our modern world':\n",
        "      if len(el[6].split(',')[1].split()) > 4:\n",
        "          in_modern_world_adding_commas.append(el)     \n",
        "    else:\n",
        "      pass\n",
        "  except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "5BbXqrpDtY63"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_modern_world_adding_commas = pandas.DataFrame(in_modern_world_adding_commas)\n",
        "in_modern_world_adding_commas.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "in_modern_world_adding_commas = in_modern_world_adding_commas.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "yYIU5lt5udMi"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def change_modern_world_add_comma(sample):\n",
        "\n",
        "  tag = 'punctuation'\n",
        "\n",
        "  pprint(\"USER'S TEXT: \" + sample[6])\n",
        "  pprint('CORRECTED TEXT: ' + str(sample[9]))\n",
        "  \n",
        "\n",
        "  i = list(sample[7].keys())[0]\n",
        "  index1 = list(sample[7].keys())[0]\n",
        "  while i != el[2]:\n",
        "    i += 1\n",
        "    if el[7][i] == ',':\n",
        "      index1 = i + 1\n",
        "\n",
        "  index2 = int(el[2]) + 1\n",
        "  indexes = range(index1, index2)\n",
        "\n",
        "  error_area = []\n",
        "  for b in indexes:\n",
        "    error_area.append(str(el[7][b]))\n",
        "\n",
        "  pprint('CORRECTION: ' + ' '.join(error_area) + ',')\n",
        "  pprint('ERROR AREA: '+ ' '.join(error_area))\n",
        "  pprint('TAG: ' + tag)"
      ],
      "metadata": {
        "id": "9AfbTrI4uh13"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in in_modern_world_adding_commas.values:\n",
        "#  change_modern_world_add_comma(el)"
      ],
      "metadata": {
        "id": "HuwVoQSNuna8"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in in_modern_world_adding_commas['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "OZu9voekupOg"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adding_commas = adding_commas[adding_commas['corrected_text'].apply(lambda x: x not in in_modern_world_adding_commas['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "Dq7FlErpvogQ"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gerund_after_commas = []\n",
        "for el in adding_commas.values:\n",
        "  try:\n",
        "    element = str(el[4])\n",
        "    f = re.findall(r'[a-zA-Z]+', element)\n",
        "    a = re.findall(r'\\d+', element)\n",
        "    k = re.findall(r'\\W', element)\n",
        "    all = f+a+k\n",
        "    if ' ' in all:\n",
        "      all.remove(' ')\n",
        "\n",
        "    length = len(all)\n",
        "    index2 = int(el[2]) + int(length) \n",
        "\n",
        "    if el[8][index2][2] == 'VBG':\n",
        "      gerund_after_commas.append(el)\n",
        "\n",
        "  except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "au9ar0DAHa1D"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gerund_after_commas = pandas.DataFrame(gerund_after_commas)\n",
        "gerund_after_commas.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "gerund_after_commas = gerund_after_commas.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "6sm8Sw2wJsp5"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in gerund_after_commas.values:\n",
        "#  change_modern_world_add_comma(el)"
      ],
      "metadata": {
        "id": "7reMUgVJWTnf"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in gerund_after_commas['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "qbdywBmNd8Ul"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adding_commas = adding_commas[adding_commas['corrected_text'].apply(lambda x: x not in gerund_after_commas['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "VVBLfofRd_Uw"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relative_clauses2 = []\n",
        "for el in adding_commas.values:\n",
        "  try:\n",
        "    element = str(el[4])\n",
        "    ind = int(el[2])\n",
        "    f = re.findall(r'[a-zA-Z]+', element)\n",
        "    a = re.findall(r'\\d+', element)\n",
        "    k = re.findall(r'\\W', element)\n",
        "    all = f+a+k\n",
        "    if ' ' in all:\n",
        "      all.remove(' ')\n",
        "\n",
        "    length = len(all)\n",
        "    index = int(el[2]) + int(length)\n",
        "\n",
        "\n",
        "    if el[8][index][0] == 'which':\n",
        "      relative_clauses2.append(el)\n",
        "    elif el[8][index][0] == 'who':\n",
        "      relative_clauses2.append(el)\n",
        "\n",
        "\n",
        "    elif el[7][ind+1] == 'which':\n",
        "      relative_clauses2.append(el)\n",
        "    elif el[7][ind+1] == 'who':\n",
        "      relative_clauses2.append(el)\n",
        "  except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "UuR_BWQWeNvY"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relative_clauses2 = pandas.DataFrame(relative_clauses2)\n",
        "relative_clauses2.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "relative_clauses2 = relative_clauses2.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "9qLyByFzfBcx"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def change_relative(sample):\n",
        "  tag = 'relative clause'\n",
        "\n",
        "  pprint(\"USER'S TEXT: \" + sample[6])\n",
        "  pprint('CORRECTED TEXT: ' + str(sample[9]))\n",
        "\n",
        "  element = str(sample[4])\n",
        "  f = re.findall(r'[a-zA-Z]+', element)\n",
        "  a = re.findall(r'\\d+', element)\n",
        "  k = re.findall(r'\\W', element)\n",
        "  all = f+a+k\n",
        "  \n",
        "  if ' ' in all:\n",
        "    all.remove(' ')\n",
        "\n",
        "  length = len(all)\n",
        "  index = int(sample[2]) + int(length) \n",
        "\n",
        "  pprint('CORRECTION: ' + ',' + ' ' + str(sample[7][index]))\n",
        "  pprint('ERROR AREA: '+ str(sample[7][index]))\n",
        "  pprint('TAG: ' + tag)"
      ],
      "metadata": {
        "id": "4QmULWIYfE4T"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in relative_clauses2.values:\n",
        " # change_relative(el)"
      ],
      "metadata": {
        "id": "RY34-NCDr6TO"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in relative_clauses2['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "ZHKJBGbith3g"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adding_commas = adding_commas[adding_commas['corrected_text'].apply(lambda x: x not in relative_clauses2['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "A_NhxfVltiSV"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "that_clauses = []\n",
        "for el in adding_commas.values:\n",
        "  index = int(el[2])\n",
        "\n",
        "  if el[7][index+1] == 'that':\n",
        "    that_clauses.append(el)"
      ],
      "metadata": {
        "id": "ZgEH3TvBtmpf"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "that_clauses = pandas.DataFrame(that_clauses)\n",
        "that_clauses.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "that_clauses = that_clauses.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "HvihPz-XvB_1"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def change_that(sample):\n",
        "  tag = 'relative clause'\n",
        "  pprint(\"USER'S TEXT: \" + sample[6])\n",
        "  pprint('CORRECTED TEXT: ' + str(sample[9]))\n",
        "\n",
        "  element = str(sample[4])\n",
        "  f = re.findall(r'[a-zA-Z]+', element)\n",
        "  a = re.findall(r'\\d+', element)\n",
        "  k = re.findall(r'\\W', element)\n",
        "  all = f+a+k\n",
        "\n",
        "  if ' ' in all:\n",
        "    all.remove(' ')\n",
        "  \n",
        "  \n",
        "  length = len(all)\n",
        "\n",
        "  ind = int(sample[2]) + int(length)\n",
        "  #nsubj\n",
        "  if sample[8][ind][3] == 'nsubj':\n",
        "    i = list(sample[7].keys())[0]\n",
        "    index1 = list(sample[7].keys())[0]\n",
        "    while i != el[2]:\n",
        "      i += 1\n",
        "      if el[7][i] == ',':\n",
        "        index1 = i + 1\n",
        "\n",
        "    index2 = int(el[2]) + 1\n",
        "    indexes = range(index1, index2)\n",
        "\n",
        "    error_area = []\n",
        "    for b in indexes:\n",
        "      error_area.append(str(el[7][b]))\n",
        "  \n",
        "\n",
        "    pprint('CORRECTION: ' + ' '.join(error_area) + ',')\n",
        "    pprint('ERROR AREA: '+ ' '.join(error_area))\n",
        "\n",
        "  #dobj\n",
        "  elif sample[8][ind][3] == 'dobj':\n",
        "    print('the comma is not needed')\n",
        "\n",
        "  #det\n",
        "  elif sample[8][ind][3] == 'det':\n",
        "    i = list(sample[7].keys())[0]\n",
        "    index1 = list(sample[7].keys())[0]\n",
        "    while i != el[2]:\n",
        "      i += 1\n",
        "      if el[7][i] == ',':\n",
        "        index1 = i + 1\n",
        "\n",
        "    index2 = int(el[2]) + 1\n",
        "    indexes = range(index1, index2)\n",
        "\n",
        "    error_area = []\n",
        "    for b in indexes:\n",
        "      error_area.append(str(el[7][b]))\n",
        "    \n",
        "    pprint('CORRECTION: ' + ' '.join(error_area) + ',')\n",
        "    pprint('ERROR AREA: '+ ' '.join(error_area))\n",
        "    \n",
        "  # prep\n",
        "  elif sample[8][ind][3] == 'prep':\n",
        "    error_area = sample[4]\n",
        "    correction = sample[5]\n",
        "\n",
        "    pprint('CORRECTION: ' + correction)\n",
        "    pprint('ERROR AREA: '+ error_area)\n",
        "    \n",
        "  elif sample[8][ind][3] == 'mark':\n",
        "    print('strange thing')\n",
        "\n",
        "  print('\\n')"
      ],
      "metadata": {
        "id": "ghkYh9ERv5WU"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in that_clauses.values:\n",
        "#  change_that(el)"
      ],
      "metadata": {
        "id": "Bs31U41II6f2"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in that_clauses['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "grEVPeQnKWZE"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adding_commas = adding_commas[adding_commas['corrected_text'].apply(lambda x: x not in that_clauses['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "4dyHPKHTKfxk"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_gerunds = []\n",
        "for el in adding_commas.values:\n",
        "  gerunds = []\n",
        "  index1 = list(el[8].keys())[0]\n",
        "  index2 = int(el[2])\n",
        "  indexes = range(index1, index2)\n",
        "  for ind in indexes:\n",
        "    gerunds.append(el[-1][ind][-1])\n",
        "\n",
        "  if el[-1][index1][2] == 'VBG':\n",
        "    if el[-1][index1][3] != 'csubj':\n",
        "      if el[6].split(',')[0] != el[9].split(',')[0]:\n",
        "        if 'nsubj' not in gerunds:\n",
        "          first_gerunds.append(el)"
      ],
      "metadata": {
        "id": "05y5TmHkLCJ9"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_gerunds = pandas.DataFrame(first_gerunds)\n",
        "first_gerunds.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "first_gerunds = first_gerunds.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "-fA2WrSMPMrG"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def change_gerund(sample):\n",
        "  tag = 'punctuation'\n",
        "\n",
        "  pprint(\"USER'S TEXT: \" + sample[6])\n",
        "  pprint('CORRECTED TEXT: ' + str(sample[9]))\n",
        "  \n",
        "  error_area = []\n",
        "  for k, v in sample[7].items():\n",
        "    if k != int(sample[2])+1:\n",
        "      error_area.append(str(v))\n",
        "    else:\n",
        "      break\n",
        "\n",
        "  pprint('CORRECTION: ' + ' '.join(error_area) + ',')\n",
        "  pprint('ERROR AREA: '+ ' '.join(error_area))\n",
        "  pprint('TAG: ' + tag)"
      ],
      "metadata": {
        "id": "BxrmxiLZPV22"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in first_gerunds.values:\n",
        " # change_gerund(el)"
      ],
      "metadata": {
        "id": "l5k-dhw0PXky"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in first_gerunds['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "DquQPI-mPiOi"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adding_commas = adding_commas[adding_commas['corrected_text'].apply(lambda x: x not in first_gerunds['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "cH3EgMhZPkXP"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conjuncs = ['and', 'where', 'while', 'so', 'wherears', 'but', 'even', 'although', 'as', 'then', 'whether', 'like', 'since', 'because', 'when', 'whilst', 'regardless', 'especially']"
      ],
      "metadata": {
        "id": "qF0mFa4PPqhk"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conjuncs_df = []\n",
        "for conj in conjuncs:\n",
        "  try:\n",
        "    for el in adding_commas.values:\n",
        "      element = str(el[4])\n",
        "      f = re.findall(r'[a-zA-Z]+', element)\n",
        "      j = re.findall(r'\\d+[,.]\\d+',element)\n",
        "      l = re.findall(r'[%]+', element)\n",
        "      change = re.findall(r\"[\\w']+|[.,!?;()%-]\", str(element))\n",
        "      if len(j) == 0:\n",
        "        all = change\n",
        "      else:\n",
        "\n",
        "        all = f+j+l\n",
        "\n",
        "      if ' ' in all:\n",
        "        all.remove(' ')\n",
        "      \n",
        "      length = len(all)\n",
        "\n",
        "      if el[7][int(el[2])+length] == conj:\n",
        "        if el[7][int(el[2])+length+1] != 'of':\n",
        "          conjuncs_df.append(el)\n",
        "  except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "TZao9UwRRRXj"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conjuncs_df = pandas.DataFrame(conjuncs_df)\n",
        "conjuncs_df.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "conjuncs_df = conjuncs_df.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "fIfh4bVWRRZ4"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def change_conj(sample):\n",
        "  tag = 'punctuation'\n",
        "  marks = string.punctuation \n",
        "  if str(sample[4]) + ',' != str(sample[5]):\n",
        "    if str(sample[4]) in marks:\n",
        "      pprint(\"USER'S TEXT: \" + sample[6])\n",
        "      pprint('CORRECTED TEXT: ' + str(sample[9]))\n",
        "      \n",
        "      error_area = sample[7][int(sample[2])] + ' ' + sample[7][int(sample[2]+1)] \n",
        "      correction = sample[10][int(sample[2])] + ' ' + sample[10][int(sample[2]+1)] \n",
        "\n",
        "      pprint('CORRECTION: ' + correction)\n",
        "      pprint('ERROR AREA: '+ error_area)\n",
        "      pprint('TAG: ' + tag)\n",
        "\n",
        "  else:\n",
        "    pprint(\"USER'S TEXT: \" + sample[6])\n",
        "    pprint('CORRECTED TEXT: ' + str(sample[9]))\n",
        "    \n",
        "    error_area = sample[7][int(sample[2]+1)]\n",
        "\n",
        "    pprint('CORRECTION: ' + ', ' + error_area)\n",
        "    pprint('ERROR AREA: '+ error_area)\n",
        "    pprint('TAG: ' + tag)"
      ],
      "metadata": {
        "id": "_3NFGadbSVwH"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in conjuncs_df.values:\n",
        "#  change_conj(el)"
      ],
      "metadata": {
        "id": "4BKMRkHgSWGg"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in conjuncs_df['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "cUIG0GKGSg1v"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adding_commas = adding_commas[adding_commas['corrected_text'].apply(lambda x: x not in conjuncs_df['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "AzqfHa7SSifP"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conjuncs_df2 = []\n",
        "for el in adding_commas.values:\n",
        "  try:\n",
        "    element = str(el[4])\n",
        "    f = re.findall(r'[a-zA-Z]+', element)\n",
        "    j = re.findall(r'\\d+[,.]\\d+',element)\n",
        "    l = re.findall(r'[%]+', element)\n",
        "    change = re.findall(r\"[\\w']+|[.,!?;()%-]\", str(element))\n",
        "    if len(j) == 0:\n",
        "      all = change\n",
        "    else:\n",
        "\n",
        "      all = f+j+l\n",
        "\n",
        "    if ' ' in all:\n",
        "      all.remove(' ')\n",
        "    \n",
        "    length = len(all)\n",
        "\n",
        "    if el[7][int(el[2])+length] == 'such':\n",
        "      if el[7][int(el[2])+length+1] == 'as':\n",
        "        conjuncs_df2.append(el)\n",
        "    elif el[7][int(el[2])+length] == 'because':\n",
        "      if el[7][int(el[2])+length+1] == 'of':\n",
        "        conjuncs_df2.append(el)\n",
        "    elif el[7][int(el[2])+length] == 'of':\n",
        "      if el[7][int(el[2])+length+1] == 'course':\n",
        "        conjuncs_df2.append(el)\n",
        "  except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "4OCb8h1YSl-L"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conjuncs_df2 = pandas.DataFrame(conjuncs_df2)\n",
        "conjuncs_df2.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "conjuncs_df2 = conjuncs_df2.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "uq5K2OXFS9K0"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def change_conj2(sample):\n",
        "  tag = 'punctuation'\n",
        "  marks = string.punctuation \n",
        "  if str(sample[4]) + ',' != str(sample[5]):\n",
        "    if str(sample[4]) in marks:\n",
        "      pprint(\"USER'S TEXT: \" + sample[6])\n",
        "      pprint('CORRECTED TEXT: ' + str(sample[9]))\n",
        "      indexes = [sample[7][int(sample[2])] , sample[7][int(sample[2]+1)] , sample[7][int(sample[2]+2)] ]\n",
        "      \n",
        "      error_area = sample[7][int(sample[2])] + ' ' + sample[7][int(sample[2]+1)] + ' ' + sample[7][int(sample[2]+2)] \n",
        "      correction = sample[10][int(sample[2])] + ' ' + sample[10][int(sample[2]+1)] + ' ' + sample[10][int(sample[2]+2)] \n",
        "\n",
        "      pprint('CORRECTION: ' + correction)\n",
        "      pprint('ERROR AREA: '+ error_area)\n",
        "      pprint('TAG: ' + tag)\n",
        "\n",
        "  else:\n",
        "    pprint(\"USER'S TEXT: \" + sample[6])\n",
        "    pprint('CORRECTED TEXT: ' + str(sample[9]))\n",
        "    indexes = [sample[7][int(sample[2]+1)], sample[7][int(sample[2]+2)] ]\n",
        "    \n",
        "    error_area =  sample[7][int(sample[2]+1)] + ' ' + sample[7][int(sample[2]+2)] \n",
        "\n",
        "    pprint('CORRECTION: ' + ', ' + error_area)\n",
        "    pprint('ERROR AREA: '+ error_area)\n",
        "    pprint('TAG: ' + tag)"
      ],
      "metadata": {
        "id": "-UpQSfJGTEod"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in conjuncs_df2.values:\n",
        " # change_conj2(el)"
      ],
      "metadata": {
        "id": "jZ3o_HCPS7tW"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in conjuncs_df2['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "gVLzUJtvTbss"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adding_commas = adding_commas[adding_commas['corrected_text'].apply(lambda x: x not in conjuncs_df2['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "q5x96kiqTdus"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preps2 = []\n",
        "for el in adding_commas.values:\n",
        "  if el[6].split(',')[0] != el[9].split(',')[0]:\n",
        "\n",
        "    if len(el[9].split(',')[0].split()) < 9:\n",
        "      index1 = list(el[8].keys())[0]\n",
        "      index2 = int(el[2])\n",
        "      indexes = range(index1,index2)\n",
        "      preps_list = []\n",
        "      for ind in indexes:  \n",
        "        preps_list.append(el[8][ind][3])\n",
        "\n",
        "\n",
        "      if 'prep' in preps_list:\n",
        "        preps2.append(el)\n"
      ],
      "metadata": {
        "id": "6AILZY2kvpqB"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preps2 = pandas.DataFrame(preps2)\n",
        "preps2.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "preps2 = preps2.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "FzYAOWBRUiCL"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def change_preps2(sample):\n",
        "  tag = 'punctuation'\n",
        "\n",
        "  pprint(\"USER'S TEXT: \" + sample[6])\n",
        "  pprint('CORRECTED TEXT: ' + str(sample[9]))\n",
        "  \n",
        "  if str(sample[4]).startswith(','):\n",
        "    if str(sample[5]).endswith(','):\n",
        "      index1 = int(list(sample[7].keys())[0])\n",
        "      index2 = int(sample[2]) + 2\n",
        "      indexes = range(index1, index2)\n",
        "      error_area = []\n",
        "      correction = []\n",
        "      for i in indexes:\n",
        "        error_area.append(str(sample[7][i]))\n",
        "        correction.append(str(sample[10][i]))\n",
        "  \n",
        "      pprint('CORRECTION: ' + ' '.join(correction[0:-1])+',')\n",
        "      pprint('ERROR AREA: '+ ' '.join(error_area))\n",
        "      pprint('TAG: ' + tag)\n",
        "\n",
        "  else:\n",
        "    index1 = int(list(sample[7].keys())[0])\n",
        "    index2 = int(sample[2]) + 1\n",
        "    indexes = range(index1, index2)\n",
        "    error_area = []\n",
        "    for i in indexes:\n",
        "      error_area.append(str(sample[7][i]))\n",
        "\n",
        "    pprint('CORRECTION: ' + ' '.join(error_area) + ',')\n",
        "    pprint('ERROR AREA: '+ ' '.join(error_area))\n",
        "    pprint('TAG: ' + tag)"
      ],
      "metadata": {
        "id": "QnR6ng-KT8GJ"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in preps2.values:\n",
        "#  change_preps2(el)"
      ],
      "metadata": {
        "id": "gdBL7hKNU4fZ"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in preps2['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "vHbIdPcyX84B"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adding_commas = adding_commas[adding_commas['corrected_text'].apply(lambda x: x not in preps2['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "YdtGsvlQYB0G"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ins = []\n",
        "for el in adding_commas.values:\n",
        "  ind = list(el[7].keys())[0]\n",
        "  if el[6].split(',')[0] != el[9].split(',')[0]:\n",
        "    if len(el[9].split(',')[0].split()) < 6:\n",
        "      if el[8][ind][0] == 'In':\n",
        "        ins.append(el)"
      ],
      "metadata": {
        "id": "P-LqYl4gTyK_"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ins = pandas.DataFrame(ins)\n",
        "ins.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "ins = ins.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "fEOI8o67Ty1s"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in ins.values:\n",
        "#  change_preps2(el)"
      ],
      "metadata": {
        "id": "Kddo8HhjU-JX"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in ins['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "C9vdceA1Ub8X"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adding_commas = adding_commas[adding_commas['corrected_text'].apply(lambda x: x not in ins['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "4jWr2a6zUdbp"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_to_infs = []\n",
        "for el in adding_commas.values:\n",
        "  ind = list(el[8].keys())[0]  \n",
        "  if el[6].split(',')[0] != el[9].split(',')[0]:\n",
        "    if el[8][ind][0] == 'To':\n",
        "      if el[8][ind+1][3] == 'advcl':\n",
        "        first_to_infs.append(el)"
      ],
      "metadata": {
        "id": "0QdvQwl_ZYWG"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_to_infs = pandas.DataFrame(first_to_infs)\n",
        "first_to_infs.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "first_to_infs = first_to_infs.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "39se6TXMZeoA"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def change_first_to(sample):\n",
        "  tag = 'punctuation'\n",
        "  index1 = int(list(sample[7].keys())[0])\n",
        "  index2 = int(sample[2]) + 1\n",
        "  indexes = range(index1, index2)\n",
        "\n",
        "  pprint(\"USER'S TEXT: \" + sample[6])\n",
        "  pprint('CORRECTED TEXT: ' + str(sample[9]))\n",
        "      \n",
        "  \n",
        "  error_area = []\n",
        "  for i in indexes:\n",
        "    error_area.append(str(sample[7][i]))\n",
        "\n",
        "  pprint('CORRECTION: ' + ' '.join(error_area) + ',')\n",
        "  pprint('ERROR AREA: '+ ' '.join(error_area))\n",
        "  pprint('TAG: ' + tag)"
      ],
      "metadata": {
        "id": "6C684fqoZs81"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in first_to_infs.values:\n",
        "#  change_first_to(el)"
      ],
      "metadata": {
        "id": "bV_EM0-zZxUR"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in first_to_infs['corrected_text'].values)]\n",
        "adding_commas = adding_commas[adding_commas['corrected_text'].apply(lambda x: x not in first_to_infs['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "z5lT7JeWZy42"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "commas_dots2 = []\n",
        "for el in adding_commas.values:\n",
        "  if str(el[4]).startswith(','):\n",
        "    if str(el[5]).startswith('.'):\n",
        "      if str(el[5])[2].islower() == False:\n",
        "        commas_dots2.append(el)"
      ],
      "metadata": {
        "id": "sm7TQSCUZ-Cv"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "commas_dots2 = pandas.DataFrame(commas_dots2)\n",
        "commas_dots2.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "commas_dots2 = commas_dots2.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "yP5idug9aD8x"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def change_commas_dots2(sample):\n",
        "  pprint(\"USER'S TEXT: \" + sample[6])\n",
        "  pprint('CORRECTED TEXT: ' + str(sample[9]))\n",
        "  tag = 'punctuation'\n",
        "\n",
        "  i = list(sample[7].keys())[0]\n",
        "  index1 = list(sample[7].keys())[0]\n",
        "  while i != el[2]:\n",
        "    i += 1\n",
        "    if el[7][i] == ',':\n",
        "      index1 = i \n",
        "\n",
        "  index2 = int(el[2]) + 2\n",
        "  indexes = range(index1, index2)\n",
        "\n",
        "  error_area = []\n",
        "  for b in indexes:\n",
        "    error_area.append(str(el[7][b]))\n",
        "\n",
        "\n",
        "  pprint('CORRECTION: ' + '. ' + ' '.join(error_area[1:]).capitalize())\n",
        "  pprint('ERROR AREA: '+ ' '.join(error_area))\n",
        "  pprint('TAG: ' + tag)\n",
        "\n",
        "\n",
        "  pprint('CORRECTION: ' + str(sample[5][2:]))\n",
        "  pprint('ERROR AREA: '+ str(sample[4][2:]))\n",
        "  pprint('TAG: ' + tag)"
      ],
      "metadata": {
        "id": "Z314jXyNaGV6"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in commas_dots2.values:\n",
        "#  change_commas_dots2(el)"
      ],
      "metadata": {
        "id": "qWp5ejgYabei"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in commas_dots2['corrected_text'].values)]\n",
        "adding_commas = adding_commas[adding_commas['corrected_text'].apply(lambda x: x not in commas_dots2['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "-XZGiqFZaizn"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gerund_constructions = []\n",
        "for el in adding_commas.values:\n",
        "  index1 = list(el[7].keys())[0]\n",
        "  index2 = int(el[2])\n",
        "  i = int(el[2])\n",
        "  while i != el[2]:\n",
        "    i -= 1\n",
        "    if el[7][i] == ',':\n",
        "      index1 = i \n",
        "      #if i[2] == 'VBG':\n",
        "\n",
        "\n",
        "  indexes = range(index1, index2)\n",
        "  poss_gerunds = []\n",
        "  for i in indexes:\n",
        "    poss_gerunds.append(el[8][i][2])\n",
        "\n",
        "  if 'VBG' in poss_gerunds:\n",
        "    gerund_constructions.append(el)"
      ],
      "metadata": {
        "id": "cKNap6-5be6o"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gerund_constructions = pandas.DataFrame(gerund_constructions)\n",
        "gerund_constructions.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "gerund_constructions = gerund_constructions.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "X3CoXOA3bgeK"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def change_gerund_constructions(sample):\n",
        "  pprint(\"USER'S TEXT: \" + sample[6])\n",
        "  pprint('CORRECTED TEXT: ' + str(sample[9]))\n",
        "  tag = 'punctuation'\n",
        "\n",
        "  i = list(sample[7].keys())[0]\n",
        "  index1 = list(sample[7].keys())[0]\n",
        "  while i != el[2]:\n",
        "    i += 1\n",
        "    if el[7][i] == ',':\n",
        "      index1 = i + 1\n",
        "\n",
        "  index2 = int(el[2]) + 1\n",
        "  indexes = range(index1, index2)\n",
        "\n",
        "  error_area = []\n",
        "  for b in indexes:\n",
        "    error_area.append(str(el[7][b]))\n",
        "\n",
        "\n",
        "  pprint('CORRECTION: ' + ' '.join(error_area) + ',')\n",
        "  pprint('ERROR AREA: '+ ' '.join(error_area))\n",
        "  pprint('TAG: ' + tag)"
      ],
      "metadata": {
        "id": "NT28H7cehSIP"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in gerund_constructions.values:\n",
        "#  change_gerund_constructions(el)"
      ],
      "metadata": {
        "id": "C9misAeGhaL3"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in gerund_constructions['corrected_text'].values)]\n",
        "adding_commas = adding_commas[adding_commas['corrected_text'].apply(lambda x: x not in gerund_constructions['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "g0g-i0cfhj6a"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opinions = []\n",
        "for el in adding_commas.values:\n",
        "  if str(el[4]) == 'opinion':\n",
        "    if str(el[4]) + ',' == str(el[5]):\n",
        "      opinions.append(el)"
      ],
      "metadata": {
        "id": "HKc8Wjr7iodk"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opinions = pandas.DataFrame(opinions)\n",
        "opinions.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "opinions = opinions.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "KLOLLjj3itfN"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in opinions.values:\n",
        "#  change_gerund_constructions(el)"
      ],
      "metadata": {
        "id": "95TQd-doivYG"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in opinions['corrected_text'].values)]\n",
        "adding_commas = adding_commas[adding_commas['corrected_text'].apply(lambda x: x not in opinions['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "L8BZUuaMi1Kv"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cases = []\n",
        "for el in adding_commas.values:\n",
        "  if str(el[4]) == 'case':\n",
        "    if str(el[4]) + ',' == str(el[5]):\n",
        "      cases.append(el)\n",
        "  elif str(el[4]) == 'cases':\n",
        "    if str(el[4]) + ',' == str(el[5]):\n",
        "      cases.append(el)"
      ],
      "metadata": {
        "id": "wf6hxBL3Lraf"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cases = pandas.DataFrame(cases)\n",
        "cases.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "cases = cases.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "vHujJzgVVUQS"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in cases.values:\n",
        " # change_gerund_constructions(el)"
      ],
      "metadata": {
        "id": "PoSI6eEaVWGQ"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in cases['corrected_text'].values)]\n",
        "adding_commas = adding_commas[adding_commas['corrected_text'].apply(lambda x: x not in cases['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "Wuo7JPWsWcL3"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "verbs_after_correction = []\n",
        "for el in adding_commas.values:\n",
        "  if str(el[4]) + ',' == str(el[5]):\n",
        "    element = str(el[4])\n",
        "    f = re.findall(r'[a-zA-Z]+', element)\n",
        "    a = re.findall(r'\\d+', element)\n",
        "    k = re.findall(r'\\W', element)\n",
        "    all = f+a+k\n",
        "    if ' ' in all:\n",
        "      all.remove(' ')\n",
        "\n",
        "    length = len(all)\n",
        "    index = int(el[2]) + int(length)\n",
        "\n",
        "    if el[8][index][2] == 'VBD':\n",
        "      verbs_after_correction.append(el)"
      ],
      "metadata": {
        "id": "XwpTB57AWle5"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "verbs_after_correction = pandas.DataFrame(verbs_after_correction)\n",
        "verbs_after_correction.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "verbs_after_correction = verbs_after_correction.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "y8lzth_WZSZo"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in verbs_after_correction.values:\n",
        "#  change_gerund_constructions(el)"
      ],
      "metadata": {
        "id": "vTDyvMU_ZWwb"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in verbs_after_correction['corrected_text'].values)]\n",
        "adding_commas = adding_commas[adding_commas['corrected_text'].apply(lambda x: x not in verbs_after_correction['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "TdPjunGmZW7J"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ifs = []\n",
        "for el in adding_commas.values:\n",
        "  if str(el[4]) + ',' == str(el[5]):\n",
        "    element = str(el[4])\n",
        "    f = re.findall(r'[a-zA-Z]+', element)\n",
        "    a = re.findall(r'\\d+', element)\n",
        "    k = re.findall(r'\\W', element)\n",
        "    all = f+a+k\n",
        "    if ' ' in all:\n",
        "      all.remove(' ')\n",
        "\n",
        "    length = len(all)\n",
        "\n",
        "    index = int(el[2]) + int(length)\n",
        "\n",
        "    if el[8][index][0] == 'if':\n",
        "      ifs.append(el)\n",
        "    elif el[8][index+1][0] == 'if':\n",
        "      ifs.append(el)\n",
        "    else:\n",
        "      index1 = list(el[7].keys())[0]\n",
        "      index2 = int(el[2])\n",
        "      i = int(el[2])\n",
        "      while i != el[2]:\n",
        "        i -= 1\n",
        "        if el[7][i] == ',':\n",
        "          index1 = i - 1\n",
        "          #if i[2] == 'VBG':\n",
        "\n",
        "\n",
        "      indexes = range(index1, index2+1)\n",
        "      poss_ifs = []\n",
        "      for i in indexes:\n",
        "        poss_ifs.append(el[8][i][0])\n",
        "\n",
        "      if poss_ifs[0] == 'If':\n",
        "        ifs.append(el)\n",
        "      else:\n",
        "        try:\n",
        "          index1 = list(el[7].keys())[0]\n",
        "          index2 = int(el[2])\n",
        "          i = int(el[2])\n",
        "          while i != index1:\n",
        "            i -= 1\n",
        "            if el[7][i] == ',':\n",
        "              index1 = i \n",
        "              \n",
        "              #if i[2] == 'VBG':\n",
        "\n",
        "\n",
        "          indexes = range(index1+1, index2+1)\n",
        "          poss_ifs = []\n",
        "          for i in indexes:\n",
        "            poss_ifs.append(el[8][i][0])\n",
        "\n",
        "          if poss_ifs[0] == 'if':\n",
        "            ifs.append(el)\n",
        "        except:\n",
        "          pass\n"
      ],
      "metadata": {
        "id": "r8kylbCYZxkd"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ifs = pandas.DataFrame(ifs)\n",
        "ifs.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "ifs = ifs.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "Pp7Oz577aHhP"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in ifs.values:\n",
        "#  change_gerund_constructions(el)"
      ],
      "metadata": {
        "id": "p0H4Xh8EaKit"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in ifs['corrected_text'].values)]\n",
        "adding_commas = adding_commas[adding_commas['corrected_text'].apply(lambda x: x not in ifs['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "mDLf11pRaK2V"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pass_commas = []\n",
        "for el in adding_commas.values:\n",
        "  if str(el[4]) == '-':\n",
        "    if str(el[5]) == ',':\n",
        "      pass_commas.append(el)"
      ],
      "metadata": {
        "id": "QvBKoZRPsVUN"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pass_commas = pandas.DataFrame(pass_commas)\n",
        "pass_commas.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "pass_commas = pass_commas.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "hP6EHXy5vuEK"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def change_pass_commas(sample):\n",
        "  pprint(\"USER'S TEXT: \" + sample[6])\n",
        "  pprint('CORRECTED TEXT: ' + str(sample[9]))\n",
        "  tag = 'punctuation'\n",
        "\n",
        "  i = list(sample[7].keys())[0]\n",
        "  index1 = list(sample[7].keys())[0]\n",
        "  while i != el[2]:\n",
        "    i += 1\n",
        "    if el[7][i] == ',':\n",
        "      index1 = i + 1\n",
        "\n",
        "  index2 = int(el[2]) + 1\n",
        "  indexes = range(index1, index2)\n",
        "\n",
        "  error_area = []\n",
        "  for b in indexes:\n",
        "    error_area.append(str(el[7][b]))\n",
        "\n",
        "\n",
        "  pprint('CORRECTION: ' + ' '.join(error_area[0:-1]) + ',')\n",
        "  pprint('ERROR AREA: '+ ' '.join(error_area))\n",
        "  pprint('TAG: ' + tag)"
      ],
      "metadata": {
        "id": "fgbkqpBjv0LG"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in pass_commas.values:\n",
        "#  change_pass_commas(el)"
      ],
      "metadata": {
        "id": "OseG-rv_v6z9"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in pass_commas['corrected_text'].values)]\n",
        "adding_commas = adding_commas[adding_commas['corrected_text'].apply(lambda x: x not in pass_commas['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "KsKfG8c0v9Ee"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colon_commas = []\n",
        "for el in adding_commas.values:\n",
        "  if str(el[4]) == ':':\n",
        "    if str(el[5]) == ',':\n",
        "      colon_commas.append(el)"
      ],
      "metadata": {
        "id": "v-0y2knswEuP"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colon_commas = pandas.DataFrame(colon_commas)\n",
        "colon_commas.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "colon_commas = colon_commas.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "HHjjNhOfwF4T"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in colon_commas.values:\n",
        "#  change_pass_commas(el)"
      ],
      "metadata": {
        "id": "rIYiDeOHwHJL"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in colon_commas['corrected_text'].values)]\n",
        "adding_commas = adding_commas[adding_commas['corrected_text'].apply(lambda x: x not in colon_commas['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "vNwqwPVawMGU"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preps2 = []\n",
        "for el in adding_commas.values:\n",
        "  i = int(el[2])\n",
        "  try:\n",
        "    if str(el[4]) == el[8][i][0]:\n",
        "      if el[8][i-1][3] == 'prep':\n",
        "        preps2.append(el)\n",
        "      elif el[8][i-2][3] == 'prep':\n",
        "        preps2.append(el)\n",
        "      elif el[8][i-3][3] == 'prep':\n",
        "        preps2.append(el)\n",
        "      elif el[8][i-4][3] == 'prep':\n",
        "        preps2.append(el)\n",
        "      elif el[8][i-5][3] == 'prep':\n",
        "        preps2.append(el)\n",
        "  except:\n",
        "    pass\n",
        "\n"
      ],
      "metadata": {
        "id": "ysC8DmO_w3dT"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preps2 = pandas.DataFrame(preps2)\n",
        "preps2.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "preps2 = preps2.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "OMCoprWzxfnq"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in preps2.values:\n",
        "  #change_gerund_constructions(el)"
      ],
      "metadata": {
        "id": "N0ye4hjAxzNW"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in preps2['corrected_text'].values)]\n",
        "adding_commas = adding_commas[adding_commas['corrected_text'].apply(lambda x: x not in preps2['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "8DDAxRRCyIrG"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agos = []\n",
        "for el in adding_commas.values:\n",
        "  if str(el[4]) == 'ago':\n",
        "    if str(el[4]) +',' == str(el[5]):\n",
        "      agos.append(el)"
      ],
      "metadata": {
        "id": "TaPQCszt1UAx"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agos = pandas.DataFrame(agos)\n",
        "agos.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "agos = agos.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "8-oXAsZx1pAv"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in agos.values:\n",
        "#  change_gerund_constructions(el)"
      ],
      "metadata": {
        "id": "AdsYju611r1N"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in agos['corrected_text'].values)]\n",
        "adding_commas = adding_commas[adding_commas['corrected_text'].apply(lambda x: x not in agos['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "71IiyvVx1xLo"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thats = []\n",
        "for el in adding_commas.values:\n",
        "  if str(el[4]) == 'that':\n",
        "    if str(el[4]) +',' == str(el[5]):\n",
        "      thats.append(el)"
      ],
      "metadata": {
        "id": "NJDeutw912AM"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thats = pandas.DataFrame(thats)\n",
        "thats.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "thats = thats.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "bN6ER0iy13tS"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in thats.values:\n",
        "#  change_gerund_constructions(el)"
      ],
      "metadata": {
        "id": "uRRjvhjk2JYW"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in thats['corrected_text'].values)]\n",
        "adding_commas = adding_commas[adding_commas['corrected_text'].apply(lambda x: x not in thats['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "PVvm1WFV3H9x"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preps3 = []\n",
        "for el in adding_commas.values:\n",
        "  if str(el[4]) + ',' == str(el[5]):\n",
        "    element = str(el[4])\n",
        "    f = re.findall(r'[a-zA-Z]+', element)\n",
        "    a = re.findall(r'\\d+', element)\n",
        "    k = re.findall(r'\\W', element)\n",
        "    all = f+a+k\n",
        "    if ' ' in all:\n",
        "      all.remove(' ')\n",
        "\n",
        "    length = len(all)\n",
        "\n",
        "    index = int(el[2]) + int(length)\n",
        "\n",
        "    if el[8][index][3] == 'prep':\n",
        "      preps3.append(el)\n",
        "    elif el[8][index+1][3] == 'prep':\n",
        "      preps3.append(el)\n",
        "    else:\n",
        "\n",
        "      i = int(el[2])\n",
        "      index1 = list(el[7].keys())[0]\n",
        "      index2 = int(el[2])\n",
        "      try:\n",
        "        if el[8][i][0] !=',':\n",
        "          while el[8][i][3] != 'prep':\n",
        "            i -= 1\n",
        "            index1 = i\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        indexes = range(index1, index2)\n",
        "        poss_ifs = []\n",
        "        for i in indexes:\n",
        "          poss_ifs.append(el[8][i][3])\n",
        "          \n",
        "\n",
        "          if poss_ifs[0] == 'prep':\n",
        "            preps3.append(el)\n",
        "          elif 'prep' in poss_ifs:\n",
        "            preps3.append(el)\n",
        "      \n",
        "      except:\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "9WxFGlho6cgp"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preps3 = pandas.DataFrame(preps3)\n",
        "preps3.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "preps3 = preps3.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "ZrpkNwhMB7Lh"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def change_preps_inside(sample):\n",
        "  tag = 'punctuation'\n",
        "\n",
        "  pprint(\"USER'S TEXT: \" + sample[6])\n",
        "  pprint('CORRECTED TEXT: ' + str(sample[9]))\n",
        "  index = int(el[2])\n",
        "  if el[8][index][3] == 'prep':\n",
        "    index1 = int(el[2])\n",
        "    error_area = el[7][index1]\n",
        "\n",
        "    pprint('CORRECTION: ' + str(error_area) + ',')\n",
        "    pprint('ERROR AREA: '+ str(error_area))\n",
        "    pprint('TAG: ' + tag)\n",
        "  elif el[8][index+1][3] == 'prep':\n",
        "    index1 = int(el[2]) + 1\n",
        "    error_area = el[7][int(el[2]) + 1]\n",
        "\n",
        "    pprint('CORRECTION: ' + ', ' + str(error_area))\n",
        "    pprint('ERROR AREA: '+ str(error_area))\n",
        "    pprint('TAG: ' + tag)\n",
        "\n",
        "  elif el[8][index+2][3] == 'prep':\n",
        "    index1 = int(el[2]) + 2\n",
        "    error_area = el[7][int(el[2]) + 1]\n",
        "\n",
        "    pprint('CORRECTION: ' + ', ' + str(error_area))\n",
        "    pprint('ERROR AREA: '+ str(error_area))\n",
        "    pprint('TAG: ' + tag)\n",
        "\n",
        "  else:\n",
        "    i = int(el[2])\n",
        "    index1 = list(el[7].keys())[0]\n",
        "    index2 = int(el[2])\n",
        "    while el[8][i][3] != 'prep':\n",
        "      i -= 1\n",
        "      index1 = i \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    indexes = range(index1, index2+1)\n",
        "    error_area = []\n",
        "    for i in indexes:\n",
        "      error_area.append(el[7][i])\n",
        "    pprint('CORRECTION: ' + ' '.join(error_area) + ',')\n",
        "    pprint('ERROR AREA: '+ ' '.join(error_area))\n",
        "    pprint('TAG: ' + tag)\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "cR-zeIl_Iftt"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in preps3.values:\n",
        "#  change_preps_inside(el)"
      ],
      "metadata": {
        "id": "HJwoxRogB-_z"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in preps3['corrected_text'].values)]\n",
        "adding_commas = adding_commas[adding_commas['corrected_text'].apply(lambda x: x not in preps3['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "pETqAD266dSW"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "other_cases1 = []\n",
        "for el in adding_commas.values:\n",
        "  if (str(el[4])) +',' == str(el[5]):\n",
        "    other_cases1.append(el)\n"
      ],
      "metadata": {
        "id": "38H8uBhLe8gt"
      },
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "other_cases1 = pandas.DataFrame(other_cases1)\n",
        "other_cases1.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "other_cases1 = other_cases1.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "4YyAnCemfNtX"
      },
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in other_cases1.values:\n",
        "#  change_gerund_constructions(el)"
      ],
      "metadata": {
        "id": "lubcoIaZfTEe"
      },
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in other_cases1['corrected_text'].values)]\n",
        "adding_commas = adding_commas[adding_commas['corrected_text'].apply(lambda x: x not in other_cases1['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "j67ulbZifnsk"
      },
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def change_other_cases2(sample):\n",
        "  tag = 'punctuation'\n",
        "  element = str(el[4])\n",
        "  f = re.findall(r'[a-zA-Z]+', element)\n",
        "  a = re.findall(r'\\d+', element)\n",
        "  k = re.findall(r'\\W', element)\n",
        "  all = f+a+k\n",
        "  if ' ' in all:\n",
        "    all.remove(' ')\n",
        "\n",
        "  length = len(all)\n",
        "\n",
        "  index1 = int(el[2])\n",
        "  index2 = int(el[2]) + int(length)\n",
        "\n",
        "  indexes = range(index1, index2)\n",
        "  error_area = []\n",
        "  for i in indexes:\n",
        "    error_area.append(el[7][i])\n",
        "\n",
        "  pprint('CORRECTION: ' + str(sample[5]))\n",
        "  pprint('ERROR AREA: '+ ' '.join(error_area))\n",
        "  pprint('TAG: ' + tag)"
      ],
      "metadata": {
        "id": "9kLzpFZNf3WX"
      },
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in adding_commas.values:\n",
        "#  change_other_cases2(el)"
      ],
      "metadata": {
        "id": "vnMLh1vDgXZI"
      },
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in adding_commas['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "ltiFiLBMePLc"
      },
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dots_end = []\n",
        "for el in punctuation.values:\n",
        "  for i in string.punctuation:\n",
        "    if i not in str(el[4]):\n",
        "      if str(el[5]).endswith('.'):\n",
        "        dots_end.append(el)"
      ],
      "metadata": {
        "id": "fGgP2eDrhnDC"
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dots_end = pandas.DataFrame(dots_end)\n",
        "dots_end.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "dots_end = dots_end.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "zOUV91Jth4Tw"
      },
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def change_dots_end(sample):\n",
        "  pprint(\"USER'S TEXT: \" + sample[6])\n",
        "  pprint('CORRECTED TEXT: ' + str(sample[9]))\n",
        "  tag = 'punctuation'\n",
        "\n",
        "  element = str(el[4])\n",
        "  f = re.findall(r'[a-zA-Z]+', element)\n",
        "  a = re.findall(r'\\d+', element)\n",
        "  k = re.findall(r'\\W', element)\n",
        "  all = f+a+k\n",
        "  if ' ' in all:\n",
        "    all.remove(' ')\n",
        "\n",
        "  length = len(all)\n",
        "\n",
        "\n",
        "  i = list(sample[7].keys())[0]\n",
        "  index1 = list(sample[7].keys())[0]\n",
        "  while i != el[2]:\n",
        "    i += 1\n",
        "    if el[7][i] == ',':\n",
        "      index1 = i + 1\n",
        "\n",
        "  index2 = int(el[2]) \n",
        "  indexes = range(index1, index2+length)\n",
        "\n",
        "  error_area = []\n",
        "  for b in indexes:\n",
        "    error_area.append(str(el[7][b]))\n",
        "\n",
        "\n",
        "  pprint('CORRECTION: ' + ' '.join(error_area) + '.')\n",
        "  pprint('ERROR AREA: '+ ' '.join(error_area))\n",
        "  pprint('TAG: ' + tag)"
      ],
      "metadata": {
        "id": "UloI2wqj02rm"
      },
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in dots_end.values:\n",
        "#  change_dots_end(el)"
      ],
      "metadata": {
        "id": "D3tneO0Z0_35"
      },
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in dots_end['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "XJKzBjV71n9P"
      },
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "poor_cases = []\n",
        "for el in punctuation.values:\n",
        "  if isinstance(el[4],float):\n",
        "    el4 = str(int(el[4]))\n",
        "  else:\n",
        "    el4 = str(el[4])\n",
        "  if el4 == str(el[5]):\n",
        "    poor_cases.append(el)"
      ],
      "metadata": {
        "id": "3LMq3lz81rTi"
      },
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "poor_cases = pandas.DataFrame(poor_cases)\n",
        "poor_cases.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "poor_cases = poor_cases.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "rXDW-2cM2JYG"
      },
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in poor_cases['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "lHmv6bi-3w0D"
      },
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dots_start = []\n",
        "for el in punctuation.values:\n",
        "  element = str(el[4])\n",
        "  f = re.findall(r'[a-zA-Z]+', element)\n",
        "  a = re.findall(r'\\d+', element)\n",
        "  k = re.findall(r'\\W', element)\n",
        "  all = f+a+k\n",
        "  if ' ' in all:\n",
        "    all.remove(' ')\n",
        "\n",
        "  if len(all) == 1:\n",
        "    if all[0].isalpha():\n",
        "      if str(el[5]).startswith('.'):\n",
        "          dots_start.append(el)"
      ],
      "metadata": {
        "id": "_MvQpH7DnDp-"
      },
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dots_start = pandas.DataFrame(dots_start)\n",
        "dots_start.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "dots_start = dots_start.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "qGuN5jJ04TO2"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def change_dots2(sample):\n",
        "\n",
        "  tag = 'punctuation'\n",
        "  parts_of_sentences = ['ROOT', 'relcl', 'nsubj', 'expl', 'conj', 'ccomp', 'aux', \n",
        "                      'neg', 'nsubjpass','punct','intj', 'acl']\n",
        "  \n",
        "  all_sents = []\n",
        "  first_sent = []\n",
        "  for el in sample[11].values():\n",
        "    if el[-1] in parts_of_sentences:\n",
        "      all_sents.append(el)\n",
        "\n",
        "  for i in all_sents:\n",
        "    first_sent.append(i)\n",
        "    if i[0] == '.':\n",
        "      ind = all_sents.index(i)\n",
        "      break\n",
        "      \n",
        "  second_sent = all_sents[ind::]\n",
        "  del first_sent[-1]\n",
        "\n",
        "  if len(first_sent) >= 4:\n",
        "    pass\n",
        "\n",
        "  else:\n",
        "    parts_of_sentences2 = ['nsubj','ROOT', 'expl']\n",
        "    sent = []\n",
        "    for el in first_sent:\n",
        "      if el in parts_of_sentences2:\n",
        "        sent.append(el)\n",
        "        if len(sent) >= 2:\n",
        "          pass\n",
        "        else:\n",
        "          'the comma is not needed'\n",
        "\n",
        "  index1 = int(list(sample[8].keys())[0])\n",
        "  index2 = int(sample[2]) + 1\n",
        "  indexes = range(index1, index2)\n",
        "  error_area = [sample[7][i] for i in indexes]\n",
        "\n",
        "  pprint(\"USER'S TEXT: \" + sample[6])\n",
        "  pprint('CORRECTED TEXT: ' + str(sample[9]))\n",
        "\n",
        "\n",
        "\n",
        "  if len(str(sample[4]).split()) == len(str(sample[5]).split()):\n",
        "    error_area = error_area[0:-1]\n",
        "    pprint('CORRECTION: ' + ' '.join(error_area) + '.')\n",
        "    pprint('ERROR AREA: '+ ' '.join(error_area))\n",
        "    pprint('TAG: ' + tag)\n",
        "\n",
        "  elif '. '+str(sample[4]).lower() == str(sample[5]).lower():\n",
        "    error_area = error_area[0:-1]\n",
        "    pprint('CORRECTION: ' + ' '.join(error_area) + '.')\n",
        "    pprint('ERROR AREA: '+ ' '.join(error_area))\n",
        "    pprint('TAG: ' + tag)\n",
        "\n",
        "  else:\n",
        "    error_area = error_area[0:-1]\n",
        "    pprint('CORRECTION: ' + ' '.join(error_area) + '.')\n",
        "    pprint('ERROR AREA: '+ ' '.join(error_area))\n",
        "    pprint('TAG: ' + tag)\n",
        "\n",
        "\n",
        "    if 'a' in str(sample[5]).lower().split() or 'the' in str(sample[5]).lower().split():\n",
        "      tag = 'articles'\n",
        "      index1 = int(sample[2]) + 1\n",
        "      index2 = int(list(sample[7].keys())[-1])\n",
        "      indexes = range(index1, index2)\n",
        "      for ind in indexes:\n",
        "        if sample[8][ind][1] != 'NOUN':\n",
        "          index2 = ind + 1\n",
        "          break\n",
        "\n",
        "      indexes = range(index1, index2+1)\n",
        "      error_area = []\n",
        "      for ind in indexes:\n",
        "        error_area.append(str(sample[7][ind]))\n",
        "\n",
        "      pprint('CORRECTION: ' + str(sample[5]).split()[1] + ' ' +' '.join(error_area))\n",
        "      pprint('ERROR AREA: '+ ' '.join(error_area))\n",
        "      pprint('TAG: ' + tag)\n",
        "\n",
        "    elif 'a' in str(sample[4]).lower().split() or 'the' in str(sample[4]).lower().split():\n",
        "      if 'a' not in str(sample[5]).lower().split() or 'the' not in str(sample[5]).lower().split():\n",
        "        tag = 'articles'\n",
        "        index1 = int(sample[2]) + 1\n",
        "        index2 = int(list(sample[7].keys())[-1])\n",
        "        indexes = range(index1, index2)\n",
        "        for ind in indexes:\n",
        "          if sample[8][ind][1] != 'NOUN':\n",
        "            index2 = ind + 1\n",
        "            break\n",
        "\n",
        "        indexes = range(index1, index2+1)\n",
        "        error_area = []\n",
        "        for ind in indexes:\n",
        "          error_area.append(str(sample[7][ind+1]))\n",
        "\n",
        "        correction = []\n",
        "        for ind in indexes:\n",
        "          correction.append(str(sample[10][ind+1]))\n",
        "\n",
        "        pprint('CORRECTION: ' + ' '.join(correction[0:-1]))\n",
        "        pprint('ERROR AREA: '+ ' '.join(error_area))\n",
        "        pprint('TAG: ' + tag)\n",
        "    else:\n",
        "      print('HERE ANOTHER CASE!!!!!!')\n",
        "\n",
        "  print('\\n')\n",
        "    "
      ],
      "metadata": {
        "id": "LuuKkyMD5y7T"
      },
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in dots_start.values:\n",
        "#  change_dots2(el)"
      ],
      "metadata": {
        "id": "71xd6cT643RI"
      },
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in dots_start['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "v67iyvEeQKGG"
      },
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "commas_start = []\n",
        "for el in punctuation.values:\n",
        "  element = str(el[4])\n",
        "  f = re.findall(r'[a-zA-Z]+', element)\n",
        "  a = re.findall(r'\\d+', element)\n",
        "  k = re.findall(r'\\W', element)\n",
        "  all = f+a+k\n",
        "  if ' ' in all:\n",
        "    all.remove(' ')\n",
        "\n",
        "  if len(all) == 1:\n",
        "    if all[0].isalpha():\n",
        "      if str(el[5]).startswith(','):\n",
        "          commas_start.append(el)"
      ],
      "metadata": {
        "id": "_pneqPbsSJ82"
      },
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "commas_start = pandas.DataFrame(commas_start)\n",
        "commas_start.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "commas_start = commas_start.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "tUTVSZs6SSVm"
      },
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def change_commas_start(sample):\n",
        "  tag = 'punctuation'\n",
        "  parts_of_sentences = ['ROOT', 'relcl', 'nsubj', 'expl', 'conj', 'ccomp', 'aux', \n",
        "                      'neg', 'nsubjpass','punct','intj', 'acl']\n",
        "  \n",
        "  all_sents = []\n",
        "  first_sent = []\n",
        "  for el in sample[11].values():\n",
        "    if el[-1] in parts_of_sentences:\n",
        "      all_sents.append(el)\n",
        "\n",
        "  for i in all_sents:\n",
        "    first_sent.append(i)\n",
        "    if i[0] == '.':\n",
        "      ind = all_sents.index(i)\n",
        "      break\n",
        "      \n",
        "  second_sent = all_sents[ind::]\n",
        "  del first_sent[-1]\n",
        "\n",
        "  if len(first_sent) >= 4:\n",
        "    pass\n",
        "\n",
        "  else:\n",
        "    parts_of_sentences2 = ['nsubj','ROOT', 'expl']\n",
        "    sent = []\n",
        "    for el in first_sent:\n",
        "      if el in parts_of_sentences2:\n",
        "        sent.append(el)\n",
        "        if len(sent) >= 2:\n",
        "          pass\n",
        "        else:\n",
        "          'the comma is not needed'\n",
        "\n",
        "  index1 = int(list(sample[8].keys())[0])\n",
        "  index2 = int(sample[2]) + 1\n",
        "  indexes = range(index1, index2)\n",
        "  error_area = [sample[7][i] for i in indexes]\n",
        "\n",
        "  pprint(\"USER'S TEXT: \" + sample[6])\n",
        "  pprint('CORRECTED TEXT: ' + str(sample[9]))\n",
        "\n",
        "  error_area = error_area[0:-1]\n",
        "  pprint('CORRECTION: ' + ' '.join(error_area) + ',')\n",
        "  pprint('ERROR AREA: '+ ' '.join(error_area))\n",
        "  pprint('TAG: ' + tag)\n",
        "\n",
        "  if ', ' + str(sample[4]).lower() == str(sample[5].lower()):\n",
        "    tag = 'capitalization'\n",
        "    pprint('CORRECTION: ' + str(sample[5].split()[1]))\n",
        "    pprint('ERROR AREA: '+  str(sample[4]))\n",
        "    pprint('TAG: ' + tag)\n",
        "  elif len(list(set(list(sample[4])) ^ set(list(sample[5])))) < 4:\n",
        "    tag = 'spelling'\n",
        "    pprint('CORRECTION: ' + str(sample[5].split()[1]))\n",
        "    pprint('ERROR AREA: '+  str(sample[4]))\n",
        "    pprint('TAG: ' + tag)\n",
        "  elif len(list(set(list(sample[4])) ^ set(list(sample[5])))) > 3:\n",
        "    tag = 'wrong word'\n",
        "    pprint('CORRECTION: ' + str(sample[5].split()[1]))\n",
        "    pprint('ERROR AREA: '+  str(sample[4]))\n",
        "    pprint('TAG: ' + tag)\n",
        "  print('\\n')"
      ],
      "metadata": {
        "id": "PDkIOEezWdsQ"
      },
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in commas_start.values:\n",
        "#  change_commas_start(el)"
      ],
      "metadata": {
        "id": "sVfamfqiTIXG"
      },
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in commas_start['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "abeaU2-xk6Tn"
      },
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_punctuation = []\n",
        "for el in punctuation.values:\n",
        "  element = str(el[4])\n",
        "  element2 = str(el[5])\n",
        "\n",
        "  f = re.findall(r'[a-zA-Z]+', element)\n",
        "  a = re.findall(r'\\d+', element)\n",
        "  k = re.findall(r'\\W', element)\n",
        "  all1 = f+a+k\n",
        "  if ' ' in all1:\n",
        "    all1.remove(' ')\n",
        "\n",
        "  f = re.findall(r'[a-zA-Z]+', element2)\n",
        "  a = re.findall(r'\\d+', element2)\n",
        "  k = re.findall(r'\\W', element2)\n",
        "  all2 = f+a+k\n",
        "  if ' ' in all2:\n",
        "    all2.remove(' ')\n",
        "  \n",
        "\n",
        "  if ',' not in all1 and '.' not in all1 and ':' not in all1:\n",
        "    if ',' not in all2 and '.' not in all2 and ':' not in all2:\n",
        "      non_punctuation.append(el)"
      ],
      "metadata": {
        "id": "GKefEtx9qzYu"
      },
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_punctuation = pandas.DataFrame(non_punctuation)\n",
        "non_punctuation.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "non_punctuation = non_punctuation.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "V2iohCbJsCiW"
      },
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def change_nonpunctuation(sample):\n",
        "  tag='?'\n",
        "  pprint(\"USER'S TEXT: \" + sample[6])\n",
        "  pprint('CORRECTED TEXT: ' + str(sample[9]))\n",
        "\n",
        "  index = sample[2]\n",
        "  correction = str(sample[5])\n",
        "  pprint('CORRECTION: ' + correction)\n",
        "  pprint('ERROR AREA: '+ str(sample[7][index]))\n",
        "  pprint('TAG: ' + tag)"
      ],
      "metadata": {
        "id": "AUcvZTzW4YHY"
      },
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in non_punctuation.values:\n",
        "#  change_nonpunctuation(el)"
      ],
      "metadata": {
        "id": "qu0tIsFtsVDK"
      },
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in non_punctuation['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "C2xf1eOs5FSF"
      },
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerals = []\n",
        "for el in punctuation.values:\n",
        "  if isinstance(el[4],float):\n",
        "    el4 = str(int(el[4]))\n",
        "  else:\n",
        "    el4 = str(el[4])\n",
        "  if el4.isnumeric():\n",
        "    if str(el[5]).endswith('%'):\n",
        "      numerals.append(el)"
      ],
      "metadata": {
        "id": "iKzUWdkp6H8r"
      },
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerals = pandas.DataFrame(numerals)\n",
        "numerals.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "numerals = numerals.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "1h03TDlt63F7"
      },
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def change_numerals(sample):\n",
        "  \n",
        "  tag='numerals'\n",
        "  pprint(\"USER'S TEXT: \" + sample[6])\n",
        "  pprint('CORRECTED TEXT: ' + str(sample[9]))\n",
        "\n",
        "  index = sample[2]\n",
        "  correction = str(sample[5])\n",
        "  pprint('CORRECTION: ' + correction)\n",
        "  pprint('ERROR AREA: '+ str(sample[7][index]))\n",
        "  pprint('TAG: ' + tag)"
      ],
      "metadata": {
        "id": "2TJfoK3Q67yI"
      },
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in numerals.values:\n",
        "#  change_numerals(el)"
      ],
      "metadata": {
        "id": "NeqaFjfc66PL"
      },
      "execution_count": 260,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in numerals['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "M-mqV8s47jYU"
      },
      "execution_count": 261,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "years2_commas = []\n",
        "for el in punctuation.values:\n",
        "  if isinstance(el[4],float):\n",
        "    el4 = str(int(el[4]))\n",
        "  else:\n",
        "    el4 = str(el[4])\n",
        "  if el4.isnumeric():\n",
        "    if el4+',' == str(el[5]):\n",
        "      years2_commas.append(el)"
      ],
      "metadata": {
        "id": "WtThqOGb8lsi"
      },
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "years2_commas = pandas.DataFrame(years2_commas)\n",
        "years2_commas.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "years2_commas = years2_commas.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "a6cKsOT380Pg"
      },
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def change_years2(sample):\n",
        "  pprint(\"USER'S TEXT: \" + sample[6])\n",
        "  pprint('CORRECTED TEXT: ' + str(sample[9]))\n",
        "  tag = 'punctuation'\n",
        "\n",
        "\n",
        "  i = list(sample[7].keys())[0]\n",
        "  index1 = list(sample[7].keys())[0]\n",
        "  while i != el[2]:\n",
        "    i += 1\n",
        "    if el[7][i] == ',':\n",
        "      index1 = i + 1\n",
        "\n",
        "  index2 = int(el[2]) \n",
        "  indexes = range(index1, index2 + 1)\n",
        "\n",
        "  error_area = []\n",
        "  for b in indexes:\n",
        "    error_area.append(str(el[7][b]))\n",
        "\n",
        "\n",
        "  pprint('CORRECTION: ' + ' '.join(error_area) + ',')\n",
        "  pprint('ERROR AREA: '+ ' '.join(error_area))\n",
        "  pprint('TAG: ' + tag)"
      ],
      "metadata": {
        "id": "mlwU3qfE-sEC"
      },
      "execution_count": 280,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in years2_commas.values:\n",
        "#  change_years2(el)"
      ],
      "metadata": {
        "id": "0EayrYeN-vMO"
      },
      "execution_count": 282,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in years2_commas['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "wdA_BoOQ87UI"
      },
      "execution_count": 283,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "commas_dot3 = []\n",
        "for el in punctuation.values:\n",
        "  try:\n",
        "\n",
        "    if str(el[4]).startswith(','):\n",
        "      if str(el[5]).startswith('.'):\n",
        "        if el[4].split()[1] == el[5].split()[1].lower():\n",
        "          commas_dot3.append(el)\n",
        "\n",
        "  except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "9LeZFE-4ACyY"
      },
      "execution_count": 304,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "commas_dot3 = pandas.DataFrame(commas_dot3)\n",
        "commas_dot3.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "commas_dot3 = commas_dot3.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "m-LtPedLAZwv"
      },
      "execution_count": 305,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def change_commas_dots3(sample):\n",
        "  tag = 'punctuation'\n",
        "  parts_of_sentences = ['ROOT', 'relcl', 'nsubj', 'expl', 'conj', 'ccomp', 'aux', \n",
        "                      'neg', 'nsubjpass','punct','intj', 'acl']\n",
        "  \n",
        "  all_sents = []\n",
        "  first_sent = []\n",
        "  for el in sample[11].values():\n",
        "\n",
        "    if el[-1] in parts_of_sentences:\n",
        "      all_sents.append(el)\n",
        "\n",
        "  for i in all_sents:\n",
        "    first_sent.append(i)\n",
        "    if i[0] == '.':\n",
        "      ind = all_sents.index(i)\n",
        "      break\n",
        "      \n",
        "  second_sent = all_sents[ind::]\n",
        "  del first_sent[-1]\n",
        "\n",
        "  if len(first_sent) >= 4:\n",
        "    pass\n",
        "\n",
        "  else:\n",
        "    parts_of_sentences2 = ['nsubj','ROOT', 'expl']\n",
        "    sent = []\n",
        "    for el in first_sent:\n",
        "      if el in parts_of_sentences2:\n",
        "        sent.append(el)\n",
        "        if len(sent) >= 2:\n",
        "          pass\n",
        "        else:\n",
        "          'the comma is not needed'\n",
        "\n",
        "  index1 = int(list(sample[8].keys())[0])\n",
        "  index2 = int(sample[2]) + 1\n",
        "  indexes = range(index1, index2)\n",
        "  error_area = [sample[7][i] for i in indexes]\n",
        "\n",
        "  pprint(\"USER'S TEXT: \" + sample[6])\n",
        "  pprint('CORRECTED TEXT: ' + str(sample[9]))\n",
        "\n",
        "  error_area = error_area[0:-1]\n",
        "  pprint('CORRECTION: ' + ' '.join(error_area) + '.')\n",
        "  pprint('ERROR AREA: '+ ' '.join(error_area))\n",
        "  pprint('TAG: ' + tag)\n",
        "\n",
        "  if str(sample[4]).endswith('the'):\n",
        "    element = str(sample[4])\n",
        "\n",
        "\n",
        "    f = re.findall(r'[a-zA-Z]+', element)\n",
        "    a = re.findall(r'\\d+', element)\n",
        "    k = re.findall(r'\\W', element)\n",
        "    all = f+a+k\n",
        "    if ' ' in all:\n",
        "      all.remove(' ')\n",
        "    length = len(all)\n",
        "    \n",
        "    tag = 'articles'\n",
        "    index = int(sample[2]) + length - 1\n",
        "    pprint('CORRECTION: ' +str(sample[7][index]))\n",
        "    pprint('ERROR AREA: '+  str(sample[7][index-1])+' '+ str(sample[7][index]))\n",
        "    pprint('TAG: ' + tag)\n",
        "\n",
        "  print('\\n')"
      ],
      "metadata": {
        "id": "lHAJz5wUGKKz"
      },
      "execution_count": 414,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in commas_dot3.values:\n",
        "#  change_commas_dots3(el)"
      ],
      "metadata": {
        "id": "VU4cCjYcGIrQ"
      },
      "execution_count": 416,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in commas_dot3['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "UcBg6usULUxT"
      },
      "execution_count": 331,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dots_commas = []\n",
        "for el in punctuation.values:\n",
        "  try:\n",
        "\n",
        "    if str(el[4]).startswith('.'):\n",
        "      if str(el[5]).startswith(','):\n",
        "        if el[4].split()[1].lower() == el[5].split()[1]:\n",
        "          dots_commas.append(el)\n",
        "\n",
        "  except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "O9AU9yp4Okmr"
      },
      "execution_count": 338,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dots_commas = pandas.DataFrame(dots_commas)\n",
        "dots_commas.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "dots_commas = dots_commas.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "2ANGE5fZOz8v"
      },
      "execution_count": 339,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def change_dots_commas(sample):\n",
        "  tag = 'punctuation'\n",
        "  parts_of_sentences = ['ROOT', 'relcl', 'nsubj', 'expl', 'conj', 'ccomp', 'aux', \n",
        "                      'neg', 'nsubjpass','punct','intj', 'acl']\n",
        "  \n",
        "  all_sents = []\n",
        "  first_sent = []\n",
        "  for el in sample[11].values():\n",
        "\n",
        "    if el[-1] in parts_of_sentences:\n",
        "      all_sents.append(el)\n",
        "\n",
        "  for i in all_sents:\n",
        "    first_sent.append(i)\n",
        "    if i[0] == '.':\n",
        "      ind = all_sents.index(i)\n",
        "      break\n",
        "      \n",
        "  second_sent = all_sents[ind::]\n",
        "  del first_sent[-1]\n",
        "\n",
        "  if len(first_sent) >= 4:\n",
        "    pass\n",
        "\n",
        "  else:\n",
        "    parts_of_sentences2 = ['nsubj','ROOT', 'expl']\n",
        "    sent = []\n",
        "    for el in first_sent:\n",
        "      if el in parts_of_sentences2:\n",
        "        sent.append(el)\n",
        "        if len(sent) >= 2:\n",
        "          pass\n",
        "        else:\n",
        "          'the comma is not needed'\n",
        "\n",
        "  index1 = int(list(sample[8].keys())[0])\n",
        "  index2 = int(sample[2]) + 1\n",
        "  indexes = range(index1, index2)\n",
        "  error_area = [sample[7][i] for i in indexes]\n",
        "\n",
        "  pprint(\"USER'S TEXT: \" + sample[6])\n",
        "  pprint('CORRECTED TEXT: ' + str(sample[9]))\n",
        "\n",
        "  error_area = error_area[0:-1]\n",
        "  pprint('CORRECTION: ' + ' '.join(error_area) + ',')\n",
        "  pprint('ERROR AREA: '+ ' '.join(error_area))\n",
        "  pprint('TAG: ' + tag)\n",
        "\n",
        "  print('\\n')"
      ],
      "metadata": {
        "id": "Ecwn7OOqPH6j"
      },
      "execution_count": 346,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in dots_commas.values:\n",
        "#  change_dots_commas(el)"
      ],
      "metadata": {
        "id": "9bhjtbheO6W5"
      },
      "execution_count": 348,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in dots_commas['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "FE_aN9MJPy3l"
      },
      "execution_count": 349,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "commas_colon = []\n",
        "for el in punctuation.values:\n",
        "  if str(el[4]).startswith(','):\n",
        "    if str(el[5]).startswith(';'):\n",
        "      commas_colon.append(el)\n"
      ],
      "metadata": {
        "id": "DYT4geg2P5m4"
      },
      "execution_count": 356,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "commas_colon = pandas.DataFrame(commas_colon)\n",
        "commas_colon.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "commas_colon = commas_colon.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "tts6PD-1RWD-"
      },
      "execution_count": 357,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def change_commas_colon(sample):\n",
        "  tag = 'punctuation'\n",
        "  parts_of_sentences = ['ROOT', 'relcl', 'nsubj', 'expl', 'conj', 'ccomp', 'aux', \n",
        "                      'neg', 'nsubjpass','punct','intj', 'acl']\n",
        "  \n",
        "  all_sents = []\n",
        "  first_sent = []\n",
        "  for el in sample[11].values():\n",
        "\n",
        "    if el[-1] in parts_of_sentences:\n",
        "      all_sents.append(el)\n",
        "\n",
        "  for i in all_sents:\n",
        "    first_sent.append(i)\n",
        "    if i[0] == '.':\n",
        "      ind = all_sents.index(i)\n",
        "      break\n",
        "      \n",
        "  second_sent = all_sents[ind::]\n",
        "  del first_sent[-1]\n",
        "\n",
        "  if len(first_sent) >= 4:\n",
        "    pass\n",
        "\n",
        "  else:\n",
        "    parts_of_sentences2 = ['nsubj','ROOT', 'expl']\n",
        "    sent = []\n",
        "    for el in first_sent:\n",
        "      if el in parts_of_sentences2:\n",
        "        sent.append(el)\n",
        "        if len(sent) >= 2:\n",
        "          pass\n",
        "        else:\n",
        "          'the comma is not needed'\n",
        "\n",
        "  index1 = int(list(sample[8].keys())[0])\n",
        "  index2 = int(sample[2]) + 2\n",
        "  indexes = range(index1, index2)\n",
        "  error_area = [sample[7][i] for i in indexes]\n",
        "\n",
        "  pprint(\"USER'S TEXT: \" + sample[6])\n",
        "  pprint('CORRECTED TEXT: ' + str(sample[9]))\n",
        "\n",
        "  error_area = error_area[0:-1]\n",
        "  pprint('CORRECTION: ' + ' '.join(error_area[0:-1]) + ';')\n",
        "  pprint('ERROR AREA: '+ ' '.join(error_area))\n",
        "  pprint('TAG: ' + tag)\n",
        "\n",
        "  print('\\n')"
      ],
      "metadata": {
        "id": "y_tIvzvORhA3"
      },
      "execution_count": 364,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in commas_colon.values:\n",
        " # change_commas_colon(el)"
      ],
      "metadata": {
        "id": "DMrsuOWCSzoh"
      },
      "execution_count": 366,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in commas_colon['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "JJIbnJMdTcnL"
      },
      "execution_count": 367,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "delete_commas2 = []\n",
        "for el in punctuation.values:\n",
        "  if str(el[4]).startswith(','):\n",
        "    if str(el[5]).isalpha():\n",
        "      delete_commas2.append(el)"
      ],
      "metadata": {
        "id": "7LKIkGQbT5C9"
      },
      "execution_count": 370,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "delete_commas2 = pandas.DataFrame(delete_commas2)\n",
        "delete_commas2.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "delete_commas2 = delete_commas2.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "Bt_pvijIXAxH"
      },
      "execution_count": 371,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def change_delete_commas2(sample):\n",
        "  tag = 'punctuation'\n",
        "  parts_of_sentences = ['ROOT', 'relcl', 'nsubj', 'expl', 'conj', 'ccomp', 'aux', \n",
        "                      'neg', 'nsubjpass','punct','intj', 'acl']\n",
        "  \n",
        "  all_sents = []\n",
        "  first_sent = []\n",
        "  for el in sample[11].values():\n",
        "    if el[-1] in parts_of_sentences:\n",
        "      all_sents.append(el)\n",
        "\n",
        "  for i in all_sents:\n",
        "    first_sent.append(i)\n",
        "    if i[0] == '.':\n",
        "      ind = all_sents.index(i)\n",
        "      break\n",
        "      \n",
        "  second_sent = all_sents[ind::]\n",
        "  del first_sent[-1]\n",
        "\n",
        "  if len(first_sent) >= 4:\n",
        "    pass\n",
        "\n",
        "  else:\n",
        "    parts_of_sentences2 = ['nsubj','ROOT', 'expl']\n",
        "    sent = []\n",
        "    for el in first_sent:\n",
        "      if el in parts_of_sentences2:\n",
        "        sent.append(el)\n",
        "        if len(sent) >= 2:\n",
        "          pass\n",
        "        else:\n",
        "          'the comma is not needed'\n",
        "\n",
        "  index1 = int(list(sample[8].keys())[0])\n",
        "  index2 = int(sample[2]) + 1\n",
        "  indexes = range(index1, index2)\n",
        "  error_area = [sample[7][i] for i in indexes]\n",
        "\n",
        "  pprint(\"USER'S TEXT: \" + sample[6])\n",
        "  pprint('CORRECTED TEXT: ' + str(sample[9]))\n",
        "\n",
        "  error_area = error_area\n",
        "  pprint('CORRECTION: ' + ' '.join(error_area[0:-1]))\n",
        "  pprint('ERROR AREA: '+ ' '.join(error_area))\n",
        "  pprint('TAG: ' + tag)\n",
        "\n",
        "  if len(list(set(list(sample[4].split()[1])) ^ set(list(sample[5])))) < 4:\n",
        "    element = sample[4]\n",
        "    f = re.findall(r'[a-zA-Z]+', element)\n",
        "    a = re.findall(r'\\d+', element)\n",
        "    k = re.findall(r'\\W', element)\n",
        "    all = f+a+k\n",
        "    if ' ' in all:\n",
        "      all.remove(' ')\n",
        "    length = len(all)\n",
        "    index = int(sample[2]) + length -1\n",
        "\n",
        "    tag = 'spelling'\n",
        "    pprint('CORRECTION: ' + str(sample[5].split()[0]))\n",
        "    pprint('ERROR AREA: '+  str(sample[7][index]))\n",
        "    pprint('TAG: ' + tag)\n"
      ],
      "metadata": {
        "id": "LtMpqNNBXDEO"
      },
      "execution_count": 407,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in delete_commas2.values:\n",
        "#  change_delete_commas2(el)"
      ],
      "metadata": {
        "id": "LbwVn9BIXR5v"
      },
      "execution_count": 409,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in delete_commas2['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "H4FfQd7gY6SQ"
      },
      "execution_count": 405,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conj_and_articles = []\n",
        "for el in punctuation.values:\n",
        "  try:\n",
        "    if el[4].isalpha():\n",
        "      element = el[5]\n",
        "      f = re.findall(r'[a-zA-Z]+', element)\n",
        "      a = re.findall(r'\\d+', element)\n",
        "      k = re.findall(r'\\W', element)\n",
        "      all = f+a+k\n",
        "      if ' ' in all:\n",
        "        all.remove(' ')\n",
        "      length = len(all)\n",
        "      index = int(el[2]) + length - 1\n",
        "      if el[11][index][1] == 'DET':\n",
        "        conj_and_articles.append(el)\n",
        "      elif el[11][index][1] == 'CCONJ':\n",
        "        conj_and_articles.append(el)\n",
        "  except:\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "mTWcQ6ITZDUq"
      },
      "execution_count": 456,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conj_and_articles = pandas.DataFrame(conj_and_articles)\n",
        "conj_and_articles.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "conj_and_articles = conj_and_articles.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "iixpn9rOaKO_"
      },
      "execution_count": 457,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def change_conj_and_articles(sample):\n",
        "  tag = 'punctuation'\n",
        "  parts_of_sentences = ['ROOT', 'relcl', 'nsubj', 'expl', 'conj', 'ccomp', 'aux', \n",
        "                      'neg', 'nsubjpass','punct','intj', 'acl']\n",
        "  \n",
        "  all_sents = []\n",
        "  first_sent = []\n",
        "  for el in sample[11].values():\n",
        "    if el[-1] in parts_of_sentences:\n",
        "      all_sents.append(el)\n",
        "\n",
        "  for i in all_sents:\n",
        "    first_sent.append(i)\n",
        "    if i[0] == '.':\n",
        "      ind = all_sents.index(i)\n",
        "      break\n",
        "      \n",
        "  second_sent = all_sents[ind::]\n",
        "  del first_sent[-1]\n",
        "\n",
        "  if len(first_sent) >= 4:\n",
        "    pass\n",
        "\n",
        "  else:\n",
        "    parts_of_sentences2 = ['nsubj','ROOT', 'expl']\n",
        "    sent = []\n",
        "    for el in first_sent:\n",
        "      if el in parts_of_sentences2:\n",
        "        sent.append(el)\n",
        "        if len(sent) >= 2:\n",
        "          pass\n",
        "        else:\n",
        "          'the comma is not needed'\n",
        "\n",
        "  index1 = int(list(sample[8].keys())[0])\n",
        "  index2 = int(sample[2]) + 1\n",
        "  indexes = range(index1, index2)\n",
        "  error_area = [sample[7][i] for i in indexes]\n",
        "\n",
        "  pprint(\"USER'S TEXT: \" + sample[6])\n",
        "  pprint('CORRECTED TEXT: ' + str(sample[9]))\n",
        "\n",
        "  error_area = error_area\n",
        "  pprint('CORRECTION: ' + ' '.join(error_area) + ',')\n",
        "  pprint('ERROR AREA: '+ ' '.join(error_area))\n",
        "  pprint('TAG: ' + tag)\n",
        "\n",
        "  element = sample[5]\n",
        "  f = re.findall(r'[a-zA-Z]+', element)\n",
        "  a = re.findall(r'\\d+', element)\n",
        "  k = re.findall(r'\\W', element)\n",
        "  all = f+a+k\n",
        "  if ' ' in all:\n",
        "    all.remove(' ')\n",
        "  length = len(all)\n",
        "  index = int(sample[2]) + length -1\n",
        "  if sample[11][index][0] == 'a':\n",
        "    element = str(sample[4])\n",
        "\n",
        "\n",
        "    f = re.findall(r'[a-zA-Z]+', element)\n",
        "    a = re.findall(r'\\d+', element)\n",
        "    k = re.findall(r'\\W', element)\n",
        "    all = f+a+k\n",
        "    if ' ' in all:\n",
        "      all.remove(' ')\n",
        "    length = len(all)\n",
        "    \n",
        "    tag = 'articles'\n",
        "    index = int(sample[2]) + length \n",
        "    pprint('CORRECTION: ' + 'a ' + str(sample[7][index]))\n",
        "    pprint('ERROR AREA: '+  str(sample[7][index]))\n",
        "    pprint('TAG: ' + tag)\n",
        "\n",
        "  elif sample[11][index][1] == 'CCONJ':\n",
        "    tag = 'absence'\n",
        "    index = sample[2]\n",
        "    pprint('CORRECTION: ' + str(sample[5]))\n",
        "    pprint('ERROR AREA: '+ str(sample[7][index]))\n",
        "    pprint('TAG: ' + tag)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bPUwTbQZbI5M"
      },
      "execution_count": 478,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in conj_and_articles.values:\n",
        "#  change_conj_and_articles(el)"
      ],
      "metadata": {
        "id": "4JFLQ7V-bS0H"
      },
      "execution_count": 479,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "solo_punct = []\n",
        "for el in punctuation.values:\n",
        "  if str(el[4]) in string.punctuation:\n",
        "    if str(el[5]) in string.punctuation:\n",
        "      solo_punct.append(el)"
      ],
      "metadata": {
        "id": "NUOKdlvJe_MW"
      },
      "execution_count": 483,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "solo_punct = pandas.DataFrame(solo_punct)\n",
        "solo_punct.rename(columns = {0: 'text_id', 1 : 'sentence_id', 2 : 'token_id', 3 : 'mistake_type', 4 : 'error_span', 5 : 'correction', 6 : 'full_text', 7 : 'tokens_ids', \n",
        "                               8 : 'dependencies', 9 : 'corrected_text', 10 : 'corrected_with_ids', 11 : 'dependencies_heptabot'}, inplace = True)\n",
        "solo_punct = solo_punct.drop_duplicates(subset=['corrected_text'], ignore_index=True)"
      ],
      "metadata": {
        "id": "8oJbLrKffLn9"
      },
      "execution_count": 484,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def change_solo_punct(sample):\n",
        "  pprint(\"USER'S TEXT: \" + sample[6])\n",
        "  pprint('CORRECTED TEXT: ' + str(sample[9]))\n",
        "  tag = 'punctuation'\n",
        "\n",
        "\n",
        "  i = list(sample[7].keys())[0]\n",
        "  index1 = list(sample[7].keys())[0]\n",
        "  while i != el[2]:\n",
        "    i += 1\n",
        "    if el[7][i] == ',':\n",
        "      index1 = i + 1\n",
        "\n",
        "  index2 = int(el[2]) \n",
        "  indexes = range(index1, index2 + 1)\n",
        "\n",
        "  error_area = []\n",
        "  for b in indexes:\n",
        "    error_area.append(str(el[7][b]))\n",
        "\n",
        "\n",
        "  pprint('CORRECTION: ' + ' '.join(error_area[0:-1])+str(sample[5]))\n",
        "  pprint('ERROR AREA: '+ ' '.join(error_area))\n",
        "  pprint('TAG: ' + tag)"
      ],
      "metadata": {
        "id": "PgsRDfOafml5"
      },
      "execution_count": 490,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for el in solo_punct.values:\n",
        "#  change_solo_punct(el)"
      ],
      "metadata": {
        "id": "H8PJEt1zfpde"
      },
      "execution_count": 492,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def change_other_cases(sample):\n",
        "  pprint(\"USER'S TEXT: \" + sample[6])\n",
        "  pprint('CORRECTED TEXT: ' + str(sample[9]))\n",
        "  tag = 'punctuation'\n",
        "\n",
        "\n",
        "  element = sample[4]\n",
        "  f = re.findall(r'[a-zA-Z]+', element)\n",
        "  a = re.findall(r'\\d+', element)\n",
        "  k = re.findall(r'\\W', element)\n",
        "  all = f+a+k\n",
        "  if ' ' in all:\n",
        "    all.remove(' ')\n",
        "  length = len(all)\n",
        "  index2 = int(sample[2]) + length\n",
        "  index1 = int(sample[2])\n",
        "  indexes = range(index1, index2)\n",
        "  \n",
        "  error_area = []\n",
        "  for ind in indexes:\n",
        "    error_area.append(str(sample[7][ind]))\n",
        "  pprint('CORRECTION: ' + str(sample[5]))\n",
        "  pprint('ERROR AREA: '+ ' '.join(error_area))\n",
        "  pprint('TAG: ' + tag)"
      ],
      "metadata": {
        "id": "YPGQO43AgPZZ"
      },
      "execution_count": 512,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for el in punctuation.values:\n",
        "  try:\n",
        "    change_other_cases(el)\n",
        "  except:\n",
        "    print(el)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bL2muh0Pgc5b",
        "outputId": "071d04b4-2a0c-4bff-9630-5e1c8fc79bea"
      },
      "execution_count": 514,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\"USER'S TEXT: As Africa Today had recenly reported 78% of students living in \"\n",
            " 'Africa simply do not have enough income to pay for University. Moreover, '\n",
            " 'there are some professions requiring special education and equipment.')\n",
            "('CORRECTED TEXT: As Africa Today had recenly reported, 78% of students living '\n",
            " 'in Africa simply do not have enough income to pay for University. Moreover, '\n",
            " 'there are some professions requiring special education and equipment.')\n",
            "'CORRECTION: , 78%'\n",
            "'ERROR AREA: 78 %'\n",
            "'TAG: punctuation'\n",
            "(\"USER'S TEXT: Only germany saw a slight increase from 1990 to 1995 To sum up, \"\n",
            " 'there is a general tendency of a decline in income difference between men '\n",
            " 'and women, which is quite positive.')\n",
            "('CORRECTED TEXT: Only germany saw a slight increase from 1990 to 1995.0 To '\n",
            " 'sum up, there is a general tendency of a decline in income difference '\n",
            " 'between men and women, which is quite positive.')\n",
            "[592.0 6.0 128.0 'punct' 1995.0 1995.0\n",
            " 'Only germany saw a slight increase from 1990 to 1995 To sum up, there is a general tendency of a decline in income difference between men and women, which is quite positive.'\n",
            " {119: 'Only', 120: 'germany', 121: 'saw', 122: 'a', 123: 'slight', 124: 'increase', 125: 'from', 126: '1990', 127: 'to', 128: '1995', 129: 'To', 130: 'sum', 131: 'up', 132: ',', 133: 'there', 134: 'is', 135: 'a', 136: 'general', 137: 'tendency', 138: 'of', 139: 'a', 140: 'decline', 141: 'in', 142: 'income', 143: 'difference', 144: 'between', 145: 'men', 146: 'and', 147: 'women', 148: ',', 149: 'which', 150: 'is', 151: 'quite', 152: 'positive', 153: '.'}\n",
            " {119: ['Only', 'ADV', 'RB', 'advmod'], 120: ['germany', 'PROPN', 'NNP', 'nsubj'], 121: ['saw', 'VERB', 'VBD', 'ccomp'], 122: ['a', 'DET', 'DT', 'det'], 123: ['slight', 'ADJ', 'JJ', 'amod'], 124: ['increase', 'NOUN', 'NN', 'nsubj'], 125: ['from', 'ADP', 'IN', 'prep'], 126: ['1990', 'NUM', 'CD', 'pobj'], 127: ['to', 'ADP', 'IN', 'prep'], 128: ['1995', 'NUM', 'CD', 'pobj'], 129: ['To', 'PART', 'TO', 'aux'], 130: ['sum', 'VERB', 'VB', 'ccomp'], 131: ['up', 'ADP', 'RP', 'prt'], 132: [',', 'PUNCT', ',', 'punct'], 133: ['there', 'PRON', 'EX', 'expl'], 134: ['is', 'AUX', 'VBZ', 'ROOT'], 135: ['a', 'DET', 'DT', 'det'], 136: ['general', 'ADJ', 'JJ', 'amod'], 137: ['tendency', 'NOUN', 'NN', 'attr'], 138: ['of', 'ADP', 'IN', 'prep'], 139: ['a', 'DET', 'DT', 'det'], 140: ['decline', 'NOUN', 'NN', 'pobj'], 141: ['in', 'ADP', 'IN', 'prep'], 142: ['income', 'NOUN', 'NN', 'compound'], 143: ['difference', 'NOUN', 'NN', 'pobj'], 144: ['between', 'ADP', 'IN', 'prep'], 145: ['men', 'NOUN', 'NNS', 'pobj'], 146: ['and', 'CCONJ', 'CC', 'cc'], 147: ['women', 'NOUN', 'NNS', 'conj'], 148: [',', 'PUNCT', ',', 'punct'], 149: ['which', 'DET', 'WDT', 'nsubj'], 150: ['is', 'AUX', 'VBZ', 'relcl'], 151: ['quite', 'ADV', 'RB', 'advmod'], 152: ['positive', 'ADJ', 'JJ', 'acomp'], 153: ['.', 'PUNCT', '.', 'punct']}\n",
            " 'Only germany saw a slight increase from 1990 to 1995.0 To sum up, there is a general tendency of a decline in income difference between men and women, which is quite positive.'\n",
            " {119: 'Only', 120: 'germany', 121: 'saw', 122: 'a', 123: 'slight', 124: 'increase', 125: 'from', 126: '1990', 127: 'to', 128: '1995', 129: '.', 130: '0', 131: 'To', 132: 'sum', 133: 'up', 134: ',', 135: 'there', 136: 'is', 137: 'a', 138: 'general', 139: 'tendency', 140: 'of', 141: 'a', 142: 'decline', 143: 'in', 144: 'income', 145: 'difference', 146: 'between', 147: 'men', 148: 'and', 149: 'women', 150: ',', 151: 'which', 152: 'is', 153: 'quite', 154: 'positive', 155: '.'}\n",
            " {119: ['Only', 'ADV', 'RB', 'advmod'], 120: ['germany', 'PROPN', 'NNP', 'nsubj'], 121: ['saw', 'VERB', 'VBD', 'ROOT'], 122: ['a', 'DET', 'DT', 'det'], 123: ['slight', 'ADJ', 'JJ', 'amod'], 124: ['increase', 'NOUN', 'NN', 'dobj'], 125: ['from', 'ADP', 'IN', 'prep'], 126: ['1990', 'NUM', 'CD', 'pobj'], 127: ['to', 'ADP', 'IN', 'prep'], 128: ['1995', 'NUM', 'CD', 'pobj'], 129: ['.', 'PUNCT', '.', 'punct'], 130: ['0', 'PUNCT', 'NFP', 'punct'], 131: ['To', 'PART', 'TO', 'aux'], 132: ['sum', 'VERB', 'VB', 'advcl'], 133: ['up', 'ADP', 'RP', 'prt'], 134: [',', 'PUNCT', ',', 'punct'], 135: ['there', 'PRON', 'EX', 'expl'], 136: ['is', 'AUX', 'VBZ', 'ROOT'], 137: ['a', 'DET', 'DT', 'det'], 138: ['general', 'ADJ', 'JJ', 'amod'], 139: ['tendency', 'NOUN', 'NN', 'attr'], 140: ['of', 'ADP', 'IN', 'prep'], 141: ['a', 'DET', 'DT', 'det'], 142: ['decline', 'NOUN', 'NN', 'pobj'], 143: ['in', 'ADP', 'IN', 'prep'], 144: ['income', 'NOUN', 'NN', 'compound'], 145: ['difference', 'NOUN', 'NN', 'pobj'], 146: ['between', 'ADP', 'IN', 'prep'], 147: ['men', 'NOUN', 'NNS', 'pobj'], 148: ['and', 'CCONJ', 'CC', 'cc'], 149: ['women', 'NOUN', 'NNS', 'conj'], 150: [',', 'PUNCT', ',', 'punct'], 151: ['which', 'DET', 'WDT', 'nsubj'], 152: ['is', 'AUX', 'VBZ', 'relcl'], 153: ['quite', 'ADV', 'RB', 'advmod'], 154: ['positive', 'ADJ', 'JJ', 'acomp'], 155: ['.', 'PUNCT', '.', 'punct']}]\n",
            "(\"USER'S TEXT: As we can see Japans dinamics on the graph, shows stable \"\n",
            " 'decrease of unequality of incomes for 10 years since 1985 to 1995 Those '\n",
            " 'dinamic is represents all of the countries on the graph, exept the Germany, '\n",
            " 'where we can see that persentage of diffference of male and female earnings '\n",
            " 'slightly rising in the last 5 years of researching. As we can see, this '\n",
            " 'graph shows us a steady drop of difference between male and female incomes.')\n",
            "('CORRECTED TEXT: As we can see Japans dinamics on the graph, shows stable '\n",
            " 'decrease of unequality of incomes for 10 years since 1985 to 1995.0 Those '\n",
            " 'dinamic is represents all of the countries on the graph, exept the Germany, '\n",
            " 'where we can see that persentage of diffference of male and female earnings '\n",
            " 'slightly rising in the last 5 years of researching. As we can see, this '\n",
            " 'graph shows us a steady drop of difference between male and female incomes.')\n",
            "[1041.0 5.0 82.0 'punct' 1995.0 1995.0\n",
            " 'As we can see Japans dinamics on the graph, shows stable decrease of unequality of incomes for 10 years since 1985 to 1995 Those dinamic is represents all of the countries on the graph, exept the Germany, where we can see that persentage of diffference of male and female earnings slightly rising in the last 5 years of researching. As we can see, this graph shows us a steady drop of difference between male and female incomes.'\n",
            " {59: 'As', 60: 'we', 61: 'can', 62: 'see', 63: 'Japans', 64: 'dinamics', 65: 'on', 66: 'the', 67: 'graph', 68: ',', 69: 'shows', 70: 'stable', 71: 'decrease', 72: 'of', 73: 'unequality', 74: 'of', 75: 'incomes', 76: 'for', 77: '10', 78: 'years', 79: 'since', 80: '1985', 81: 'to', 82: '1995', 83: 'Those', 84: 'dinamic', 85: 'is', 86: 'represents', 87: 'all', 88: 'of', 89: 'the', 90: 'countries', 91: 'on', 92: 'the', 93: 'graph', 94: ',', 95: 'exept', 96: 'the', 97: 'Germany', 98: ',', 99: 'where', 100: 'we', 101: 'can', 102: 'see', 103: 'that', 104: 'persentage', 105: 'of', 106: 'diffference', 107: 'of', 108: 'male', 109: 'and', 110: 'female', 111: 'earnings', 112: 'slightly', 113: 'rising', 114: 'in', 115: 'the', 116: 'last', 117: '5', 118: 'years', 119: 'of', 120: 'researching', 121: '.', 122: 'As', 123: 'we', 124: 'can', 125: 'see', 126: ',', 127: 'this', 128: 'graph', 129: 'shows', 130: 'us', 131: 'a', 132: 'steady', 133: 'drop', 134: 'of', 135: 'difference', 136: 'between', 137: 'male', 138: 'and', 139: 'female', 140: 'incomes', 141: '.'}\n",
            " {59: ['As', 'SCONJ', 'IN', 'mark'], 60: ['we', 'PRON', 'PRP', 'nsubj'], 61: ['can', 'VERB', 'MD', 'aux'], 62: ['see', 'VERB', 'VB', 'advcl'], 63: ['Japans', 'PROPN', 'NNPS', 'compound'], 64: ['dinamics', 'NOUN', 'NNS', 'dobj'], 65: ['on', 'ADP', 'IN', 'prep'], 66: ['the', 'DET', 'DT', 'det'], 67: ['graph', 'NOUN', 'NN', 'pobj'], 68: [',', 'PUNCT', ',', 'punct'], 69: ['shows', 'VERB', 'VBZ', 'ROOT'], 70: ['stable', 'ADJ', 'JJ', 'amod'], 71: ['decrease', 'NOUN', 'NN', 'dobj'], 72: ['of', 'ADP', 'IN', 'prep'], 73: ['unequality', 'NOUN', 'NN', 'pobj'], 74: ['of', 'ADP', 'IN', 'prep'], 75: ['incomes', 'NOUN', 'NNS', 'pobj'], 76: ['for', 'ADP', 'IN', 'prep'], 77: ['10', 'NUM', 'CD', 'nummod'], 78: ['years', 'NOUN', 'NNS', 'pobj'], 79: ['since', 'SCONJ', 'IN', 'prep'], 80: ['1985', 'NUM', 'CD', 'pobj'], 81: ['to', 'ADP', 'IN', 'prep'], 82: ['1995', 'NUM', 'CD', 'pobj'], 83: ['Those', 'DET', 'DT', 'det'], 84: ['dinamic', 'NOUN', 'NN', 'nsubj'], 85: ['is', 'AUX', 'VBZ', 'ROOT'], 86: ['represents', 'VERB', 'VBZ', 'attr'], 87: ['all', 'DET', 'DT', 'dobj'], 88: ['of', 'ADP', 'IN', 'prep'], 89: ['the', 'DET', 'DT', 'det'], 90: ['countries', 'NOUN', 'NNS', 'pobj'], 91: ['on', 'ADP', 'IN', 'prep'], 92: ['the', 'DET', 'DT', 'det'], 93: ['graph', 'NOUN', 'NN', 'pobj'], 94: [',', 'PUNCT', ',', 'punct'], 95: ['exept', 'VERB', 'VBD', 'conj'], 96: ['the', 'DET', 'DT', 'det'], 97: ['Germany', 'PROPN', 'NNP', 'dobj'], 98: [',', 'PUNCT', ',', 'punct'], 99: ['where', 'ADV', 'WRB', 'advmod'], 100: ['we', 'PRON', 'PRP', 'nsubj'], 101: ['can', 'VERB', 'MD', 'aux'], 102: ['see', 'VERB', 'VB', 'relcl'], 103: ['that', 'DET', 'DT', 'det'], 104: ['persentage', 'NOUN', 'NN', 'nsubj'], 105: ['of', 'ADP', 'IN', 'prep'], 106: ['diffference', 'NOUN', 'NN', 'pobj'], 107: ['of', 'ADP', 'IN', 'prep'], 108: ['male', 'ADJ', 'JJ', 'amod'], 109: ['and', 'CCONJ', 'CC', 'cc'], 110: ['female', 'ADJ', 'JJ', 'conj'], 111: ['earnings', 'NOUN', 'NNS', 'pobj'], 112: ['slightly', 'ADV', 'RB', 'advmod'], 113: ['rising', 'VERB', 'VBG', 'ccomp'], 114: ['in', 'ADP', 'IN', 'prep'], 115: ['the', 'DET', 'DT', 'det'], 116: ['last', 'ADJ', 'JJ', 'amod'], 117: ['5', 'NUM', 'CD', 'nummod'], 118: ['years', 'NOUN', 'NNS', 'pobj'], 119: ['of', 'ADP', 'IN', 'prep'], 120: ['researching', 'VERB', 'VBG', 'pcomp'], 121: ['.', 'PUNCT', '.', 'punct'], 122: ['As', 'SCONJ', 'IN', 'mark'], 123: ['we', 'PRON', 'PRP', 'nsubj'], 124: ['can', 'VERB', 'MD', 'aux'], 125: ['see', 'VERB', 'VB', 'advcl'], 126: [',', 'PUNCT', ',', 'punct'], 127: ['this', 'DET', 'DT', 'det'], 128: ['graph', 'NOUN', 'NN', 'nsubj'], 129: ['shows', 'VERB', 'VBZ', 'ROOT'], 130: ['us', 'PRON', 'PRP', 'dative'], 131: ['a', 'DET', 'DT', 'det'], 132: ['steady', 'ADJ', 'JJ', 'amod'], 133: ['drop', 'NOUN', 'NN', 'dobj'], 134: ['of', 'ADP', 'IN', 'prep'], 135: ['difference', 'NOUN', 'NN', 'pobj'], 136: ['between', 'ADP', 'IN', 'prep'], 137: ['male', 'ADJ', 'JJ', 'amod'], 138: ['and', 'CCONJ', 'CC', 'cc'], 139: ['female', 'ADJ', 'JJ', 'conj'], 140: ['incomes', 'NOUN', 'NNS', 'pobj'], 141: ['.', 'PUNCT', '.', 'punct']}\n",
            " 'As we can see Japans dinamics on the graph, shows stable decrease of unequality of incomes for 10 years since 1985 to 1995.0 Those dinamic is represents all of the countries on the graph, exept the Germany, where we can see that persentage of diffference of male and female earnings slightly rising in the last 5 years of researching. As we can see, this graph shows us a steady drop of difference between male and female incomes.'\n",
            " {59: 'As', 60: 'we', 61: 'can', 62: 'see', 63: 'Japans', 64: 'dinamics', 65: 'on', 66: 'the', 67: 'graph', 68: ',', 69: 'shows', 70: 'stable', 71: 'decrease', 72: 'of', 73: 'unequality', 74: 'of', 75: 'incomes', 76: 'for', 77: '10', 78: 'years', 79: 'since', 80: '1985', 81: 'to', 82: '1995', 83: '.', 84: '0', 85: 'Those', 86: 'dinamic', 87: 'is', 88: 'represents', 89: 'all', 90: 'of', 91: 'the', 92: 'countries', 93: 'on', 94: 'the', 95: 'graph', 96: ',', 97: 'exept', 98: 'the', 99: 'Germany', 100: ',', 101: 'where', 102: 'we', 103: 'can', 104: 'see', 105: 'that', 106: 'persentage', 107: 'of', 108: 'diffference', 109: 'of', 110: 'male', 111: 'and', 112: 'female', 113: 'earnings', 114: 'slightly', 115: 'rising', 116: 'in', 117: 'the', 118: 'last', 119: '5', 120: 'years', 121: 'of', 122: 'researching', 123: '.', 124: 'As', 125: 'we', 126: 'can', 127: 'see', 128: ',', 129: 'this', 130: 'graph', 131: 'shows', 132: 'us', 133: 'a', 134: 'steady', 135: 'drop', 136: 'of', 137: 'difference', 138: 'between', 139: 'male', 140: 'and', 141: 'female', 142: 'incomes', 143: '.'}\n",
            " {59: ['As', 'SCONJ', 'IN', 'mark'], 60: ['we', 'PRON', 'PRP', 'nsubj'], 61: ['can', 'VERB', 'MD', 'aux'], 62: ['see', 'VERB', 'VB', 'advcl'], 63: ['Japans', 'PROPN', 'NNPS', 'compound'], 64: ['dinamics', 'NOUN', 'NNS', 'dobj'], 65: ['on', 'ADP', 'IN', 'prep'], 66: ['the', 'DET', 'DT', 'det'], 67: ['graph', 'NOUN', 'NN', 'pobj'], 68: [',', 'PUNCT', ',', 'punct'], 69: ['shows', 'VERB', 'VBZ', 'ROOT'], 70: ['stable', 'ADJ', 'JJ', 'amod'], 71: ['decrease', 'NOUN', 'NN', 'dobj'], 72: ['of', 'ADP', 'IN', 'prep'], 73: ['unequality', 'NOUN', 'NN', 'pobj'], 74: ['of', 'ADP', 'IN', 'prep'], 75: ['incomes', 'NOUN', 'NNS', 'pobj'], 76: ['for', 'ADP', 'IN', 'prep'], 77: ['10', 'NUM', 'CD', 'nummod'], 78: ['years', 'NOUN', 'NNS', 'pobj'], 79: ['since', 'SCONJ', 'IN', 'prep'], 80: ['1985', 'NUM', 'CD', 'pobj'], 81: ['to', 'ADP', 'IN', 'prep'], 82: ['1995', 'NUM', 'CD', 'pobj'], 83: ['.', 'PUNCT', '.', 'punct'], 84: ['0', 'PUNCT', 'NFP', 'ROOT'], 85: ['Those', 'DET', 'DT', 'det'], 86: ['dinamic', 'NOUN', 'NN', 'nsubj'], 87: ['is', 'AUX', 'VBZ', 'ROOT'], 88: ['represents', 'VERB', 'VBZ', 'attr'], 89: ['all', 'DET', 'DT', 'dobj'], 90: ['of', 'ADP', 'IN', 'prep'], 91: ['the', 'DET', 'DT', 'det'], 92: ['countries', 'NOUN', 'NNS', 'pobj'], 93: ['on', 'ADP', 'IN', 'prep'], 94: ['the', 'DET', 'DT', 'det'], 95: ['graph', 'NOUN', 'NN', 'pobj'], 96: [',', 'PUNCT', ',', 'punct'], 97: ['exept', 'VERB', 'VBD', 'conj'], 98: ['the', 'DET', 'DT', 'det'], 99: ['Germany', 'PROPN', 'NNP', 'dobj'], 100: [',', 'PUNCT', ',', 'punct'], 101: ['where', 'ADV', 'WRB', 'advmod'], 102: ['we', 'PRON', 'PRP', 'nsubj'], 103: ['can', 'VERB', 'MD', 'aux'], 104: ['see', 'VERB', 'VB', 'relcl'], 105: ['that', 'DET', 'DT', 'det'], 106: ['persentage', 'NOUN', 'NN', 'nsubj'], 107: ['of', 'ADP', 'IN', 'prep'], 108: ['diffference', 'NOUN', 'NN', 'pobj'], 109: ['of', 'ADP', 'IN', 'prep'], 110: ['male', 'ADJ', 'JJ', 'amod'], 111: ['and', 'CCONJ', 'CC', 'cc'], 112: ['female', 'ADJ', 'JJ', 'conj'], 113: ['earnings', 'NOUN', 'NNS', 'pobj'], 114: ['slightly', 'ADV', 'RB', 'advmod'], 115: ['rising', 'VERB', 'VBG', 'ccomp'], 116: ['in', 'ADP', 'IN', 'prep'], 117: ['the', 'DET', 'DT', 'det'], 118: ['last', 'ADJ', 'JJ', 'amod'], 119: ['5', 'NUM', 'CD', 'nummod'], 120: ['years', 'NOUN', 'NNS', 'pobj'], 121: ['of', 'ADP', 'IN', 'prep'], 122: ['researching', 'VERB', 'VBG', 'pcomp'], 123: ['.', 'PUNCT', '.', 'punct'], 124: ['As', 'SCONJ', 'IN', 'mark'], 125: ['we', 'PRON', 'PRP', 'nsubj'], 126: ['can', 'VERB', 'MD', 'aux'], 127: ['see', 'VERB', 'VB', 'advcl'], 128: [',', 'PUNCT', ',', 'punct'], 129: ['this', 'DET', 'DT', 'det'], 130: ['graph', 'NOUN', 'NN', 'nsubj'], 131: ['shows', 'VERB', 'VBZ', 'ROOT'], 132: ['us', 'PRON', 'PRP', 'dative'], 133: ['a', 'DET', 'DT', 'det'], 134: ['steady', 'ADJ', 'JJ', 'amod'], 135: ['drop', 'NOUN', 'NN', 'dobj'], 136: ['of', 'ADP', 'IN', 'prep'], 137: ['difference', 'NOUN', 'NN', 'pobj'], 138: ['between', 'ADP', 'IN', 'prep'], 139: ['male', 'ADJ', 'JJ', 'amod'], 140: ['and', 'CCONJ', 'CC', 'cc'], 141: ['female', 'ADJ', 'JJ', 'conj'], 142: ['incomes', 'NOUN', 'NNS', 'pobj'], 143: ['.', 'PUNCT', '.', 'punct']}]\n",
            "(\"USER'S TEXT: In UK difference decreased from around 36 by 13 percent, \"\n",
            " 'similar fall was in USA, but where it started with 39 percent and ended with '\n",
            " '20.')\n",
            "('CORRECTED TEXT: In UK difference decreased from around 36 by 13 percent. A '\n",
            " 'similar fall was in USA, but where it started with 39 percent and ended with '\n",
            " '20.')\n",
            "'CORRECTION: . A'\n",
            "'ERROR AREA: ,'\n",
            "'TAG: punctuation'\n",
            "(\"USER'S TEXT: Moreover, school eduction or at least elementary one is \"\n",
            " 'obligatory in most countries, it means everybody should have this education. '\n",
            " 'It will be very extremely unfair if all people have to study, but not '\n",
            " 'everyone could afford it.')\n",
            "('CORRECTED TEXT: Moreover, school eduction or at least elementary one is '\n",
            " 'obligatory in most countries. That means everybody should have this '\n",
            " 'education. It will be very extremely unfair if all people have to study, but '\n",
            " 'not everyone could afford it.')\n",
            "'CORRECTION: . That'\n",
            "'ERROR AREA: , it'\n",
            "'TAG: punctuation'\n",
            "(\"USER'S TEXT: 37 On the one hand, there will be difficult for government to \"\n",
            " 'pay for all schools and universities in their countries. Because if the '\n",
            " 'education is free it must be paid from the government.')\n",
            "('CORRECTED TEXT: 37 On the one hand, there will be difficult for government '\n",
            " 'to pay for all schools and universities in their countries because if the '\n",
            " 'education is free it must be paid from the government.')\n",
            "'CORRECTION: because'\n",
            "'ERROR AREA: . Because'\n",
            "'TAG: punctuation'\n",
            "(\"USER'S TEXT: First, a free education have to be given those which passed \"\n",
            " 'their exams in school well. Because, some of students do not want to study '\n",
            " 'at all, and for governments it will be just a waste of money that can be '\n",
            " 'treated for something usefull.')\n",
            "('CORRECTED TEXT: First, a free education have to be given those which passed '\n",
            " 'their exams in school well, because some of students do not want to study at '\n",
            " 'all, and for governments it will be just a waste of money that can be '\n",
            " 'treated for something usefull.')\n",
            "'CORRECTION: , because'\n",
            "'ERROR AREA: . Because ,'\n",
            "'TAG: punctuation'\n",
            "(\"USER'S TEXT: Although there are some people who justify education being \"\n",
            " 'expensive explaining that it is done so the schools, colleges and '\n",
            " 'universities are not overcrowded and there will be enough space for the ones '\n",
            " 'who are willing to study. But I assume this is completely wrong and this is '\n",
            " 'a duty of government to establish favourable conditions for everyone and to '\n",
            " 'provide enough places and opportunities for people to be educated.')\n",
            "('CORRECTED TEXT: Although there are some people who justify education being '\n",
            " 'expensive explaining that it is done so the schools, colleges and '\n",
            " 'universities are not overcrowded and there will be enough space for the ones '\n",
            " 'who are willing to study, I assume this is completely wrong and this is a '\n",
            " 'duty of government to establish favourable conditions for everyone and to '\n",
            " 'provide enough places and opportunities for people to be educated.')\n",
            "'CORRECTION: ,'\n",
            "'ERROR AREA: . But'\n",
            "'TAG: punctuation'\n",
            "(\"USER'S TEXT: Japan is also a bit different from the other countries, since \"\n",
            " 'their decrees has remained. Gradual throughout the whole given period, as '\n",
            " 'other countries had a slightly more slow change after 1990 almost becoming '\n",
            " 'stabilized.')\n",
            "('CORRECTED TEXT: Japan is also a bit different from the other countries, '\n",
            " 'since their decrees has remained gradual throughout the whole given period, '\n",
            " 'as other countries had a slightly more slow change after 1990 almost '\n",
            " 'becoming stabilized.')\n",
            "'CORRECTION: gradual'\n",
            "'ERROR AREA: . Gradual'\n",
            "'TAG: punctuation'\n",
            "(\"USER'S TEXT: In conclusion. i want to say that in today's world of equar \"\n",
            " 'rights, everyone should receive education.')\n",
            "(\"CORRECTED TEXT: In conclusion, I want to say that in today's world of equar \"\n",
            " 'rights, everyone should receive education.')\n",
            "'CORRECTION: , I'\n",
            "'ERROR AREA: . i'\n",
            "'TAG: punctuation'\n",
            "(\"USER'S TEXT: One the other hand, there are some basic thing that every \"\n",
            " 'people must know. Their language, bare phisics low, math, cultural features '\n",
            " 'and more.')\n",
            "('CORRECTED TEXT: One the other hand, there are some basic thing that every '\n",
            " 'people must know э=-@ their language, bare phisics low, math, cultural '\n",
            " 'features and more.')\n",
            "'CORRECTION: э=-@ their'\n",
            "'ERROR AREA: . Their'\n",
            "'TAG: punctuation'\n",
            "(\"USER'S TEXT: For instance, there was one of the most famous Russian \"\n",
            " 'scientists Michail Lomonosov; he was from average family. But he was very '\n",
            " 'talented and the government invested in his skills.')\n",
            "('CORRECTED TEXT: For instance, there was one of the most famous Russian '\n",
            " 'scientists Michail Lomonosov. He was from average family. But he was very '\n",
            " 'talented and the government invested in his skills.')\n",
            "'CORRECTION: . He'\n",
            "'ERROR AREA: ; he'\n",
            "'TAG: punctuation'\n",
            "(\"USER'S TEXT: Looking at other countries can be seen that there is no high \"\n",
            " 'peaks of getting income. For instance at the beginning UK had around 36 '\n",
            " 'percent but what happens further, it goes down fairly fast and in 1995 they '\n",
            " 'had less than 25 percent.')\n",
            "('CORRECTED TEXT: Looking at other countries, it can be seen that there is no '\n",
            " 'high peaks of getting income. For instance at the beginning UK had around 36 '\n",
            " 'percent but what happens further, it goes down fairly fast and in 1995 they '\n",
            " 'had less than 25 percent.')\n",
            "'CORRECTION: countries, it'\n",
            "'ERROR AREA: countries'\n",
            "'TAG: punctuation'\n",
            "(\"USER'S TEXT: In the other countries was the opposite situation. The earning \"\n",
            " 'in New Zealand, the Usa, the UK and Germany stood at 20, 30, approximately '\n",
            " '32.5 and 10 percent respectively.')\n",
            "('CORRECTED TEXT: In the other countries, it was the opposite situation. The '\n",
            " 'earning in New Zealand, the Usa, the UK and Germany stood at 20, 30, '\n",
            " 'approximately 32.5 and 10 percent respectively.')\n",
            "'CORRECTION: countries, it'\n",
            "'ERROR AREA: countries'\n",
            "'TAG: punctuation'\n",
            "(\"USER'S TEXT: So in this essay will be demonstrates both points of view. One \"\n",
            " 'of the positions is that education is a basic human right.')\n",
            "('CORRECTED TEXT: So in this essay, I will be demonstrates both points of '\n",
            " 'view. One of the positions is that education is a basic human right.')\n",
            "'CORRECTION: essay, I'\n",
            "'ERROR AREA: essay'\n",
            "'TAG: punctuation'\n",
            "(\"USER'S TEXT: Firstly, it is impossible to exept liers and kind of bad \"\n",
            " 'behaviour, for example, giving money to teachers, tutors or professors for '\n",
            " 'getting better place in better school or university and in the future for '\n",
            " 'getting the highest marks. The problem refers to luxurity again, because the '\n",
            " 'man who has more money will be more sucsessfull.')\n",
            "('CORRECTED TEXT: Firstly, it is impossible to exept liers and kind of bad '\n",
            " 'behaviour, for example, giving money to teachers, tutors or professors for '\n",
            " 'getting better place in better school or university and in the future, to '\n",
            " 'get the highest marks. The problem refers to luxurity again, because the man '\n",
            " 'who has more money will be more sucsessfull.')\n",
            "'CORRECTION: , to get'\n",
            "'ERROR AREA: for getting'\n",
            "'TAG: punctuation'\n",
            "(\"USER'S TEXT: the impressive results among others show countries like new \"\n",
            " 'zealand an uk. where the diffetnece in earnig between men and women is lower '\n",
            " 'than other developed countries.')\n",
            "('CORRECTED TEXT: the impressive results among others show countries like New '\n",
            " 'Zealand and the UK, where the diffetnece in earnig between men and women is '\n",
            " 'lower than other developed countries.')\n",
            "'CORRECTION: New Zealand and the UK,'\n",
            "'ERROR AREA: new zealand an uk . where the'\n",
            "'TAG: punctuation'\n",
            "(\"USER'S TEXT: From my humble opinion, every person should have the same \"\n",
            " 'abilities to study only depends on who they are want to be in the future, '\n",
            " 'and hot how much money their family ready to spend it on. Moreover, i truly '\n",
            " 'believe, that free education gives opportunity to people, who can not afford '\n",
            " 'paying for studying, but who is truly in getting knowledge and dreaming to '\n",
            " 'become the best.')\n",
            "('CORRECTED TEXT: From my humble opinion, every person should have the same '\n",
            " 'abilities to study. It only depends on who they are want to be in the '\n",
            " 'future, and hot how much money their family ready to spend it on. Moreover, '\n",
            " 'i truly believe, that free education gives opportunity to people, who can '\n",
            " 'not afford paying for studying, but who is truly in getting knowledge and '\n",
            " 'dreaming to become the best.')\n",
            "'CORRECTION: study. It'\n",
            "'ERROR AREA: study'\n",
            "'TAG: punctuation'\n",
            "(\"USER'S TEXT: One part think, that education should be provided for small \"\n",
            " 'elite gropus, but another one, that include me, think, that it must be '\n",
            " 'provided for everyone. Personally I am convieced, that the earlier steps of '\n",
            " 'education is essential right for every person.')\n",
            "('CORRECTED TEXT: One part thinks that education should be provided for small '\n",
            " 'elite gropus, but another one, that include me, think, that it must be '\n",
            " 'provided for everyone. Personally I am convieced, that the earlier steps of '\n",
            " 'education is essential right for every person.')\n",
            "'CORRECTION: thinks'\n",
            "'ERROR AREA: think ,'\n",
            "'TAG: punctuation'\n",
            "(\"USER'S TEXT: One part think, that education should be provided for small \"\n",
            " 'elite gropus, but another one, that include me, think, that it must be '\n",
            " 'provided for everyone. Personally I am convieced, that the earlier steps of '\n",
            " 'education is essential right for every person.')\n",
            "('CORRECTED TEXT: One part think, that education should be provided for small '\n",
            " 'elite gropus, but another one, that include me, thinks that it must be '\n",
            " 'provided for everyone. Personally I am convieced, that the earlier steps of '\n",
            " 'education is essential right for every person.')\n",
            "'CORRECTION: thinks'\n",
            "'ERROR AREA: think ,'\n",
            "'TAG: punctuation'\n",
            "\"USER'S TEXT: In Japan, USA and UK did not exceeded 8%.\"\n",
            "'CORRECTED TEXT: In Japan, USA and UK, it did not exceeded 8%.'\n",
            "'CORRECTION: UK, it'\n",
            "'ERROR AREA: UK'\n",
            "'TAG: punctuation'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in solo_punct['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "twarNQpSf5wy"
      },
      "execution_count": 494,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fsbh8zeWfUWN",
        "outputId": "538fbb6b-0a90-45a7-89b1-687443c49e62"
      },
      "execution_count": 495,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      text_id  sentence_id  token_id mistake_type          error_span  \\\n",
              "16      731.0          6.0     128.0        punct                 78%   \n",
              "127     592.0          6.0     128.0        punct              1995.0   \n",
              "140    1041.0          5.0      82.0        punct              1995.0   \n",
              "275     344.0          8.0     187.0        punct                   ,   \n",
              "921     803.0          6.0     112.0        punct                , it   \n",
              "1083    370.0          4.0      65.0        punct           . Because   \n",
              "1096    323.0         14.0     247.0        punct          . Because,   \n",
              "1097   1598.0         10.0     285.0        punct               . But   \n",
              "1100    852.0          5.0     137.0        punct           . Gradual   \n",
              "1101   1509.0         16.0     275.0        punct                 . i   \n",
              "1107   1154.0         11.0     218.0        punct             . Their   \n",
              "1119    505.0         13.0     232.0        punct                ; he   \n",
              "1481    900.0          6.0     126.0        punct           countries   \n",
              "1483    943.0          5.0      90.0        punct           countries   \n",
              "1725    273.0          6.0      63.0        punct               essay   \n",
              "1811   1570.0         10.0     172.0        punct         for getting   \n",
              "2262    728.0         11.0     187.0        punct  new zealand an uk.   \n",
              "2761   1503.0         12.0     301.0        punct               study   \n",
              "2916   1404.0          3.0      38.0        punct              think,   \n",
              "2917   1404.0          3.0      58.0        punct              think,   \n",
              "3010    858.0         12.0     186.0        punct                  UK   \n",
              "\n",
              "                   correction  \\\n",
              "16                      , 78%   \n",
              "127                    1995.0   \n",
              "140                    1995.0   \n",
              "275                       . A   \n",
              "921                    . That   \n",
              "1083                  because   \n",
              "1096                , because   \n",
              "1097                        ,   \n",
              "1100                  gradual   \n",
              "1101                      , I   \n",
              "1107               э=-@ their   \n",
              "1119                     . He   \n",
              "1481            countries, it   \n",
              "1483            countries, it   \n",
              "1725                 essay, I   \n",
              "1811                 , to get   \n",
              "2262  New Zealand and the UK,   \n",
              "2761                study. It   \n",
              "2916                   thinks   \n",
              "2917                   thinks   \n",
              "3010                   UK, it   \n",
              "\n",
              "                                              full_text  \\\n",
              "16    As Africa Today had recenly reported 78% of st...   \n",
              "127   Only germany saw a slight increase from 1990 t...   \n",
              "140   As we can see Japans dinamics on the graph, sh...   \n",
              "275   In UK difference decreased from around 36 by 1...   \n",
              "921   Moreover, school eduction or at least elementa...   \n",
              "1083  37 On the one hand, there will be difficult fo...   \n",
              "1096  First, a free education have to be given those...   \n",
              "1097  Although there are some people who justify edu...   \n",
              "1100  Japan is also a bit different from the other c...   \n",
              "1101  In conclusion. i want to say that in today's w...   \n",
              "1107  One the other hand, there are some basic thing...   \n",
              "1119  For instance, there was one of the most famous...   \n",
              "1481  Looking at other countries can be seen that th...   \n",
              "1483  In the other countries was the opposite situat...   \n",
              "1725  So in this essay will be demonstrates both poi...   \n",
              "1811  Firstly, it is impossible to exept liers and k...   \n",
              "2262  the impressive results among others show count...   \n",
              "2761  From my humble opinion, every person should ha...   \n",
              "2916  One part think, that education should be provi...   \n",
              "2917  One part think, that education should be provi...   \n",
              "3010          In Japan, USA and UK did not exceeded 8%.   \n",
              "\n",
              "                                             tokens_ids  \\\n",
              "16    {122: 'As', 123: 'Africa', 124: 'Today', 125: ...   \n",
              "127   {119: 'Only', 120: 'germany', 121: 'saw', 122:...   \n",
              "140   {59: 'As', 60: 'we', 61: 'can', 62: 'see', 63:...   \n",
              "275   {177: 'In', 178: 'UK', 179: 'difference', 180:...   \n",
              "921   {98: 'Moreover', 99: ',', 100: 'school', 101: ...   \n",
              "1083  {43: '37', 44: 'On', 45: 'the', 46: 'one', 47:...   \n",
              "1096  {230: 'First', 231: ',', 232: 'a', 233: 'free'...   \n",
              "1097  {246: 'Although', 247: 'there', 248: 'are', 24...   \n",
              "1100  {121: 'Japan', 122: 'is', 123: 'also', 124: 'a...   \n",
              "1101  {273: 'In', 274: 'conclusion', 275: '.', 276: ...   \n",
              "1107  {203: 'One', 204: 'the', 205: 'other', 206: 'h...   \n",
              "1119  {218: 'For', 219: 'instance', 220: ',', 221: '...   \n",
              "1481  {123: 'Looking', 124: 'at', 125: 'other', 126:...   \n",
              "1483  {87: 'In', 88: 'the', 89: 'other', 90: 'countr...   \n",
              "1725  {60: 'So', 61: 'in', 62: 'this', 63: 'essay', ...   \n",
              "1811  {134: 'Firstly', 135: ',', 136: 'it', 137: 'is...   \n",
              "2262  {179: 'the', 180: 'impressive', 181: 'results'...   \n",
              "2761  {288: 'From', 289: 'my', 290: 'humble', 291: '...   \n",
              "2916  {36: 'One', 37: 'part', 38: 'think', 39: ',', ...   \n",
              "2917  {36: 'One', 37: 'part', 38: 'think', 39: ',', ...   \n",
              "3010  {181: 'In', 182: 'Japan', 183: ',', 184: 'USA'...   \n",
              "\n",
              "                                           dependencies  \\\n",
              "16    {122: ['As', 'SCONJ', 'IN', 'mark'], 123: ['Af...   \n",
              "127   {119: ['Only', 'ADV', 'RB', 'advmod'], 120: ['...   \n",
              "140   {59: ['As', 'SCONJ', 'IN', 'mark'], 60: ['we',...   \n",
              "275   {177: ['In', 'ADP', 'IN', 'prep'], 178: ['UK',...   \n",
              "921   {98: ['Moreover', 'ADV', 'RB', 'advmod'], 99: ...   \n",
              "1083  {43: ['37', 'NUM', 'CD', 'ROOT'], 44: ['On', '...   \n",
              "1096  {230: ['First', 'ADV', 'RB', 'advmod'], 231: [...   \n",
              "1097  {246: ['Although', 'SCONJ', 'IN', 'mark'], 247...   \n",
              "1100  {121: ['Japan', 'PROPN', 'NNP', 'nsubj'], 122:...   \n",
              "1101  {273: ['In', 'ADP', 'IN', 'ROOT'], 274: ['conc...   \n",
              "1107  {203: ['One', 'NUM', 'CD', 'nummod'], 204: ['t...   \n",
              "1119  {218: ['For', 'ADP', 'IN', 'prep'], 219: ['ins...   \n",
              "1481  {123: ['Looking', 'VERB', 'VBG', 'csubjpass'],...   \n",
              "1483  {87: ['In', 'ADP', 'IN', 'prep'], 88: ['the', ...   \n",
              "1725  {60: ['So', 'ADV', 'RB', 'cc'], 61: ['in', 'AD...   \n",
              "1811  {134: ['Firstly', 'ADV', 'RB', 'advmod'], 135:...   \n",
              "2262  {179: ['the', 'DET', 'DT', 'det'], 180: ['impr...   \n",
              "2761  {288: ['From', 'ADP', 'IN', 'prep'], 289: ['my...   \n",
              "2916  {36: ['One', 'NUM', 'CD', 'nummod'], 37: ['par...   \n",
              "2917  {36: ['One', 'NUM', 'CD', 'nummod'], 37: ['par...   \n",
              "3010  {181: ['In', 'ADP', 'IN', 'prep'], 182: ['Japa...   \n",
              "\n",
              "                                         corrected_text  \\\n",
              "16    As Africa Today had recenly reported, 78% of s...   \n",
              "127   Only germany saw a slight increase from 1990 t...   \n",
              "140   As we can see Japans dinamics on the graph, sh...   \n",
              "275   In UK difference decreased from around 36 by 1...   \n",
              "921   Moreover, school eduction or at least elementa...   \n",
              "1083  37 On the one hand, there will be difficult fo...   \n",
              "1096  First, a free education have to be given those...   \n",
              "1097  Although there are some people who justify edu...   \n",
              "1100  Japan is also a bit different from the other c...   \n",
              "1101  In conclusion, I want to say that in today's w...   \n",
              "1107  One the other hand, there are some basic thing...   \n",
              "1119  For instance, there was one of the most famous...   \n",
              "1481  Looking at other countries, it can be seen tha...   \n",
              "1483  In the other countries, it was the opposite si...   \n",
              "1725  So in this essay, I will be demonstrates both ...   \n",
              "1811  Firstly, it is impossible to exept liers and k...   \n",
              "2262  the impressive results among others show count...   \n",
              "2761  From my humble opinion, every person should ha...   \n",
              "2916  One part thinks that education should be provi...   \n",
              "2917  One part think, that education should be provi...   \n",
              "3010      In Japan, USA and UK, it did not exceeded 8%.   \n",
              "\n",
              "                                     corrected_with_ids  \\\n",
              "16    {122: 'As', 123: 'Africa', 124: 'Today', 125: ...   \n",
              "127   {119: 'Only', 120: 'germany', 121: 'saw', 122:...   \n",
              "140   {59: 'As', 60: 'we', 61: 'can', 62: 'see', 63:...   \n",
              "275   {177: 'In', 178: 'UK', 179: 'difference', 180:...   \n",
              "921   {98: 'Moreover', 99: ',', 100: 'school', 101: ...   \n",
              "1083  {43: '37', 44: 'On', 45: 'the', 46: 'one', 47:...   \n",
              "1096  {230: 'First', 231: ',', 232: 'a', 233: 'free'...   \n",
              "1097  {246: 'Although', 247: 'there', 248: 'are', 24...   \n",
              "1100  {121: 'Japan', 122: 'is', 123: 'also', 124: 'a...   \n",
              "1101  {273: 'In', 274: 'conclusion', 275: ',', 276: ...   \n",
              "1107  {203: 'One', 204: 'the', 205: 'other', 206: 'h...   \n",
              "1119  {218: 'For', 219: 'instance', 220: ',', 221: '...   \n",
              "1481  {123: 'Looking', 124: 'at', 125: 'other', 126:...   \n",
              "1483  {87: 'In', 88: 'the', 89: 'other', 90: 'countr...   \n",
              "1725  {60: 'So', 61: 'in', 62: 'this', 63: 'essay', ...   \n",
              "1811  {134: 'Firstly', 135: ',', 136: 'it', 137: 'is...   \n",
              "2262  {179: 'the', 180: 'impressive', 181: 'results'...   \n",
              "2761  {288: 'From', 289: 'my', 290: 'humble', 291: '...   \n",
              "2916  {36: 'One', 37: 'part', 38: 'thinks', 39: 'tha...   \n",
              "2917  {36: 'One', 37: 'part', 38: 'think', 39: ',', ...   \n",
              "3010  {181: 'In', 182: 'Japan', 183: ',', 184: 'USA'...   \n",
              "\n",
              "                                  dependencies_heptabot  \n",
              "16    {122: ['As', 'SCONJ', 'IN', 'mark'], 123: ['Af...  \n",
              "127   {119: ['Only', 'ADV', 'RB', 'advmod'], 120: ['...  \n",
              "140   {59: ['As', 'SCONJ', 'IN', 'mark'], 60: ['we',...  \n",
              "275   {177: ['In', 'ADP', 'IN', 'prep'], 178: ['UK',...  \n",
              "921   {98: ['Moreover', 'ADV', 'RB', 'advmod'], 99: ...  \n",
              "1083  {43: ['37', 'NUM', 'CD', 'ROOT'], 44: ['On', '...  \n",
              "1096  {230: ['First', 'ADV', 'RB', 'advmod'], 231: [...  \n",
              "1097  {246: ['Although', 'SCONJ', 'IN', 'mark'], 247...  \n",
              "1100  {121: ['Japan', 'PROPN', 'NNP', 'nsubj'], 122:...  \n",
              "1101  {273: ['In', 'ADP', 'IN', 'prep'], 274: ['conc...  \n",
              "1107  {203: ['One', 'NUM', 'CD', 'nummod'], 204: ['t...  \n",
              "1119  {218: ['For', 'ADP', 'IN', 'prep'], 219: ['ins...  \n",
              "1481  {123: ['Looking', 'VERB', 'VBG', 'advcl'], 124...  \n",
              "1483  {87: ['In', 'ADP', 'IN', 'prep'], 88: ['the', ...  \n",
              "1725  {60: ['So', 'ADV', 'RB', 'advmod'], 61: ['in',...  \n",
              "1811  {134: ['Firstly', 'ADV', 'RB', 'advmod'], 135:...  \n",
              "2262  {179: ['the', 'DET', 'DT', 'det'], 180: ['impr...  \n",
              "2761  {288: ['From', 'ADP', 'IN', 'prep'], 289: ['my...  \n",
              "2916  {36: ['One', 'NUM', 'CD', 'nummod'], 37: ['par...  \n",
              "2917  {36: ['One', 'NUM', 'CD', 'nummod'], 37: ['par...  \n",
              "3010  {181: ['In', 'ADP', 'IN', 'prep'], 182: ['Japa...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2160e3d2-6ffa-4c67-9e36-18ae161ba59d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>sentence_id</th>\n",
              "      <th>token_id</th>\n",
              "      <th>mistake_type</th>\n",
              "      <th>error_span</th>\n",
              "      <th>correction</th>\n",
              "      <th>full_text</th>\n",
              "      <th>tokens_ids</th>\n",
              "      <th>dependencies</th>\n",
              "      <th>corrected_text</th>\n",
              "      <th>corrected_with_ids</th>\n",
              "      <th>dependencies_heptabot</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>731.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>78%</td>\n",
              "      <td>, 78%</td>\n",
              "      <td>As Africa Today had recenly reported 78% of st...</td>\n",
              "      <td>{122: 'As', 123: 'Africa', 124: 'Today', 125: ...</td>\n",
              "      <td>{122: ['As', 'SCONJ', 'IN', 'mark'], 123: ['Af...</td>\n",
              "      <td>As Africa Today had recenly reported, 78% of s...</td>\n",
              "      <td>{122: 'As', 123: 'Africa', 124: 'Today', 125: ...</td>\n",
              "      <td>{122: ['As', 'SCONJ', 'IN', 'mark'], 123: ['Af...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>592.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>1995.0</td>\n",
              "      <td>1995.0</td>\n",
              "      <td>Only germany saw a slight increase from 1990 t...</td>\n",
              "      <td>{119: 'Only', 120: 'germany', 121: 'saw', 122:...</td>\n",
              "      <td>{119: ['Only', 'ADV', 'RB', 'advmod'], 120: ['...</td>\n",
              "      <td>Only germany saw a slight increase from 1990 t...</td>\n",
              "      <td>{119: 'Only', 120: 'germany', 121: 'saw', 122:...</td>\n",
              "      <td>{119: ['Only', 'ADV', 'RB', 'advmod'], 120: ['...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>1041.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>82.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>1995.0</td>\n",
              "      <td>1995.0</td>\n",
              "      <td>As we can see Japans dinamics on the graph, sh...</td>\n",
              "      <td>{59: 'As', 60: 'we', 61: 'can', 62: 'see', 63:...</td>\n",
              "      <td>{59: ['As', 'SCONJ', 'IN', 'mark'], 60: ['we',...</td>\n",
              "      <td>As we can see Japans dinamics on the graph, sh...</td>\n",
              "      <td>{59: 'As', 60: 'we', 61: 'can', 62: 'see', 63:...</td>\n",
              "      <td>{59: ['As', 'SCONJ', 'IN', 'mark'], 60: ['we',...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>275</th>\n",
              "      <td>344.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>187.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>,</td>\n",
              "      <td>. A</td>\n",
              "      <td>In UK difference decreased from around 36 by 1...</td>\n",
              "      <td>{177: 'In', 178: 'UK', 179: 'difference', 180:...</td>\n",
              "      <td>{177: ['In', 'ADP', 'IN', 'prep'], 178: ['UK',...</td>\n",
              "      <td>In UK difference decreased from around 36 by 1...</td>\n",
              "      <td>{177: 'In', 178: 'UK', 179: 'difference', 180:...</td>\n",
              "      <td>{177: ['In', 'ADP', 'IN', 'prep'], 178: ['UK',...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>921</th>\n",
              "      <td>803.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>, it</td>\n",
              "      <td>. That</td>\n",
              "      <td>Moreover, school eduction or at least elementa...</td>\n",
              "      <td>{98: 'Moreover', 99: ',', 100: 'school', 101: ...</td>\n",
              "      <td>{98: ['Moreover', 'ADV', 'RB', 'advmod'], 99: ...</td>\n",
              "      <td>Moreover, school eduction or at least elementa...</td>\n",
              "      <td>{98: 'Moreover', 99: ',', 100: 'school', 101: ...</td>\n",
              "      <td>{98: ['Moreover', 'ADV', 'RB', 'advmod'], 99: ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1083</th>\n",
              "      <td>370.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>. Because</td>\n",
              "      <td>because</td>\n",
              "      <td>37 On the one hand, there will be difficult fo...</td>\n",
              "      <td>{43: '37', 44: 'On', 45: 'the', 46: 'one', 47:...</td>\n",
              "      <td>{43: ['37', 'NUM', 'CD', 'ROOT'], 44: ['On', '...</td>\n",
              "      <td>37 On the one hand, there will be difficult fo...</td>\n",
              "      <td>{43: '37', 44: 'On', 45: 'the', 46: 'one', 47:...</td>\n",
              "      <td>{43: ['37', 'NUM', 'CD', 'ROOT'], 44: ['On', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1096</th>\n",
              "      <td>323.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>247.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>. Because,</td>\n",
              "      <td>, because</td>\n",
              "      <td>First, a free education have to be given those...</td>\n",
              "      <td>{230: 'First', 231: ',', 232: 'a', 233: 'free'...</td>\n",
              "      <td>{230: ['First', 'ADV', 'RB', 'advmod'], 231: [...</td>\n",
              "      <td>First, a free education have to be given those...</td>\n",
              "      <td>{230: 'First', 231: ',', 232: 'a', 233: 'free'...</td>\n",
              "      <td>{230: ['First', 'ADV', 'RB', 'advmod'], 231: [...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1097</th>\n",
              "      <td>1598.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>285.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>. But</td>\n",
              "      <td>,</td>\n",
              "      <td>Although there are some people who justify edu...</td>\n",
              "      <td>{246: 'Although', 247: 'there', 248: 'are', 24...</td>\n",
              "      <td>{246: ['Although', 'SCONJ', 'IN', 'mark'], 247...</td>\n",
              "      <td>Although there are some people who justify edu...</td>\n",
              "      <td>{246: 'Although', 247: 'there', 248: 'are', 24...</td>\n",
              "      <td>{246: ['Although', 'SCONJ', 'IN', 'mark'], 247...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1100</th>\n",
              "      <td>852.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>. Gradual</td>\n",
              "      <td>gradual</td>\n",
              "      <td>Japan is also a bit different from the other c...</td>\n",
              "      <td>{121: 'Japan', 122: 'is', 123: 'also', 124: 'a...</td>\n",
              "      <td>{121: ['Japan', 'PROPN', 'NNP', 'nsubj'], 122:...</td>\n",
              "      <td>Japan is also a bit different from the other c...</td>\n",
              "      <td>{121: 'Japan', 122: 'is', 123: 'also', 124: 'a...</td>\n",
              "      <td>{121: ['Japan', 'PROPN', 'NNP', 'nsubj'], 122:...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1101</th>\n",
              "      <td>1509.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>275.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>. i</td>\n",
              "      <td>, I</td>\n",
              "      <td>In conclusion. i want to say that in today's w...</td>\n",
              "      <td>{273: 'In', 274: 'conclusion', 275: '.', 276: ...</td>\n",
              "      <td>{273: ['In', 'ADP', 'IN', 'ROOT'], 274: ['conc...</td>\n",
              "      <td>In conclusion, I want to say that in today's w...</td>\n",
              "      <td>{273: 'In', 274: 'conclusion', 275: ',', 276: ...</td>\n",
              "      <td>{273: ['In', 'ADP', 'IN', 'prep'], 274: ['conc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1107</th>\n",
              "      <td>1154.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>218.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>. Their</td>\n",
              "      <td>э=-@ their</td>\n",
              "      <td>One the other hand, there are some basic thing...</td>\n",
              "      <td>{203: 'One', 204: 'the', 205: 'other', 206: 'h...</td>\n",
              "      <td>{203: ['One', 'NUM', 'CD', 'nummod'], 204: ['t...</td>\n",
              "      <td>One the other hand, there are some basic thing...</td>\n",
              "      <td>{203: 'One', 204: 'the', 205: 'other', 206: 'h...</td>\n",
              "      <td>{203: ['One', 'NUM', 'CD', 'nummod'], 204: ['t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1119</th>\n",
              "      <td>505.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>232.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>; he</td>\n",
              "      <td>. He</td>\n",
              "      <td>For instance, there was one of the most famous...</td>\n",
              "      <td>{218: 'For', 219: 'instance', 220: ',', 221: '...</td>\n",
              "      <td>{218: ['For', 'ADP', 'IN', 'prep'], 219: ['ins...</td>\n",
              "      <td>For instance, there was one of the most famous...</td>\n",
              "      <td>{218: 'For', 219: 'instance', 220: ',', 221: '...</td>\n",
              "      <td>{218: ['For', 'ADP', 'IN', 'prep'], 219: ['ins...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1481</th>\n",
              "      <td>900.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>countries</td>\n",
              "      <td>countries, it</td>\n",
              "      <td>Looking at other countries can be seen that th...</td>\n",
              "      <td>{123: 'Looking', 124: 'at', 125: 'other', 126:...</td>\n",
              "      <td>{123: ['Looking', 'VERB', 'VBG', 'csubjpass'],...</td>\n",
              "      <td>Looking at other countries, it can be seen tha...</td>\n",
              "      <td>{123: 'Looking', 124: 'at', 125: 'other', 126:...</td>\n",
              "      <td>{123: ['Looking', 'VERB', 'VBG', 'advcl'], 124...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1483</th>\n",
              "      <td>943.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>countries</td>\n",
              "      <td>countries, it</td>\n",
              "      <td>In the other countries was the opposite situat...</td>\n",
              "      <td>{87: 'In', 88: 'the', 89: 'other', 90: 'countr...</td>\n",
              "      <td>{87: ['In', 'ADP', 'IN', 'prep'], 88: ['the', ...</td>\n",
              "      <td>In the other countries, it was the opposite si...</td>\n",
              "      <td>{87: 'In', 88: 'the', 89: 'other', 90: 'countr...</td>\n",
              "      <td>{87: ['In', 'ADP', 'IN', 'prep'], 88: ['the', ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1725</th>\n",
              "      <td>273.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>63.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>essay</td>\n",
              "      <td>essay, I</td>\n",
              "      <td>So in this essay will be demonstrates both poi...</td>\n",
              "      <td>{60: 'So', 61: 'in', 62: 'this', 63: 'essay', ...</td>\n",
              "      <td>{60: ['So', 'ADV', 'RB', 'cc'], 61: ['in', 'AD...</td>\n",
              "      <td>So in this essay, I will be demonstrates both ...</td>\n",
              "      <td>{60: 'So', 61: 'in', 62: 'this', 63: 'essay', ...</td>\n",
              "      <td>{60: ['So', 'ADV', 'RB', 'advmod'], 61: ['in',...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1811</th>\n",
              "      <td>1570.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>172.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>for getting</td>\n",
              "      <td>, to get</td>\n",
              "      <td>Firstly, it is impossible to exept liers and k...</td>\n",
              "      <td>{134: 'Firstly', 135: ',', 136: 'it', 137: 'is...</td>\n",
              "      <td>{134: ['Firstly', 'ADV', 'RB', 'advmod'], 135:...</td>\n",
              "      <td>Firstly, it is impossible to exept liers and k...</td>\n",
              "      <td>{134: 'Firstly', 135: ',', 136: 'it', 137: 'is...</td>\n",
              "      <td>{134: ['Firstly', 'ADV', 'RB', 'advmod'], 135:...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2262</th>\n",
              "      <td>728.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>187.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>new zealand an uk.</td>\n",
              "      <td>New Zealand and the UK,</td>\n",
              "      <td>the impressive results among others show count...</td>\n",
              "      <td>{179: 'the', 180: 'impressive', 181: 'results'...</td>\n",
              "      <td>{179: ['the', 'DET', 'DT', 'det'], 180: ['impr...</td>\n",
              "      <td>the impressive results among others show count...</td>\n",
              "      <td>{179: 'the', 180: 'impressive', 181: 'results'...</td>\n",
              "      <td>{179: ['the', 'DET', 'DT', 'det'], 180: ['impr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2761</th>\n",
              "      <td>1503.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>301.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>study</td>\n",
              "      <td>study. It</td>\n",
              "      <td>From my humble opinion, every person should ha...</td>\n",
              "      <td>{288: 'From', 289: 'my', 290: 'humble', 291: '...</td>\n",
              "      <td>{288: ['From', 'ADP', 'IN', 'prep'], 289: ['my...</td>\n",
              "      <td>From my humble opinion, every person should ha...</td>\n",
              "      <td>{288: 'From', 289: 'my', 290: 'humble', 291: '...</td>\n",
              "      <td>{288: ['From', 'ADP', 'IN', 'prep'], 289: ['my...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2916</th>\n",
              "      <td>1404.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>think,</td>\n",
              "      <td>thinks</td>\n",
              "      <td>One part think, that education should be provi...</td>\n",
              "      <td>{36: 'One', 37: 'part', 38: 'think', 39: ',', ...</td>\n",
              "      <td>{36: ['One', 'NUM', 'CD', 'nummod'], 37: ['par...</td>\n",
              "      <td>One part thinks that education should be provi...</td>\n",
              "      <td>{36: 'One', 37: 'part', 38: 'thinks', 39: 'tha...</td>\n",
              "      <td>{36: ['One', 'NUM', 'CD', 'nummod'], 37: ['par...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2917</th>\n",
              "      <td>1404.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>think,</td>\n",
              "      <td>thinks</td>\n",
              "      <td>One part think, that education should be provi...</td>\n",
              "      <td>{36: 'One', 37: 'part', 38: 'think', 39: ',', ...</td>\n",
              "      <td>{36: ['One', 'NUM', 'CD', 'nummod'], 37: ['par...</td>\n",
              "      <td>One part think, that education should be provi...</td>\n",
              "      <td>{36: 'One', 37: 'part', 38: 'think', 39: ',', ...</td>\n",
              "      <td>{36: ['One', 'NUM', 'CD', 'nummod'], 37: ['par...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3010</th>\n",
              "      <td>858.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>UK</td>\n",
              "      <td>UK, it</td>\n",
              "      <td>In Japan, USA and UK did not exceeded 8%.</td>\n",
              "      <td>{181: 'In', 182: 'Japan', 183: ',', 184: 'USA'...</td>\n",
              "      <td>{181: ['In', 'ADP', 'IN', 'prep'], 182: ['Japa...</td>\n",
              "      <td>In Japan, USA and UK, it did not exceeded 8%.</td>\n",
              "      <td>{181: 'In', 182: 'Japan', 183: ',', 184: 'USA'...</td>\n",
              "      <td>{181: ['In', 'ADP', 'IN', 'prep'], 182: ['Japa...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2160e3d2-6ffa-4c67-9e36-18ae161ba59d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2160e3d2-6ffa-4c67-9e36-18ae161ba59d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2160e3d2-6ffa-4c67-9e36-18ae161ba59d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 495
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation[punctuation['corrected_text'].apply(lambda x: x not in conj_and_articles['corrected_text'].values)]"
      ],
      "metadata": {
        "id": "Xa_W523hanPh"
      },
      "execution_count": 480,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LGXrEqmze4AK",
        "outputId": "e5faef0a-a937-4d76-f738-3a3c875cd5b3"
      },
      "execution_count": 481,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      text_id  sentence_id  token_id mistake_type          error_span  \\\n",
              "16      731.0          6.0     128.0        punct                 78%   \n",
              "127     592.0          6.0     128.0        punct              1995.0   \n",
              "140    1041.0          5.0      82.0        punct              1995.0   \n",
              "275     344.0          8.0     187.0        punct                   ,   \n",
              "921     803.0          6.0     112.0        punct                , it   \n",
              "1043      6.0          8.0     221.0        punct                   .   \n",
              "1044     15.0          8.0     131.0        punct                   .   \n",
              "1045     25.0          3.0      58.0        punct                   .   \n",
              "1046     54.0          5.0     122.0        punct                   .   \n",
              "1047     58.0          9.0     199.0        punct                   .   \n",
              "1048    111.0          2.0      39.0        punct                   .   \n",
              "1049    130.0          6.0      99.0        punct                   .   \n",
              "1050    246.0          9.0     245.0        punct                   .   \n",
              "1051    260.0          4.0      61.0        punct                   .   \n",
              "1052    261.0         10.0     200.0        punct                   .   \n",
              "1053    374.0         12.0     188.0        punct                   .   \n",
              "1054    454.0         12.0     217.0        punct                   .   \n",
              "1055    637.0          6.0     116.0        punct                   .   \n",
              "1056    646.0         16.0     244.0        punct                   .   \n",
              "1057    668.0         17.0     308.0        punct                   .   \n",
              "1058    728.0          5.0      87.0        punct                   .   \n",
              "1059    786.0          3.0      66.0        punct                   .   \n",
              "1060    793.0          7.0     218.0        punct                   .   \n",
              "1061    853.0          9.0     310.0        punct                   .   \n",
              "1062    956.0         12.0     241.0        punct                   .   \n",
              "1063    963.0          2.0      36.0        punct                   .   \n",
              "1064   1014.0          8.0     107.0        punct                   .   \n",
              "1065   1063.0          5.0     134.0        punct                   .   \n",
              "1066   1245.0         12.0     262.0        punct                   .   \n",
              "1067   1251.0          5.0     103.0        punct                   .   \n",
              "1068   1351.0          1.0      60.0        punct                   .   \n",
              "1069   1369.0          5.0      67.0        punct                   .   \n",
              "1070   1395.0         24.0     399.0        punct                   .   \n",
              "1071   1493.0          6.0     102.0        punct                   .   \n",
              "1072   1501.0         18.0     314.0        punct                   .   \n",
              "1073   1512.0          1.0      27.0        punct                   .   \n",
              "1074   1550.0          4.0      74.0        punct                   .   \n",
              "1075   1550.0         12.0     259.0        punct                   .   \n",
              "1076   1555.0         13.0     205.0        punct                   .   \n",
              "1077   1555.0         14.0     234.0        punct                   .   \n",
              "1083    370.0          4.0      65.0        punct           . Because   \n",
              "1096    323.0         14.0     247.0        punct          . Because,   \n",
              "1097   1598.0         10.0     285.0        punct               . But   \n",
              "1100    852.0          5.0     137.0        punct           . Gradual   \n",
              "1101   1509.0         16.0     275.0        punct                 . i   \n",
              "1107   1154.0         11.0     218.0        punct             . Their   \n",
              "1119    505.0         13.0     232.0        punct                ; he   \n",
              "1481    900.0          6.0     126.0        punct           countries   \n",
              "1483    943.0          5.0      90.0        punct           countries   \n",
              "1725    273.0          6.0      63.0        punct               essay   \n",
              "1811   1570.0         10.0     172.0        punct         for getting   \n",
              "2262    728.0         11.0     187.0        punct  new zealand an uk.   \n",
              "2761   1503.0         12.0     301.0        punct               study   \n",
              "2916   1404.0          3.0      38.0        punct              think,   \n",
              "2917   1404.0          3.0      58.0        punct              think,   \n",
              "3010    858.0         12.0     186.0        punct                  UK   \n",
              "\n",
              "                   correction  \\\n",
              "16                      , 78%   \n",
              "127                    1995.0   \n",
              "140                    1995.0   \n",
              "275                       . A   \n",
              "921                    . That   \n",
              "1043                        ?   \n",
              "1044                        ,   \n",
              "1045                        ,   \n",
              "1046                        ?   \n",
              "1047                        ?   \n",
              "1048                        ,   \n",
              "1049                        ,   \n",
              "1050                            \n",
              "1051                        ,   \n",
              "1052                        ,   \n",
              "1053                        ,   \n",
              "1054                            \n",
              "1055                        ,   \n",
              "1056                        ,   \n",
              "1057                        ,   \n",
              "1058                        ,   \n",
              "1059                        ,   \n",
              "1060                            \n",
              "1061                        ,   \n",
              "1062                            \n",
              "1063                        ,   \n",
              "1064                        ,   \n",
              "1065                            \n",
              "1066                        ,   \n",
              "1067                        ,   \n",
              "1068                        ?   \n",
              "1069                        ,   \n",
              "1070                            \n",
              "1071                        ,   \n",
              "1072                        ,   \n",
              "1073                            \n",
              "1074                        ?   \n",
              "1075                        ,   \n",
              "1076                        ?   \n",
              "1077                        ?   \n",
              "1083                  because   \n",
              "1096                , because   \n",
              "1097                        ,   \n",
              "1100                  gradual   \n",
              "1101                      , I   \n",
              "1107               э=-@ their   \n",
              "1119                     . He   \n",
              "1481            countries, it   \n",
              "1483            countries, it   \n",
              "1725                 essay, I   \n",
              "1811                 , to get   \n",
              "2262  New Zealand and the UK,   \n",
              "2761                study. It   \n",
              "2916                   thinks   \n",
              "2917                   thinks   \n",
              "3010                   UK, it   \n",
              "\n",
              "                                              full_text  \\\n",
              "16    As Africa Today had recenly reported 78% of st...   \n",
              "127   Only germany saw a slight increase from 1990 t...   \n",
              "140   As we can see Japans dinamics on the graph, sh...   \n",
              "275   In UK difference decreased from around 36 by 1...   \n",
              "921   Moreover, school eduction or at least elementa...   \n",
              "1043  For instance, a lot of famous people as Lomono...   \n",
              "1044  In Germany the difference plummed by more than...   \n",
              "1045  According to the graph. in 1980 USA rate was a...   \n",
              "1046  Thus, how can perspective programmer get into ...   \n",
              "1047  Of course, there were, are and will be people ...   \n",
              "1048  Overall. while statistics for all countries ex...   \n",
              "1049  That is to say that education ultimately thriv...   \n",
              "1050  The percentage difference in income between ge...   \n",
              "1051  People from Japan have maximum earnings, and t...   \n",
              "1052  Secondly. every year the world shows new thing...   \n",
              "1053  In fact. the njtjrious soft skill essential fo...   \n",
              "1054  It. can really can be helpful in developing ou...   \n",
              "1055  As regards the USA. it also came down consider...   \n",
              "1056  In conclusion. authorities should pay an atten...   \n",
              "1057  In conclusion, most of us thinks that every hu...   \n",
              "1058  the number are devastating. considering this g...   \n",
              "1059  However, while in 4 out of 5 countries, shown ...   \n",
              "1060  That is why education can not be considered as...   \n",
              "1061  For example, let's take a look into history. w...   \n",
              "1062  However, I still hold the opinion. that educat...   \n",
              "1063  Overall. the graph shows that in all countries...   \n",
              "1064  and he/ she can do some mistakes. and the resu...   \n",
              "1065  What is more, the differences in income in Ger...   \n",
              "1066  One other moment, if you a very clever child i...   \n",
              "1067  So to manage such task school should't be comp...   \n",
              "1068  The idea that education, including schools and...   \n",
              "1069  Firstly. equality which is the core of getting...   \n",
              "1070  Why did authors selected Germany instead of,. ...   \n",
              "1071  Each of us have a birth in a different familie...   \n",
              "1072  276 To sum up, I still consider that education...   \n",
              "1073  The line graph depicts the differnce of income...   \n",
              "1074  And if one person has right to study, why the ...   \n",
              "1075  All in all, education the most part of educati...   \n",
              "1076  What for to take in university student who do ...   \n",
              "1077  But you will say: how about people that want t...   \n",
              "1083  37 On the one hand, there will be difficult fo...   \n",
              "1096  First, a free education have to be given those...   \n",
              "1097  Although there are some people who justify edu...   \n",
              "1100  Japan is also a bit different from the other c...   \n",
              "1101  In conclusion. i want to say that in today's w...   \n",
              "1107  One the other hand, there are some basic thing...   \n",
              "1119  For instance, there was one of the most famous...   \n",
              "1481  Looking at other countries can be seen that th...   \n",
              "1483  In the other countries was the opposite situat...   \n",
              "1725  So in this essay will be demonstrates both poi...   \n",
              "1811  Firstly, it is impossible to exept liers and k...   \n",
              "2262  the impressive results among others show count...   \n",
              "2761  From my humble opinion, every person should ha...   \n",
              "2916  One part think, that education should be provi...   \n",
              "2917  One part think, that education should be provi...   \n",
              "3010          In Japan, USA and UK did not exceeded 8%.   \n",
              "\n",
              "                                             tokens_ids  \\\n",
              "16    {122: 'As', 123: 'Africa', 124: 'Today', 125: ...   \n",
              "127   {119: 'Only', 120: 'germany', 121: 'saw', 122:...   \n",
              "140   {59: 'As', 60: 'we', 61: 'can', 62: 'see', 63:...   \n",
              "275   {177: 'In', 178: 'UK', 179: 'difference', 180:...   \n",
              "921   {98: 'Moreover', 99: ',', 100: 'school', 101: ...   \n",
              "1043  {175: 'For', 176: 'instance', 177: ',', 178: '...   \n",
              "1044  {118: 'In', 119: 'Germany', 120: 'the', 121: '...   \n",
              "1045  {54: 'According', 55: 'to', 56: 'the', 57: 'gr...   \n",
              "1046  {100: 'Thus', 101: ',', 102: 'how', 103: 'can'...   \n",
              "1047  {169: 'Of', 170: 'course', 171: ',', 172: 'the...   \n",
              "1048  {38: 'Overall', 39: '.', 40: 'while', 41: 'sta...   \n",
              "1049  {89: 'That', 90: 'is', 91: 'to', 92: 'say', 93...   \n",
              "1050  {214: 'The', 215: 'percentage', 216: 'differen...   \n",
              "1051  {49: 'People', 50: 'from', 51: 'Japan', 52: 'h...   \n",
              "1052  {199: 'Secondly', 200: '.', 201: 'every', 202:...   \n",
              "1053  {186: 'In', 187: 'fact', 188: '.', 189: 'the',...   \n",
              "1054  {216: 'It', 217: '.', 218: 'can', 219: 'really...   \n",
              "1055  {112: 'As', 113: 'regards', 114: 'the', 115: '...   \n",
              "1056  {242: 'In', 243: 'conclusion', 244: '.', 245: ...   \n",
              "1057  {293: 'In', 294: 'conclusion', 295: ',', 296: ...   \n",
              "1058  {83: 'the', 84: 'number', 85: 'are', 86: 'deva...   \n",
              "1059  {44: 'However', 45: ',', 46: 'while', 47: 'in'...   \n",
              "1060  {209: 'That', 210: 'is', 211: 'why', 212: 'edu...   \n",
              "1061  {300: 'For', 301: 'example', 302: ',', 303: 'l...   \n",
              "1062  {234: 'However', 235: ',', 236: 'I', 237: 'sti...   \n",
              "1063  {35: 'Overall', 36: '.', 37: 'the', 38: 'graph...   \n",
              "1064  {99: 'and', 100: 'he', 101: '/', 102: 'she', 1...   \n",
              "1065  {107: 'What', 108: 'is', 109: 'more', 110: ','...   \n",
              "1066  {248: 'One', 249: 'other', 250: 'moment', 251:...   \n",
              "1067  {65: 'So', 66: 'to', 67: 'manage', 68: 'such',...   \n",
              "1068  {1: 'The', 2: 'idea', 3: 'that', 4: 'education...   \n",
              "1069  {66: 'Firstly', 67: '.', 68: 'equality', 69: '...   \n",
              "1070  {391: 'Why', 392: 'did', 393: 'authors', 394: ...   \n",
              "1071  {92: 'Each', 93: 'of', 94: 'us', 95: 'have', 9...   \n",
              "1072  {298: '276', 299: 'To', 300: 'sum', 301: 'up',...   \n",
              "1073  {1: 'The', 2: 'line', 3: 'graph', 4: 'depicts'...   \n",
              "1074  {58: 'And', 59: 'if', 60: 'one', 61: 'person',...   \n",
              "1075  {244: 'All', 245: 'in', 246: 'all', 247: ',', ...   \n",
              "1076  {192: 'What', 193: 'for', 194: 'to', 195: 'tak...   \n",
              "1077  {206: 'But', 207: 'you', 208: 'will', 209: 'sa...   \n",
              "1083  {43: '37', 44: 'On', 45: 'the', 46: 'one', 47:...   \n",
              "1096  {230: 'First', 231: ',', 232: 'a', 233: 'free'...   \n",
              "1097  {246: 'Although', 247: 'there', 248: 'are', 24...   \n",
              "1100  {121: 'Japan', 122: 'is', 123: 'also', 124: 'a...   \n",
              "1101  {273: 'In', 274: 'conclusion', 275: '.', 276: ...   \n",
              "1107  {203: 'One', 204: 'the', 205: 'other', 206: 'h...   \n",
              "1119  {218: 'For', 219: 'instance', 220: ',', 221: '...   \n",
              "1481  {123: 'Looking', 124: 'at', 125: 'other', 126:...   \n",
              "1483  {87: 'In', 88: 'the', 89: 'other', 90: 'countr...   \n",
              "1725  {60: 'So', 61: 'in', 62: 'this', 63: 'essay', ...   \n",
              "1811  {134: 'Firstly', 135: ',', 136: 'it', 137: 'is...   \n",
              "2262  {179: 'the', 180: 'impressive', 181: 'results'...   \n",
              "2761  {288: 'From', 289: 'my', 290: 'humble', 291: '...   \n",
              "2916  {36: 'One', 37: 'part', 38: 'think', 39: ',', ...   \n",
              "2917  {36: 'One', 37: 'part', 38: 'think', 39: ',', ...   \n",
              "3010  {181: 'In', 182: 'Japan', 183: ',', 184: 'USA'...   \n",
              "\n",
              "                                           dependencies  \\\n",
              "16    {122: ['As', 'SCONJ', 'IN', 'mark'], 123: ['Af...   \n",
              "127   {119: ['Only', 'ADV', 'RB', 'advmod'], 120: ['...   \n",
              "140   {59: ['As', 'SCONJ', 'IN', 'mark'], 60: ['we',...   \n",
              "275   {177: ['In', 'ADP', 'IN', 'prep'], 178: ['UK',...   \n",
              "921   {98: ['Moreover', 'ADV', 'RB', 'advmod'], 99: ...   \n",
              "1043  {175: ['For', 'ADP', 'IN', 'prep'], 176: ['ins...   \n",
              "1044  {118: ['In', 'ADP', 'IN', 'prep'], 119: ['Germ...   \n",
              "1045  {54: ['According', 'VERB', 'VBG', 'ROOT'], 55:...   \n",
              "1046  {100: ['Thus', 'ADV', 'RB', 'advmod'], 101: ['...   \n",
              "1047  {169: ['Of', 'ADV', 'RB', 'advmod'], 170: ['co...   \n",
              "1048  {38: ['Overall', 'ADV', 'RB', 'ROOT'], 39: ['....   \n",
              "1049  {89: ['That', 'DET', 'DT', 'nsubj'], 90: ['is'...   \n",
              "1050  {214: ['The', 'DET', 'DT', 'det'], 215: ['perc...   \n",
              "1051  {49: ['People', 'NOUN', 'NNS', 'nsubj'], 50: [...   \n",
              "1052  {199: ['Secondly', 'ADV', 'RB', 'ROOT'], 200: ...   \n",
              "1053  {186: ['In', 'ADP', 'IN', 'ROOT'], 187: ['fact...   \n",
              "1054  {216: ['It', 'PRON', 'PRP', 'ROOT'], 217: ['.'...   \n",
              "1055  {112: ['As', 'SCONJ', 'IN', 'mark'], 113: ['re...   \n",
              "1056  {242: ['In', 'ADP', 'IN', 'ROOT'], 243: ['conc...   \n",
              "1057  {293: ['In', 'ADP', 'IN', 'prep'], 294: ['conc...   \n",
              "1058  {83: ['the', 'DET', 'DT', 'det'], 84: ['number...   \n",
              "1059  {44: ['However', 'ADV', 'RB', 'advmod'], 45: [...   \n",
              "1060  {209: ['That', 'DET', 'DT', 'nsubj'], 210: ['i...   \n",
              "1061  {300: ['For', 'ADP', 'IN', 'prep'], 301: ['exa...   \n",
              "1062  {234: ['However', 'ADV', 'RB', 'advmod'], 235:...   \n",
              "1063  {35: ['Overall', 'ADV', 'RB', 'ROOT'], 36: ['....   \n",
              "1064  {99: ['and', 'CCONJ', 'CC', 'cc'], 100: ['he',...   \n",
              "1065  {107: ['What', 'PRON', 'WP', 'nsubj'], 108: ['...   \n",
              "1066  {248: ['One', 'NUM', 'CD', 'nummod'], 249: ['o...   \n",
              "1067  {65: ['So', 'ADV', 'RB', 'advmod'], 66: ['to',...   \n",
              "1068  {1: ['The', 'DET', 'DT', 'det'], 2: ['idea', '...   \n",
              "1069  {66: ['Firstly', 'ADV', 'RB', 'ROOT'], 67: ['....   \n",
              "1070  {391: ['Why', 'ADV', 'WRB', 'advmod'], 392: ['...   \n",
              "1071  {92: ['Each', 'DET', 'DT', 'nsubj'], 93: ['of'...   \n",
              "1072  {298: ['276', 'NUM', 'CD', 'ROOT'], 299: ['To'...   \n",
              "1073  {1: ['The', 'DET', 'DT', 'det'], 2: ['line', '...   \n",
              "1074  {58: ['And', 'CCONJ', 'CC', 'cc'], 59: ['if', ...   \n",
              "1075  {244: ['All', 'DET', 'DT', 'advmod'], 245: ['i...   \n",
              "1076  {192: ['What', 'PRON', 'WP', 'dobj'], 193: ['f...   \n",
              "1077  {206: ['But', 'CCONJ', 'CC', 'cc'], 207: ['you...   \n",
              "1083  {43: ['37', 'NUM', 'CD', 'ROOT'], 44: ['On', '...   \n",
              "1096  {230: ['First', 'ADV', 'RB', 'advmod'], 231: [...   \n",
              "1097  {246: ['Although', 'SCONJ', 'IN', 'mark'], 247...   \n",
              "1100  {121: ['Japan', 'PROPN', 'NNP', 'nsubj'], 122:...   \n",
              "1101  {273: ['In', 'ADP', 'IN', 'ROOT'], 274: ['conc...   \n",
              "1107  {203: ['One', 'NUM', 'CD', 'nummod'], 204: ['t...   \n",
              "1119  {218: ['For', 'ADP', 'IN', 'prep'], 219: ['ins...   \n",
              "1481  {123: ['Looking', 'VERB', 'VBG', 'csubjpass'],...   \n",
              "1483  {87: ['In', 'ADP', 'IN', 'prep'], 88: ['the', ...   \n",
              "1725  {60: ['So', 'ADV', 'RB', 'cc'], 61: ['in', 'AD...   \n",
              "1811  {134: ['Firstly', 'ADV', 'RB', 'advmod'], 135:...   \n",
              "2262  {179: ['the', 'DET', 'DT', 'det'], 180: ['impr...   \n",
              "2761  {288: ['From', 'ADP', 'IN', 'prep'], 289: ['my...   \n",
              "2916  {36: ['One', 'NUM', 'CD', 'nummod'], 37: ['par...   \n",
              "2917  {36: ['One', 'NUM', 'CD', 'nummod'], 37: ['par...   \n",
              "3010  {181: ['In', 'ADP', 'IN', 'prep'], 182: ['Japa...   \n",
              "\n",
              "                                         corrected_text  \\\n",
              "16    As Africa Today had recenly reported, 78% of s...   \n",
              "127   Only germany saw a slight increase from 1990 t...   \n",
              "140   As we can see Japans dinamics on the graph, sh...   \n",
              "275   In UK difference decreased from around 36 by 1...   \n",
              "921   Moreover, school eduction or at least elementa...   \n",
              "1043  For instance, a lot of famous people as Lomono...   \n",
              "1044  In Germany the difference plummed by more than...   \n",
              "1045  According to the graph, in 1980 USA rate was a...   \n",
              "1046  Thus, how can perspective programmer get into ...   \n",
              "1047  Of course, there were, are and will be people ...   \n",
              "1048  Overall, while statistics for all countries ex...   \n",
              "1049  That is to say that education ultimately thriv...   \n",
              "1050  The percentage difference in income between ge...   \n",
              "1051  People from Japan have maximum earnings, and t...   \n",
              "1052  Secondly, every year the world shows new thing...   \n",
              "1053  In fact, the njtjrious soft skill essential fo...   \n",
              "1054  It  can really can be helpful in developing ou...   \n",
              "1055  As regards the USA, it also came down consider...   \n",
              "1056  In conclusion, authorities should pay an atten...   \n",
              "1057  In conclusion, most of us thinks that every hu...   \n",
              "1058  the number are devastating, considering this g...   \n",
              "1059  However, while in 4 out of 5 countries, shown ...   \n",
              "1060  That is why education can not be considered as...   \n",
              "1061  For example, let's take a look into history, w...   \n",
              "1062  However, I still hold the opinion  that educat...   \n",
              "1063  Overall, the graph shows that in all countries...   \n",
              "1064  and he/ she can do some mistakes, and the resu...   \n",
              "1065  What is more, the differences in income in Ger...   \n",
              "1066  One other moment, if you a very clever child i...   \n",
              "1067  So to manage such task school should't be comp...   \n",
              "1068  The idea that education, including schools and...   \n",
              "1069  Firstly, equality which is the core of getting...   \n",
              "1070  Why did authors selected Germany instead of,  ...   \n",
              "1071  Each of us have a birth in a different familie...   \n",
              "1072  276 To sum up, I still consider that education...   \n",
              "1073  The line graph depicts the differnce of income...   \n",
              "1074  And if one person has right to study, why the ...   \n",
              "1075  All in all, education the most part of educati...   \n",
              "1076  What for to take in university student who do ...   \n",
              "1077  But you will say: how about people that want t...   \n",
              "1083  37 On the one hand, there will be difficult fo...   \n",
              "1096  First, a free education have to be given those...   \n",
              "1097  Although there are some people who justify edu...   \n",
              "1100  Japan is also a bit different from the other c...   \n",
              "1101  In conclusion, I want to say that in today's w...   \n",
              "1107  One the other hand, there are some basic thing...   \n",
              "1119  For instance, there was one of the most famous...   \n",
              "1481  Looking at other countries, it can be seen tha...   \n",
              "1483  In the other countries, it was the opposite si...   \n",
              "1725  So in this essay, I will be demonstrates both ...   \n",
              "1811  Firstly, it is impossible to exept liers and k...   \n",
              "2262  the impressive results among others show count...   \n",
              "2761  From my humble opinion, every person should ha...   \n",
              "2916  One part thinks that education should be provi...   \n",
              "2917  One part think, that education should be provi...   \n",
              "3010      In Japan, USA and UK, it did not exceeded 8%.   \n",
              "\n",
              "                                     corrected_with_ids  \\\n",
              "16    {122: 'As', 123: 'Africa', 124: 'Today', 125: ...   \n",
              "127   {119: 'Only', 120: 'germany', 121: 'saw', 122:...   \n",
              "140   {59: 'As', 60: 'we', 61: 'can', 62: 'see', 63:...   \n",
              "275   {177: 'In', 178: 'UK', 179: 'difference', 180:...   \n",
              "921   {98: 'Moreover', 99: ',', 100: 'school', 101: ...   \n",
              "1043  {175: 'For', 176: 'instance', 177: ',', 178: '...   \n",
              "1044  {118: 'In', 119: 'Germany', 120: 'the', 121: '...   \n",
              "1045  {54: 'According', 55: 'to', 56: 'the', 57: 'gr...   \n",
              "1046  {100: 'Thus', 101: ',', 102: 'how', 103: 'can'...   \n",
              "1047  {169: 'Of', 170: 'course', 171: ',', 172: 'the...   \n",
              "1048  {38: 'Overall', 39: ',', 40: 'while', 41: 'sta...   \n",
              "1049  {89: 'That', 90: 'is', 91: 'to', 92: 'say', 93...   \n",
              "1050  {214: 'The', 215: 'percentage', 216: 'differen...   \n",
              "1051  {49: 'People', 50: 'from', 51: 'Japan', 52: 'h...   \n",
              "1052  {199: 'Secondly', 200: ',', 201: 'every', 202:...   \n",
              "1053  {186: 'In', 187: 'fact', 188: ',', 189: 'the',...   \n",
              "1054  {216: 'It', 217: 'can', 218: 'really', 219: 'c...   \n",
              "1055  {112: 'As', 113: 'regards', 114: 'the', 115: '...   \n",
              "1056  {242: 'In', 243: 'conclusion', 244: ',', 245: ...   \n",
              "1057  {293: 'In', 294: 'conclusion', 295: ',', 296: ...   \n",
              "1058  {83: 'the', 84: 'number', 85: 'are', 86: 'deva...   \n",
              "1059  {44: 'However', 45: ',', 46: 'while', 47: 'in'...   \n",
              "1060  {209: 'That', 210: 'is', 211: 'why', 212: 'edu...   \n",
              "1061  {300: 'For', 301: 'example', 302: ',', 303: 'l...   \n",
              "1062  {234: 'However', 235: ',', 236: 'I', 237: 'sti...   \n",
              "1063  {35: 'Overall', 36: ',', 37: 'the', 38: 'graph...   \n",
              "1064  {99: 'and', 100: 'he', 101: 'she', 102: 'can',...   \n",
              "1065  {107: 'What', 108: 'is', 109: 'more', 110: ','...   \n",
              "1066  {248: 'One', 249: 'other', 250: 'moment', 251:...   \n",
              "1067  {65: 'So', 66: 'to', 67: 'manage', 68: 'such',...   \n",
              "1068  {1: 'The', 2: 'idea', 3: 'that', 4: 'education...   \n",
              "1069  {66: 'Firstly', 67: ',', 68: 'equality', 69: '...   \n",
              "1070  {391: 'Why', 392: 'did', 393: 'authors', 394: ...   \n",
              "1071  {92: 'Each', 93: 'of', 94: 'us', 95: 'have', 9...   \n",
              "1072  {298: '276', 299: 'To', 300: 'sum', 301: 'up',...   \n",
              "1073  {1: 'The', 2: 'line', 3: 'graph', 4: 'depicts'...   \n",
              "1074  {58: 'And', 59: 'if', 60: 'one', 61: 'person',...   \n",
              "1075  {244: 'All', 245: 'in', 246: 'all', 247: ',', ...   \n",
              "1076  {192: 'What', 193: 'for', 194: 'to', 195: 'tak...   \n",
              "1077  {206: 'But', 207: 'you', 208: 'will', 209: 'sa...   \n",
              "1083  {43: '37', 44: 'On', 45: 'the', 46: 'one', 47:...   \n",
              "1096  {230: 'First', 231: ',', 232: 'a', 233: 'free'...   \n",
              "1097  {246: 'Although', 247: 'there', 248: 'are', 24...   \n",
              "1100  {121: 'Japan', 122: 'is', 123: 'also', 124: 'a...   \n",
              "1101  {273: 'In', 274: 'conclusion', 275: ',', 276: ...   \n",
              "1107  {203: 'One', 204: 'the', 205: 'other', 206: 'h...   \n",
              "1119  {218: 'For', 219: 'instance', 220: ',', 221: '...   \n",
              "1481  {123: 'Looking', 124: 'at', 125: 'other', 126:...   \n",
              "1483  {87: 'In', 88: 'the', 89: 'other', 90: 'countr...   \n",
              "1725  {60: 'So', 61: 'in', 62: 'this', 63: 'essay', ...   \n",
              "1811  {134: 'Firstly', 135: ',', 136: 'it', 137: 'is...   \n",
              "2262  {179: 'the', 180: 'impressive', 181: 'results'...   \n",
              "2761  {288: 'From', 289: 'my', 290: 'humble', 291: '...   \n",
              "2916  {36: 'One', 37: 'part', 38: 'thinks', 39: 'tha...   \n",
              "2917  {36: 'One', 37: 'part', 38: 'think', 39: ',', ...   \n",
              "3010  {181: 'In', 182: 'Japan', 183: ',', 184: 'USA'...   \n",
              "\n",
              "                                  dependencies_heptabot  \n",
              "16    {122: ['As', 'SCONJ', 'IN', 'mark'], 123: ['Af...  \n",
              "127   {119: ['Only', 'ADV', 'RB', 'advmod'], 120: ['...  \n",
              "140   {59: ['As', 'SCONJ', 'IN', 'mark'], 60: ['we',...  \n",
              "275   {177: ['In', 'ADP', 'IN', 'prep'], 178: ['UK',...  \n",
              "921   {98: ['Moreover', 'ADV', 'RB', 'advmod'], 99: ...  \n",
              "1043  {175: ['For', 'ADP', 'IN', 'prep'], 176: ['ins...  \n",
              "1044  {118: ['In', 'ADP', 'IN', 'prep'], 119: ['Germ...  \n",
              "1045  {54: ['According', 'VERB', 'VBG', 'prep'], 55:...  \n",
              "1046  {100: ['Thus', 'ADV', 'RB', 'advmod'], 101: ['...  \n",
              "1047  {169: ['Of', 'ADV', 'RB', 'advmod'], 170: ['co...  \n",
              "1048  {38: ['Overall', 'ADV', 'RB', 'advmod'], 39: [...  \n",
              "1049  {89: ['That', 'DET', 'DT', 'nsubj'], 90: ['is'...  \n",
              "1050  {214: ['The', 'DET', 'DT', 'det'], 215: ['perc...  \n",
              "1051  {49: ['People', 'NOUN', 'NNS', 'nsubj'], 50: [...  \n",
              "1052  {199: ['Secondly', 'ADV', 'RB', 'advmod'], 200...  \n",
              "1053  {186: ['In', 'ADP', 'IN', 'prep'], 187: ['fact...  \n",
              "1054  {216: ['It', 'PRON', 'PRP', 'nsubj'], 217: ['c...  \n",
              "1055  {112: ['As', 'SCONJ', 'IN', 'mark'], 113: ['re...  \n",
              "1056  {242: ['In', 'ADP', 'IN', 'prep'], 243: ['conc...  \n",
              "1057  {293: ['In', 'ADP', 'IN', 'prep'], 294: ['conc...  \n",
              "1058  {83: ['the', 'DET', 'DT', 'det'], 84: ['number...  \n",
              "1059  {44: ['However', 'ADV', 'RB', 'advmod'], 45: [...  \n",
              "1060  {209: ['That', 'DET', 'DT', 'nsubj'], 210: ['i...  \n",
              "1061  {300: ['For', 'ADP', 'IN', 'prep'], 301: ['exa...  \n",
              "1062  {234: ['However', 'ADV', 'RB', 'advmod'], 235:...  \n",
              "1063  {35: ['Overall', 'ADV', 'RB', 'advmod'], 36: [...  \n",
              "1064  {99: ['and', 'CCONJ', 'CC', 'cc'], 100: ['he',...  \n",
              "1065  {107: ['What', 'PRON', 'WP', 'nsubj'], 108: ['...  \n",
              "1066  {248: ['One', 'NUM', 'CD', 'nummod'], 249: ['o...  \n",
              "1067  {65: ['So', 'ADV', 'RB', 'advmod'], 66: ['to',...  \n",
              "1068  {1: ['The', 'DET', 'DT', 'det'], 2: ['idea', '...  \n",
              "1069  {66: ['Firstly', 'ADV', 'RB', 'advmod'], 67: [...  \n",
              "1070  {391: ['Why', 'ADV', 'WRB', 'advmod'], 392: ['...  \n",
              "1071  {92: ['Each', 'DET', 'DT', 'nsubj'], 93: ['of'...  \n",
              "1072  {298: ['276', 'NUM', 'CD', 'ROOT'], 299: ['To'...  \n",
              "1073  {1: ['The', 'DET', 'DT', 'det'], 2: ['line', '...  \n",
              "1074  {58: ['And', 'CCONJ', 'CC', 'cc'], 59: ['if', ...  \n",
              "1075  {244: ['All', 'DET', 'DT', 'advmod'], 245: ['i...  \n",
              "1076  {192: ['What', 'PRON', 'WP', 'ROOT'], 193: ['f...  \n",
              "1077  {206: ['But', 'CCONJ', 'CC', 'cc'], 207: ['you...  \n",
              "1083  {43: ['37', 'NUM', 'CD', 'ROOT'], 44: ['On', '...  \n",
              "1096  {230: ['First', 'ADV', 'RB', 'advmod'], 231: [...  \n",
              "1097  {246: ['Although', 'SCONJ', 'IN', 'mark'], 247...  \n",
              "1100  {121: ['Japan', 'PROPN', 'NNP', 'nsubj'], 122:...  \n",
              "1101  {273: ['In', 'ADP', 'IN', 'prep'], 274: ['conc...  \n",
              "1107  {203: ['One', 'NUM', 'CD', 'nummod'], 204: ['t...  \n",
              "1119  {218: ['For', 'ADP', 'IN', 'prep'], 219: ['ins...  \n",
              "1481  {123: ['Looking', 'VERB', 'VBG', 'advcl'], 124...  \n",
              "1483  {87: ['In', 'ADP', 'IN', 'prep'], 88: ['the', ...  \n",
              "1725  {60: ['So', 'ADV', 'RB', 'advmod'], 61: ['in',...  \n",
              "1811  {134: ['Firstly', 'ADV', 'RB', 'advmod'], 135:...  \n",
              "2262  {179: ['the', 'DET', 'DT', 'det'], 180: ['impr...  \n",
              "2761  {288: ['From', 'ADP', 'IN', 'prep'], 289: ['my...  \n",
              "2916  {36: ['One', 'NUM', 'CD', 'nummod'], 37: ['par...  \n",
              "2917  {36: ['One', 'NUM', 'CD', 'nummod'], 37: ['par...  \n",
              "3010  {181: ['In', 'ADP', 'IN', 'prep'], 182: ['Japa...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-67481e55-06ec-44d9-89c3-9f8cb5ce4ba1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>sentence_id</th>\n",
              "      <th>token_id</th>\n",
              "      <th>mistake_type</th>\n",
              "      <th>error_span</th>\n",
              "      <th>correction</th>\n",
              "      <th>full_text</th>\n",
              "      <th>tokens_ids</th>\n",
              "      <th>dependencies</th>\n",
              "      <th>corrected_text</th>\n",
              "      <th>corrected_with_ids</th>\n",
              "      <th>dependencies_heptabot</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>731.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>78%</td>\n",
              "      <td>, 78%</td>\n",
              "      <td>As Africa Today had recenly reported 78% of st...</td>\n",
              "      <td>{122: 'As', 123: 'Africa', 124: 'Today', 125: ...</td>\n",
              "      <td>{122: ['As', 'SCONJ', 'IN', 'mark'], 123: ['Af...</td>\n",
              "      <td>As Africa Today had recenly reported, 78% of s...</td>\n",
              "      <td>{122: 'As', 123: 'Africa', 124: 'Today', 125: ...</td>\n",
              "      <td>{122: ['As', 'SCONJ', 'IN', 'mark'], 123: ['Af...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>592.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>1995.0</td>\n",
              "      <td>1995.0</td>\n",
              "      <td>Only germany saw a slight increase from 1990 t...</td>\n",
              "      <td>{119: 'Only', 120: 'germany', 121: 'saw', 122:...</td>\n",
              "      <td>{119: ['Only', 'ADV', 'RB', 'advmod'], 120: ['...</td>\n",
              "      <td>Only germany saw a slight increase from 1990 t...</td>\n",
              "      <td>{119: 'Only', 120: 'germany', 121: 'saw', 122:...</td>\n",
              "      <td>{119: ['Only', 'ADV', 'RB', 'advmod'], 120: ['...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>1041.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>82.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>1995.0</td>\n",
              "      <td>1995.0</td>\n",
              "      <td>As we can see Japans dinamics on the graph, sh...</td>\n",
              "      <td>{59: 'As', 60: 'we', 61: 'can', 62: 'see', 63:...</td>\n",
              "      <td>{59: ['As', 'SCONJ', 'IN', 'mark'], 60: ['we',...</td>\n",
              "      <td>As we can see Japans dinamics on the graph, sh...</td>\n",
              "      <td>{59: 'As', 60: 'we', 61: 'can', 62: 'see', 63:...</td>\n",
              "      <td>{59: ['As', 'SCONJ', 'IN', 'mark'], 60: ['we',...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>275</th>\n",
              "      <td>344.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>187.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>,</td>\n",
              "      <td>. A</td>\n",
              "      <td>In UK difference decreased from around 36 by 1...</td>\n",
              "      <td>{177: 'In', 178: 'UK', 179: 'difference', 180:...</td>\n",
              "      <td>{177: ['In', 'ADP', 'IN', 'prep'], 178: ['UK',...</td>\n",
              "      <td>In UK difference decreased from around 36 by 1...</td>\n",
              "      <td>{177: 'In', 178: 'UK', 179: 'difference', 180:...</td>\n",
              "      <td>{177: ['In', 'ADP', 'IN', 'prep'], 178: ['UK',...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>921</th>\n",
              "      <td>803.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>, it</td>\n",
              "      <td>. That</td>\n",
              "      <td>Moreover, school eduction or at least elementa...</td>\n",
              "      <td>{98: 'Moreover', 99: ',', 100: 'school', 101: ...</td>\n",
              "      <td>{98: ['Moreover', 'ADV', 'RB', 'advmod'], 99: ...</td>\n",
              "      <td>Moreover, school eduction or at least elementa...</td>\n",
              "      <td>{98: 'Moreover', 99: ',', 100: 'school', 101: ...</td>\n",
              "      <td>{98: ['Moreover', 'ADV', 'RB', 'advmod'], 99: ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1043</th>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>221.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td>?</td>\n",
              "      <td>For instance, a lot of famous people as Lomono...</td>\n",
              "      <td>{175: 'For', 176: 'instance', 177: ',', 178: '...</td>\n",
              "      <td>{175: ['For', 'ADP', 'IN', 'prep'], 176: ['ins...</td>\n",
              "      <td>For instance, a lot of famous people as Lomono...</td>\n",
              "      <td>{175: 'For', 176: 'instance', 177: ',', 178: '...</td>\n",
              "      <td>{175: ['For', 'ADP', 'IN', 'prep'], 176: ['ins...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1044</th>\n",
              "      <td>15.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td>,</td>\n",
              "      <td>In Germany the difference plummed by more than...</td>\n",
              "      <td>{118: 'In', 119: 'Germany', 120: 'the', 121: '...</td>\n",
              "      <td>{118: ['In', 'ADP', 'IN', 'prep'], 119: ['Germ...</td>\n",
              "      <td>In Germany the difference plummed by more than...</td>\n",
              "      <td>{118: 'In', 119: 'Germany', 120: 'the', 121: '...</td>\n",
              "      <td>{118: ['In', 'ADP', 'IN', 'prep'], 119: ['Germ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1045</th>\n",
              "      <td>25.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td>,</td>\n",
              "      <td>According to the graph. in 1980 USA rate was a...</td>\n",
              "      <td>{54: 'According', 55: 'to', 56: 'the', 57: 'gr...</td>\n",
              "      <td>{54: ['According', 'VERB', 'VBG', 'ROOT'], 55:...</td>\n",
              "      <td>According to the graph, in 1980 USA rate was a...</td>\n",
              "      <td>{54: 'According', 55: 'to', 56: 'the', 57: 'gr...</td>\n",
              "      <td>{54: ['According', 'VERB', 'VBG', 'prep'], 55:...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1046</th>\n",
              "      <td>54.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td>?</td>\n",
              "      <td>Thus, how can perspective programmer get into ...</td>\n",
              "      <td>{100: 'Thus', 101: ',', 102: 'how', 103: 'can'...</td>\n",
              "      <td>{100: ['Thus', 'ADV', 'RB', 'advmod'], 101: ['...</td>\n",
              "      <td>Thus, how can perspective programmer get into ...</td>\n",
              "      <td>{100: 'Thus', 101: ',', 102: 'how', 103: 'can'...</td>\n",
              "      <td>{100: ['Thus', 'ADV', 'RB', 'advmod'], 101: ['...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1047</th>\n",
              "      <td>58.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>199.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td>?</td>\n",
              "      <td>Of course, there were, are and will be people ...</td>\n",
              "      <td>{169: 'Of', 170: 'course', 171: ',', 172: 'the...</td>\n",
              "      <td>{169: ['Of', 'ADV', 'RB', 'advmod'], 170: ['co...</td>\n",
              "      <td>Of course, there were, are and will be people ...</td>\n",
              "      <td>{169: 'Of', 170: 'course', 171: ',', 172: 'the...</td>\n",
              "      <td>{169: ['Of', 'ADV', 'RB', 'advmod'], 170: ['co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048</th>\n",
              "      <td>111.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td>,</td>\n",
              "      <td>Overall. while statistics for all countries ex...</td>\n",
              "      <td>{38: 'Overall', 39: '.', 40: 'while', 41: 'sta...</td>\n",
              "      <td>{38: ['Overall', 'ADV', 'RB', 'ROOT'], 39: ['....</td>\n",
              "      <td>Overall, while statistics for all countries ex...</td>\n",
              "      <td>{38: 'Overall', 39: ',', 40: 'while', 41: 'sta...</td>\n",
              "      <td>{38: ['Overall', 'ADV', 'RB', 'advmod'], 39: [...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1049</th>\n",
              "      <td>130.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td>,</td>\n",
              "      <td>That is to say that education ultimately thriv...</td>\n",
              "      <td>{89: 'That', 90: 'is', 91: 'to', 92: 'say', 93...</td>\n",
              "      <td>{89: ['That', 'DET', 'DT', 'nsubj'], 90: ['is'...</td>\n",
              "      <td>That is to say that education ultimately thriv...</td>\n",
              "      <td>{89: 'That', 90: 'is', 91: 'to', 92: 'say', 93...</td>\n",
              "      <td>{89: ['That', 'DET', 'DT', 'nsubj'], 90: ['is'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1050</th>\n",
              "      <td>246.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>245.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td></td>\n",
              "      <td>The percentage difference in income between ge...</td>\n",
              "      <td>{214: 'The', 215: 'percentage', 216: 'differen...</td>\n",
              "      <td>{214: ['The', 'DET', 'DT', 'det'], 215: ['perc...</td>\n",
              "      <td>The percentage difference in income between ge...</td>\n",
              "      <td>{214: 'The', 215: 'percentage', 216: 'differen...</td>\n",
              "      <td>{214: ['The', 'DET', 'DT', 'det'], 215: ['perc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1051</th>\n",
              "      <td>260.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>61.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td>,</td>\n",
              "      <td>People from Japan have maximum earnings, and t...</td>\n",
              "      <td>{49: 'People', 50: 'from', 51: 'Japan', 52: 'h...</td>\n",
              "      <td>{49: ['People', 'NOUN', 'NNS', 'nsubj'], 50: [...</td>\n",
              "      <td>People from Japan have maximum earnings, and t...</td>\n",
              "      <td>{49: 'People', 50: 'from', 51: 'Japan', 52: 'h...</td>\n",
              "      <td>{49: ['People', 'NOUN', 'NNS', 'nsubj'], 50: [...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1052</th>\n",
              "      <td>261.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>200.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td>,</td>\n",
              "      <td>Secondly. every year the world shows new thing...</td>\n",
              "      <td>{199: 'Secondly', 200: '.', 201: 'every', 202:...</td>\n",
              "      <td>{199: ['Secondly', 'ADV', 'RB', 'ROOT'], 200: ...</td>\n",
              "      <td>Secondly, every year the world shows new thing...</td>\n",
              "      <td>{199: 'Secondly', 200: ',', 201: 'every', 202:...</td>\n",
              "      <td>{199: ['Secondly', 'ADV', 'RB', 'advmod'], 200...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1053</th>\n",
              "      <td>374.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>188.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td>,</td>\n",
              "      <td>In fact. the njtjrious soft skill essential fo...</td>\n",
              "      <td>{186: 'In', 187: 'fact', 188: '.', 189: 'the',...</td>\n",
              "      <td>{186: ['In', 'ADP', 'IN', 'ROOT'], 187: ['fact...</td>\n",
              "      <td>In fact, the njtjrious soft skill essential fo...</td>\n",
              "      <td>{186: 'In', 187: 'fact', 188: ',', 189: 'the',...</td>\n",
              "      <td>{186: ['In', 'ADP', 'IN', 'prep'], 187: ['fact...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1054</th>\n",
              "      <td>454.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>217.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td></td>\n",
              "      <td>It. can really can be helpful in developing ou...</td>\n",
              "      <td>{216: 'It', 217: '.', 218: 'can', 219: 'really...</td>\n",
              "      <td>{216: ['It', 'PRON', 'PRP', 'ROOT'], 217: ['.'...</td>\n",
              "      <td>It  can really can be helpful in developing ou...</td>\n",
              "      <td>{216: 'It', 217: 'can', 218: 'really', 219: 'c...</td>\n",
              "      <td>{216: ['It', 'PRON', 'PRP', 'nsubj'], 217: ['c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1055</th>\n",
              "      <td>637.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td>,</td>\n",
              "      <td>As regards the USA. it also came down consider...</td>\n",
              "      <td>{112: 'As', 113: 'regards', 114: 'the', 115: '...</td>\n",
              "      <td>{112: ['As', 'SCONJ', 'IN', 'mark'], 113: ['re...</td>\n",
              "      <td>As regards the USA, it also came down consider...</td>\n",
              "      <td>{112: 'As', 113: 'regards', 114: 'the', 115: '...</td>\n",
              "      <td>{112: ['As', 'SCONJ', 'IN', 'mark'], 113: ['re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1056</th>\n",
              "      <td>646.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>244.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td>,</td>\n",
              "      <td>In conclusion. authorities should pay an atten...</td>\n",
              "      <td>{242: 'In', 243: 'conclusion', 244: '.', 245: ...</td>\n",
              "      <td>{242: ['In', 'ADP', 'IN', 'ROOT'], 243: ['conc...</td>\n",
              "      <td>In conclusion, authorities should pay an atten...</td>\n",
              "      <td>{242: 'In', 243: 'conclusion', 244: ',', 245: ...</td>\n",
              "      <td>{242: ['In', 'ADP', 'IN', 'prep'], 243: ['conc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1057</th>\n",
              "      <td>668.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>308.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td>,</td>\n",
              "      <td>In conclusion, most of us thinks that every hu...</td>\n",
              "      <td>{293: 'In', 294: 'conclusion', 295: ',', 296: ...</td>\n",
              "      <td>{293: ['In', 'ADP', 'IN', 'prep'], 294: ['conc...</td>\n",
              "      <td>In conclusion, most of us thinks that every hu...</td>\n",
              "      <td>{293: 'In', 294: 'conclusion', 295: ',', 296: ...</td>\n",
              "      <td>{293: ['In', 'ADP', 'IN', 'prep'], 294: ['conc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1058</th>\n",
              "      <td>728.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td>,</td>\n",
              "      <td>the number are devastating. considering this g...</td>\n",
              "      <td>{83: 'the', 84: 'number', 85: 'are', 86: 'deva...</td>\n",
              "      <td>{83: ['the', 'DET', 'DT', 'det'], 84: ['number...</td>\n",
              "      <td>the number are devastating, considering this g...</td>\n",
              "      <td>{83: 'the', 84: 'number', 85: 'are', 86: 'deva...</td>\n",
              "      <td>{83: ['the', 'DET', 'DT', 'det'], 84: ['number...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1059</th>\n",
              "      <td>786.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td>,</td>\n",
              "      <td>However, while in 4 out of 5 countries, shown ...</td>\n",
              "      <td>{44: 'However', 45: ',', 46: 'while', 47: 'in'...</td>\n",
              "      <td>{44: ['However', 'ADV', 'RB', 'advmod'], 45: [...</td>\n",
              "      <td>However, while in 4 out of 5 countries, shown ...</td>\n",
              "      <td>{44: 'However', 45: ',', 46: 'while', 47: 'in'...</td>\n",
              "      <td>{44: ['However', 'ADV', 'RB', 'advmod'], 45: [...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1060</th>\n",
              "      <td>793.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>218.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td></td>\n",
              "      <td>That is why education can not be considered as...</td>\n",
              "      <td>{209: 'That', 210: 'is', 211: 'why', 212: 'edu...</td>\n",
              "      <td>{209: ['That', 'DET', 'DT', 'nsubj'], 210: ['i...</td>\n",
              "      <td>That is why education can not be considered as...</td>\n",
              "      <td>{209: 'That', 210: 'is', 211: 'why', 212: 'edu...</td>\n",
              "      <td>{209: ['That', 'DET', 'DT', 'nsubj'], 210: ['i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1061</th>\n",
              "      <td>853.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>310.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td>,</td>\n",
              "      <td>For example, let's take a look into history. w...</td>\n",
              "      <td>{300: 'For', 301: 'example', 302: ',', 303: 'l...</td>\n",
              "      <td>{300: ['For', 'ADP', 'IN', 'prep'], 301: ['exa...</td>\n",
              "      <td>For example, let's take a look into history, w...</td>\n",
              "      <td>{300: 'For', 301: 'example', 302: ',', 303: 'l...</td>\n",
              "      <td>{300: ['For', 'ADP', 'IN', 'prep'], 301: ['exa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1062</th>\n",
              "      <td>956.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>241.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td></td>\n",
              "      <td>However, I still hold the opinion. that educat...</td>\n",
              "      <td>{234: 'However', 235: ',', 236: 'I', 237: 'sti...</td>\n",
              "      <td>{234: ['However', 'ADV', 'RB', 'advmod'], 235:...</td>\n",
              "      <td>However, I still hold the opinion  that educat...</td>\n",
              "      <td>{234: 'However', 235: ',', 236: 'I', 237: 'sti...</td>\n",
              "      <td>{234: ['However', 'ADV', 'RB', 'advmod'], 235:...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1063</th>\n",
              "      <td>963.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td>,</td>\n",
              "      <td>Overall. the graph shows that in all countries...</td>\n",
              "      <td>{35: 'Overall', 36: '.', 37: 'the', 38: 'graph...</td>\n",
              "      <td>{35: ['Overall', 'ADV', 'RB', 'ROOT'], 36: ['....</td>\n",
              "      <td>Overall, the graph shows that in all countries...</td>\n",
              "      <td>{35: 'Overall', 36: ',', 37: 'the', 38: 'graph...</td>\n",
              "      <td>{35: ['Overall', 'ADV', 'RB', 'advmod'], 36: [...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1064</th>\n",
              "      <td>1014.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>107.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td>,</td>\n",
              "      <td>and he/ she can do some mistakes. and the resu...</td>\n",
              "      <td>{99: 'and', 100: 'he', 101: '/', 102: 'she', 1...</td>\n",
              "      <td>{99: ['and', 'CCONJ', 'CC', 'cc'], 100: ['he',...</td>\n",
              "      <td>and he/ she can do some mistakes, and the resu...</td>\n",
              "      <td>{99: 'and', 100: 'he', 101: 'she', 102: 'can',...</td>\n",
              "      <td>{99: ['and', 'CCONJ', 'CC', 'cc'], 100: ['he',...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1065</th>\n",
              "      <td>1063.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>134.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td></td>\n",
              "      <td>What is more, the differences in income in Ger...</td>\n",
              "      <td>{107: 'What', 108: 'is', 109: 'more', 110: ','...</td>\n",
              "      <td>{107: ['What', 'PRON', 'WP', 'nsubj'], 108: ['...</td>\n",
              "      <td>What is more, the differences in income in Ger...</td>\n",
              "      <td>{107: 'What', 108: 'is', 109: 'more', 110: ','...</td>\n",
              "      <td>{107: ['What', 'PRON', 'WP', 'nsubj'], 108: ['...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1066</th>\n",
              "      <td>1245.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>262.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td>,</td>\n",
              "      <td>One other moment, if you a very clever child i...</td>\n",
              "      <td>{248: 'One', 249: 'other', 250: 'moment', 251:...</td>\n",
              "      <td>{248: ['One', 'NUM', 'CD', 'nummod'], 249: ['o...</td>\n",
              "      <td>One other moment, if you a very clever child i...</td>\n",
              "      <td>{248: 'One', 249: 'other', 250: 'moment', 251:...</td>\n",
              "      <td>{248: ['One', 'NUM', 'CD', 'nummod'], 249: ['o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1067</th>\n",
              "      <td>1251.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>103.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td>,</td>\n",
              "      <td>So to manage such task school should't be comp...</td>\n",
              "      <td>{65: 'So', 66: 'to', 67: 'manage', 68: 'such',...</td>\n",
              "      <td>{65: ['So', 'ADV', 'RB', 'advmod'], 66: ['to',...</td>\n",
              "      <td>So to manage such task school should't be comp...</td>\n",
              "      <td>{65: 'So', 66: 'to', 67: 'manage', 68: 'such',...</td>\n",
              "      <td>{65: ['So', 'ADV', 'RB', 'advmod'], 66: ['to',...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1068</th>\n",
              "      <td>1351.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td>?</td>\n",
              "      <td>The idea that education, including schools and...</td>\n",
              "      <td>{1: 'The', 2: 'idea', 3: 'that', 4: 'education...</td>\n",
              "      <td>{1: ['The', 'DET', 'DT', 'det'], 2: ['idea', '...</td>\n",
              "      <td>The idea that education, including schools and...</td>\n",
              "      <td>{1: 'The', 2: 'idea', 3: 'that', 4: 'education...</td>\n",
              "      <td>{1: ['The', 'DET', 'DT', 'det'], 2: ['idea', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1069</th>\n",
              "      <td>1369.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td>,</td>\n",
              "      <td>Firstly. equality which is the core of getting...</td>\n",
              "      <td>{66: 'Firstly', 67: '.', 68: 'equality', 69: '...</td>\n",
              "      <td>{66: ['Firstly', 'ADV', 'RB', 'ROOT'], 67: ['....</td>\n",
              "      <td>Firstly, equality which is the core of getting...</td>\n",
              "      <td>{66: 'Firstly', 67: ',', 68: 'equality', 69: '...</td>\n",
              "      <td>{66: ['Firstly', 'ADV', 'RB', 'advmod'], 67: [...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1070</th>\n",
              "      <td>1395.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>399.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td></td>\n",
              "      <td>Why did authors selected Germany instead of,. ...</td>\n",
              "      <td>{391: 'Why', 392: 'did', 393: 'authors', 394: ...</td>\n",
              "      <td>{391: ['Why', 'ADV', 'WRB', 'advmod'], 392: ['...</td>\n",
              "      <td>Why did authors selected Germany instead of,  ...</td>\n",
              "      <td>{391: 'Why', 392: 'did', 393: 'authors', 394: ...</td>\n",
              "      <td>{391: ['Why', 'ADV', 'WRB', 'advmod'], 392: ['...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1071</th>\n",
              "      <td>1493.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>102.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td>,</td>\n",
              "      <td>Each of us have a birth in a different familie...</td>\n",
              "      <td>{92: 'Each', 93: 'of', 94: 'us', 95: 'have', 9...</td>\n",
              "      <td>{92: ['Each', 'DET', 'DT', 'nsubj'], 93: ['of'...</td>\n",
              "      <td>Each of us have a birth in a different familie...</td>\n",
              "      <td>{92: 'Each', 93: 'of', 94: 'us', 95: 'have', 9...</td>\n",
              "      <td>{92: ['Each', 'DET', 'DT', 'nsubj'], 93: ['of'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1072</th>\n",
              "      <td>1501.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>314.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td>,</td>\n",
              "      <td>276 To sum up, I still consider that education...</td>\n",
              "      <td>{298: '276', 299: 'To', 300: 'sum', 301: 'up',...</td>\n",
              "      <td>{298: ['276', 'NUM', 'CD', 'ROOT'], 299: ['To'...</td>\n",
              "      <td>276 To sum up, I still consider that education...</td>\n",
              "      <td>{298: '276', 299: 'To', 300: 'sum', 301: 'up',...</td>\n",
              "      <td>{298: ['276', 'NUM', 'CD', 'ROOT'], 299: ['To'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1073</th>\n",
              "      <td>1512.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td></td>\n",
              "      <td>The line graph depicts the differnce of income...</td>\n",
              "      <td>{1: 'The', 2: 'line', 3: 'graph', 4: 'depicts'...</td>\n",
              "      <td>{1: ['The', 'DET', 'DT', 'det'], 2: ['line', '...</td>\n",
              "      <td>The line graph depicts the differnce of income...</td>\n",
              "      <td>{1: 'The', 2: 'line', 3: 'graph', 4: 'depicts'...</td>\n",
              "      <td>{1: ['The', 'DET', 'DT', 'det'], 2: ['line', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1074</th>\n",
              "      <td>1550.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td>?</td>\n",
              "      <td>And if one person has right to study, why the ...</td>\n",
              "      <td>{58: 'And', 59: 'if', 60: 'one', 61: 'person',...</td>\n",
              "      <td>{58: ['And', 'CCONJ', 'CC', 'cc'], 59: ['if', ...</td>\n",
              "      <td>And if one person has right to study, why the ...</td>\n",
              "      <td>{58: 'And', 59: 'if', 60: 'one', 61: 'person',...</td>\n",
              "      <td>{58: ['And', 'CCONJ', 'CC', 'cc'], 59: ['if', ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1075</th>\n",
              "      <td>1550.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>259.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td>,</td>\n",
              "      <td>All in all, education the most part of educati...</td>\n",
              "      <td>{244: 'All', 245: 'in', 246: 'all', 247: ',', ...</td>\n",
              "      <td>{244: ['All', 'DET', 'DT', 'advmod'], 245: ['i...</td>\n",
              "      <td>All in all, education the most part of educati...</td>\n",
              "      <td>{244: 'All', 245: 'in', 246: 'all', 247: ',', ...</td>\n",
              "      <td>{244: ['All', 'DET', 'DT', 'advmod'], 245: ['i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1076</th>\n",
              "      <td>1555.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>205.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td>?</td>\n",
              "      <td>What for to take in university student who do ...</td>\n",
              "      <td>{192: 'What', 193: 'for', 194: 'to', 195: 'tak...</td>\n",
              "      <td>{192: ['What', 'PRON', 'WP', 'dobj'], 193: ['f...</td>\n",
              "      <td>What for to take in university student who do ...</td>\n",
              "      <td>{192: 'What', 193: 'for', 194: 'to', 195: 'tak...</td>\n",
              "      <td>{192: ['What', 'PRON', 'WP', 'ROOT'], 193: ['f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1077</th>\n",
              "      <td>1555.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>234.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td>?</td>\n",
              "      <td>But you will say: how about people that want t...</td>\n",
              "      <td>{206: 'But', 207: 'you', 208: 'will', 209: 'sa...</td>\n",
              "      <td>{206: ['But', 'CCONJ', 'CC', 'cc'], 207: ['you...</td>\n",
              "      <td>But you will say: how about people that want t...</td>\n",
              "      <td>{206: 'But', 207: 'you', 208: 'will', 209: 'sa...</td>\n",
              "      <td>{206: ['But', 'CCONJ', 'CC', 'cc'], 207: ['you...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1083</th>\n",
              "      <td>370.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>. Because</td>\n",
              "      <td>because</td>\n",
              "      <td>37 On the one hand, there will be difficult fo...</td>\n",
              "      <td>{43: '37', 44: 'On', 45: 'the', 46: 'one', 47:...</td>\n",
              "      <td>{43: ['37', 'NUM', 'CD', 'ROOT'], 44: ['On', '...</td>\n",
              "      <td>37 On the one hand, there will be difficult fo...</td>\n",
              "      <td>{43: '37', 44: 'On', 45: 'the', 46: 'one', 47:...</td>\n",
              "      <td>{43: ['37', 'NUM', 'CD', 'ROOT'], 44: ['On', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1096</th>\n",
              "      <td>323.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>247.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>. Because,</td>\n",
              "      <td>, because</td>\n",
              "      <td>First, a free education have to be given those...</td>\n",
              "      <td>{230: 'First', 231: ',', 232: 'a', 233: 'free'...</td>\n",
              "      <td>{230: ['First', 'ADV', 'RB', 'advmod'], 231: [...</td>\n",
              "      <td>First, a free education have to be given those...</td>\n",
              "      <td>{230: 'First', 231: ',', 232: 'a', 233: 'free'...</td>\n",
              "      <td>{230: ['First', 'ADV', 'RB', 'advmod'], 231: [...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1097</th>\n",
              "      <td>1598.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>285.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>. But</td>\n",
              "      <td>,</td>\n",
              "      <td>Although there are some people who justify edu...</td>\n",
              "      <td>{246: 'Although', 247: 'there', 248: 'are', 24...</td>\n",
              "      <td>{246: ['Although', 'SCONJ', 'IN', 'mark'], 247...</td>\n",
              "      <td>Although there are some people who justify edu...</td>\n",
              "      <td>{246: 'Although', 247: 'there', 248: 'are', 24...</td>\n",
              "      <td>{246: ['Although', 'SCONJ', 'IN', 'mark'], 247...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1100</th>\n",
              "      <td>852.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>. Gradual</td>\n",
              "      <td>gradual</td>\n",
              "      <td>Japan is also a bit different from the other c...</td>\n",
              "      <td>{121: 'Japan', 122: 'is', 123: 'also', 124: 'a...</td>\n",
              "      <td>{121: ['Japan', 'PROPN', 'NNP', 'nsubj'], 122:...</td>\n",
              "      <td>Japan is also a bit different from the other c...</td>\n",
              "      <td>{121: 'Japan', 122: 'is', 123: 'also', 124: 'a...</td>\n",
              "      <td>{121: ['Japan', 'PROPN', 'NNP', 'nsubj'], 122:...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1101</th>\n",
              "      <td>1509.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>275.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>. i</td>\n",
              "      <td>, I</td>\n",
              "      <td>In conclusion. i want to say that in today's w...</td>\n",
              "      <td>{273: 'In', 274: 'conclusion', 275: '.', 276: ...</td>\n",
              "      <td>{273: ['In', 'ADP', 'IN', 'ROOT'], 274: ['conc...</td>\n",
              "      <td>In conclusion, I want to say that in today's w...</td>\n",
              "      <td>{273: 'In', 274: 'conclusion', 275: ',', 276: ...</td>\n",
              "      <td>{273: ['In', 'ADP', 'IN', 'prep'], 274: ['conc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1107</th>\n",
              "      <td>1154.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>218.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>. Their</td>\n",
              "      <td>э=-@ their</td>\n",
              "      <td>One the other hand, there are some basic thing...</td>\n",
              "      <td>{203: 'One', 204: 'the', 205: 'other', 206: 'h...</td>\n",
              "      <td>{203: ['One', 'NUM', 'CD', 'nummod'], 204: ['t...</td>\n",
              "      <td>One the other hand, there are some basic thing...</td>\n",
              "      <td>{203: 'One', 204: 'the', 205: 'other', 206: 'h...</td>\n",
              "      <td>{203: ['One', 'NUM', 'CD', 'nummod'], 204: ['t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1119</th>\n",
              "      <td>505.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>232.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>; he</td>\n",
              "      <td>. He</td>\n",
              "      <td>For instance, there was one of the most famous...</td>\n",
              "      <td>{218: 'For', 219: 'instance', 220: ',', 221: '...</td>\n",
              "      <td>{218: ['For', 'ADP', 'IN', 'prep'], 219: ['ins...</td>\n",
              "      <td>For instance, there was one of the most famous...</td>\n",
              "      <td>{218: 'For', 219: 'instance', 220: ',', 221: '...</td>\n",
              "      <td>{218: ['For', 'ADP', 'IN', 'prep'], 219: ['ins...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1481</th>\n",
              "      <td>900.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>countries</td>\n",
              "      <td>countries, it</td>\n",
              "      <td>Looking at other countries can be seen that th...</td>\n",
              "      <td>{123: 'Looking', 124: 'at', 125: 'other', 126:...</td>\n",
              "      <td>{123: ['Looking', 'VERB', 'VBG', 'csubjpass'],...</td>\n",
              "      <td>Looking at other countries, it can be seen tha...</td>\n",
              "      <td>{123: 'Looking', 124: 'at', 125: 'other', 126:...</td>\n",
              "      <td>{123: ['Looking', 'VERB', 'VBG', 'advcl'], 124...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1483</th>\n",
              "      <td>943.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>countries</td>\n",
              "      <td>countries, it</td>\n",
              "      <td>In the other countries was the opposite situat...</td>\n",
              "      <td>{87: 'In', 88: 'the', 89: 'other', 90: 'countr...</td>\n",
              "      <td>{87: ['In', 'ADP', 'IN', 'prep'], 88: ['the', ...</td>\n",
              "      <td>In the other countries, it was the opposite si...</td>\n",
              "      <td>{87: 'In', 88: 'the', 89: 'other', 90: 'countr...</td>\n",
              "      <td>{87: ['In', 'ADP', 'IN', 'prep'], 88: ['the', ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1725</th>\n",
              "      <td>273.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>63.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>essay</td>\n",
              "      <td>essay, I</td>\n",
              "      <td>So in this essay will be demonstrates both poi...</td>\n",
              "      <td>{60: 'So', 61: 'in', 62: 'this', 63: 'essay', ...</td>\n",
              "      <td>{60: ['So', 'ADV', 'RB', 'cc'], 61: ['in', 'AD...</td>\n",
              "      <td>So in this essay, I will be demonstrates both ...</td>\n",
              "      <td>{60: 'So', 61: 'in', 62: 'this', 63: 'essay', ...</td>\n",
              "      <td>{60: ['So', 'ADV', 'RB', 'advmod'], 61: ['in',...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1811</th>\n",
              "      <td>1570.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>172.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>for getting</td>\n",
              "      <td>, to get</td>\n",
              "      <td>Firstly, it is impossible to exept liers and k...</td>\n",
              "      <td>{134: 'Firstly', 135: ',', 136: 'it', 137: 'is...</td>\n",
              "      <td>{134: ['Firstly', 'ADV', 'RB', 'advmod'], 135:...</td>\n",
              "      <td>Firstly, it is impossible to exept liers and k...</td>\n",
              "      <td>{134: 'Firstly', 135: ',', 136: 'it', 137: 'is...</td>\n",
              "      <td>{134: ['Firstly', 'ADV', 'RB', 'advmod'], 135:...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2262</th>\n",
              "      <td>728.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>187.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>new zealand an uk.</td>\n",
              "      <td>New Zealand and the UK,</td>\n",
              "      <td>the impressive results among others show count...</td>\n",
              "      <td>{179: 'the', 180: 'impressive', 181: 'results'...</td>\n",
              "      <td>{179: ['the', 'DET', 'DT', 'det'], 180: ['impr...</td>\n",
              "      <td>the impressive results among others show count...</td>\n",
              "      <td>{179: 'the', 180: 'impressive', 181: 'results'...</td>\n",
              "      <td>{179: ['the', 'DET', 'DT', 'det'], 180: ['impr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2761</th>\n",
              "      <td>1503.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>301.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>study</td>\n",
              "      <td>study. It</td>\n",
              "      <td>From my humble opinion, every person should ha...</td>\n",
              "      <td>{288: 'From', 289: 'my', 290: 'humble', 291: '...</td>\n",
              "      <td>{288: ['From', 'ADP', 'IN', 'prep'], 289: ['my...</td>\n",
              "      <td>From my humble opinion, every person should ha...</td>\n",
              "      <td>{288: 'From', 289: 'my', 290: 'humble', 291: '...</td>\n",
              "      <td>{288: ['From', 'ADP', 'IN', 'prep'], 289: ['my...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2916</th>\n",
              "      <td>1404.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>think,</td>\n",
              "      <td>thinks</td>\n",
              "      <td>One part think, that education should be provi...</td>\n",
              "      <td>{36: 'One', 37: 'part', 38: 'think', 39: ',', ...</td>\n",
              "      <td>{36: ['One', 'NUM', 'CD', 'nummod'], 37: ['par...</td>\n",
              "      <td>One part thinks that education should be provi...</td>\n",
              "      <td>{36: 'One', 37: 'part', 38: 'thinks', 39: 'tha...</td>\n",
              "      <td>{36: ['One', 'NUM', 'CD', 'nummod'], 37: ['par...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2917</th>\n",
              "      <td>1404.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>think,</td>\n",
              "      <td>thinks</td>\n",
              "      <td>One part think, that education should be provi...</td>\n",
              "      <td>{36: 'One', 37: 'part', 38: 'think', 39: ',', ...</td>\n",
              "      <td>{36: ['One', 'NUM', 'CD', 'nummod'], 37: ['par...</td>\n",
              "      <td>One part think, that education should be provi...</td>\n",
              "      <td>{36: 'One', 37: 'part', 38: 'think', 39: ',', ...</td>\n",
              "      <td>{36: ['One', 'NUM', 'CD', 'nummod'], 37: ['par...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3010</th>\n",
              "      <td>858.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>punct</td>\n",
              "      <td>UK</td>\n",
              "      <td>UK, it</td>\n",
              "      <td>In Japan, USA and UK did not exceeded 8%.</td>\n",
              "      <td>{181: 'In', 182: 'Japan', 183: ',', 184: 'USA'...</td>\n",
              "      <td>{181: ['In', 'ADP', 'IN', 'prep'], 182: ['Japa...</td>\n",
              "      <td>In Japan, USA and UK, it did not exceeded 8%.</td>\n",
              "      <td>{181: 'In', 182: 'Japan', 183: ',', 184: 'USA'...</td>\n",
              "      <td>{181: ['In', 'ADP', 'IN', 'prep'], 182: ['Japa...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-67481e55-06ec-44d9-89c3-9f8cb5ce4ba1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-67481e55-06ec-44d9-89c3-9f8cb5ce4ba1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-67481e55-06ec-44d9-89c3-9f8cb5ce4ba1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 481
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for el in punctuation.values:\n",
        "  print(el[4])\n",
        "  print(el[5])\n",
        "  print(el[6])\n",
        "  print(el[9])\n",
        "  print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivpT5k3_-94a",
        "outputId": "8d884b84-456f-4d0f-c571-0923d2d355c2"
      },
      "execution_count": 402,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "78%\n",
            ", 78%\n",
            "As Africa Today had recenly reported 78% of students living in Africa simply do not have enough income to pay for University. Moreover, there are some professions requiring special education and equipment.\n",
            "As Africa Today had recenly reported, 78% of students living in Africa simply do not have enough income to pay for University. Moreover, there are some professions requiring special education and equipment.\n",
            "\n",
            "\n",
            "1995.0\n",
            "1995.0\n",
            "Only germany saw a slight increase from 1990 to 1995 To sum up, there is a general tendency of a decline in income difference between men and women, which is quite positive.\n",
            "Only germany saw a slight increase from 1990 to 1995.0 To sum up, there is a general tendency of a decline in income difference between men and women, which is quite positive.\n",
            "\n",
            "\n",
            "1995.0\n",
            "1995.0\n",
            "As we can see Japans dinamics on the graph, shows stable decrease of unequality of incomes for 10 years since 1985 to 1995 Those dinamic is represents all of the countries on the graph, exept the Germany, where we can see that persentage of diffference of male and female earnings slightly rising in the last 5 years of researching. As we can see, this graph shows us a steady drop of difference between male and female incomes.\n",
            "As we can see Japans dinamics on the graph, shows stable decrease of unequality of incomes for 10 years since 1985 to 1995.0 Those dinamic is represents all of the countries on the graph, exept the Germany, where we can see that persentage of diffference of male and female earnings slightly rising in the last 5 years of researching. As we can see, this graph shows us a steady drop of difference between male and female incomes.\n",
            "\n",
            "\n",
            ",\n",
            ". A\n",
            "In UK difference decreased from around 36 by 13 percent, similar fall was in USA, but where it started with 39 percent and ended with 20.\n",
            "In UK difference decreased from around 36 by 13 percent. A similar fall was in USA, but where it started with 39 percent and ended with 20.\n",
            "\n",
            "\n",
            ", it\n",
            ". That\n",
            "Moreover, school eduction or at least elementary one is obligatory in most countries, it means everybody should have this education. It will be very extremely unfair if all people have to study, but not everyone could afford it.\n",
            "Moreover, school eduction or at least elementary one is obligatory in most countries. That means everybody should have this education. It will be very extremely unfair if all people have to study, but not everyone could afford it.\n",
            "\n",
            "\n",
            ".\n",
            "?\n",
            "For instance, a lot of famous people as Lomonosov or Mendeleev had very poor financial support, but got education, but how many potentially great scientists, musicians, politics and poets did not get it and did not show theirs talents to humanity. Secondly, education for everyone lets to make step forward in progress of all civilization.\n",
            "For instance, a lot of famous people as Lomonosov or Mendeleev had very poor financial support, but got education, but how many potentially great scientists, musicians, politics and poets did not get it and did not show theirs talents to humanity? Secondly, education for everyone lets to make step forward in progress of all civilization.\n",
            "\n",
            "\n",
            ".\n",
            ",\n",
            "In Germany the difference plummed by more than 10% in 10 years. but after that raised slowly and showed 12% of difference of salaries in 1995.\n",
            "In Germany the difference plummed by more than 10% in 10 years, but after that raised slowly and showed 12% of difference of salaries in 1995.\n",
            "\n",
            "\n",
            ".\n",
            ",\n",
            "According to the graph. in 1980 USA rate was almost as high as Japan and was almost 40%.\n",
            "According to the graph, in 1980 USA rate was almost as high as Japan and was almost 40%.\n",
            "\n",
            "\n",
            ".\n",
            "?\n",
            "Thus, how can perspective programmer get into IT sphere if he has n't learned the basement of mathematics and computer science. Secondly, differnt level of access to education creates class inequality.\n",
            "Thus, how can perspective programmer get into IT sphere if he has n't learned the basement of mathematics and computer science? Secondly, differnt level of access to education creates class inequality.\n",
            "\n",
            "\n",
            ".\n",
            "?\n",
            "Of course, there were, are and will be people who would think that education will not help them anyway, so why spending money or time on that. For the first matter the education SHOULD be available for free in every corner of the modern world.\n",
            "Of course, there were, are and will be people who would think that education will not help them anyway, so why spending money or time on that? For the first matter the education SHOULD be available for free in every corner of the modern world.\n",
            "\n",
            "\n",
            ".\n",
            ",\n",
            "Overall. while statistics for all countries experienced significant changes through the period, all countries demonstrated a decreasing long-run trend.\n",
            "Overall, while statistics for all countries experienced significant changes through the period, all countries demonstrated a decreasing long-run trend.\n",
            "\n",
            "\n",
            ".\n",
            ",\n",
            "That is to say that education ultimately thrives the mankind. and it is our duty as human being to foster mankind.\n",
            "That is to say that education ultimately thrives the mankind, and it is our duty as human being to foster mankind.\n",
            "\n",
            "\n",
            ".\n",
            "\n",
            "The percentage difference in income between genders has decreased by about 10 percent in Japan, Germany, New Zeland and the UK, and by 20 percents in the USA.\n",
            "The percentage difference in income between genders has decreased by about 10 percent in Japan, Germany, New Zeland and the UK, and by 20 percents in the USA \n",
            "\n",
            "\n",
            ".\n",
            ",\n",
            "People from Japan have maximum earnings, and this was in 1985. and most little of earning have people from Germany in 1990, but men and women from Germany do not have minimum earnings in end, In start three and two countries have about amount of earnings (Japan, the UK, USA and New Zealand, Germany). People from UK and New Zealand become have better earning than people from another country between 1980 and 1985.\n",
            "People from Japan have maximum earnings, and this was in 1985, and most little of earning have people from Germany in 1990, but men and women from Germany do not have minimum earnings in end, In start three and two countries have about amount of earnings (Japan, the UK, USA and New Zealand, Germany). People from UK and New Zealand become have better earning than people from another country between 1980 and 1985.\n",
            "\n",
            "\n",
            ".\n",
            ",\n",
            "Secondly. every year the world shows new things such as a virus, for example, Covid-19 and people should research this and help people, who need it.\n",
            "Secondly, every year the world shows new things such as a virus, for example, Covid-19 and people should research this and help people, who need it.\n",
            "\n",
            "\n",
            ".\n",
            ",\n",
            "In fact. the njtjrious soft skill essential for a well-balanced personatity.\n",
            "In fact, the njtjrious soft skill essential for a well-balanced personatity.\n",
            "\n",
            "\n",
            ".\n",
            "\n",
            "It. can really can be helpful in developing our future By the way there are some people who disagree with this point of view.\n",
            "It  can really can be helpful in developing our future By the way there are some people who disagree with this point of view.\n",
            "\n",
            "\n",
            ".\n",
            ",\n",
            "As regards the USA. it also came down considerably up to 1995 and accounted for around 20%/ Regarding New Zeland and Germany, the percentage was similar in 1984 (nearly 20%) and it declined slightly in 1985 and accounted for approximatelly 11%.\n",
            "As regards the USA, it also came down considerably up to 1995 and accounted for around 20%/ Regarding New Zeland and Germany, the percentage was similar in 1984 (nearly 20%) and it declined slightly in 1985 and accounted for approximatelly 11%.\n",
            "\n",
            "\n",
            ".\n",
            ",\n",
            "In conclusion. authorities should pay an attention to free education for every citizen: because it will have a positive impact on population and on country in all.\n",
            "In conclusion, authorities should pay an attention to free education for every citizen: because it will have a positive impact on population and on country in all.\n",
            "\n",
            "\n",
            ".\n",
            ",\n",
            "In conclusion, most of us thinks that every human have right to be educated. especially free.\n",
            "In conclusion, most of us thinks that every human have right to be educated, especially free.\n",
            "\n",
            "\n",
            ".\n",
            ",\n",
            "the number are devastating. considering this graph was made from the statistics of developed countries such as the USA, UK, and other economically developed countries.\n",
            "the number are devastating, considering this graph was made from the statistics of developed countries such as the USA, UK, and other economically developed countries.\n",
            "\n",
            "\n",
            ".\n",
            ",\n",
            "However, while in 4 out of 5 countries, shown on the graph the smallest difference was spotted in year 1995. it is not the same in Germany, where the lowest results were spotted in year 1990 and the percentage difference just started increasing after. In 1980, Japan was the country that has the largest gap income, at 40 percents, followed shortly after by USA with approximately 38 percents.\n",
            "However, while in 4 out of 5 countries, shown on the graph the smallest difference was spotted in year 1995, it is not the same in Germany, where the lowest results were spotted in year 1990 and the percentage difference just started increasing after. In 1980, Japan was the country that has the largest gap income, at 40 percents, followed shortly after by USA with approximately 38 percents.\n",
            "\n",
            "\n",
            ".\n",
            "\n",
            "That is why education can not be considered as. a privilege of a wealthy people or some kind of a luxury.\n",
            "That is why education can not be considered as  a privilege of a wealthy people or some kind of a luxury.\n",
            "\n",
            "\n",
            ".\n",
            ",\n",
            "For example, let's take a look into history. where we can see that governments which provided education to most of their citizens had a faster growing economy, was more successful, and overall the life was more pleassant as education would have an effect on all spheres.\n",
            "For example, let's take a look into history, where we can see that governments which provided education to most of their citizens had a faster growing economy, was more successful, and overall the life was more pleassant as education would have an effect on all spheres.\n",
            "\n",
            "\n",
            ".\n",
            "\n",
            "However, I still hold the opinion. that education is truly a basic human right, but to survive one does not nesessarily need to move throuth all its stages: from attending seconday school to getting a bachelors' degree.\n",
            "However, I still hold the opinion  that education is truly a basic human right, but to survive one does not nesessarily need to move throuth all its stages: from attending seconday school to getting a bachelors' degree.\n",
            "\n",
            "\n",
            ".\n",
            ",\n",
            "Overall. the graph shows that in all countries percentage of difference in salaries has dropped dramatically by around 10 per cent.\n",
            "Overall, the graph shows that in all countries percentage of difference in salaries has dropped dramatically by around 10 per cent.\n",
            "\n",
            "\n",
            ".\n",
            ",\n",
            "and he/ she can do some mistakes. and the resualtes of exams will be bad.\n",
            "and he/ she can do some mistakes, and the resualtes of exams will be bad.\n",
            "\n",
            "\n",
            ".\n",
            "\n",
            "What is more, the differences in income in Germany and New Zealand fall in ten percent that is not such as marked than in the USA. (from 23 to 13 per centage in Germany, from 20% to 10% in New Zealand).\n",
            "What is more, the differences in income in Germany and New Zealand fall in ten percent that is not such as marked than in the USA  (from 23 to 13 per centage in Germany, from 20% to 10% in New Zealand).\n",
            "\n",
            "\n",
            ".\n",
            ",\n",
            "One other moment, if you a very clever child in a poor family. I think in this case we can take help from investors.\n",
            "One other moment, if you a very clever child in a poor family, I think in this case we can take help from investors.\n",
            "\n",
            "\n",
            ".\n",
            ",\n",
            "So to manage such task school should't be complex system, school stuff for example teachers are not required to be scientists, the best of the best The just have to know how to communicate with kids. how to burst interest to new knowlege in them.\n",
            "So to manage such task school should't be complex system, school stuff for example teachers are not required to be scientists, the best of the best The just have to know how to communicate with kids, how to burst interest to new knowlege in them.\n",
            "\n",
            "\n",
            ".\n",
            "?\n",
            "The idea that education, including schools and universities, needs to be free and debates around this idea are very old, and it is impossible to discuss without answer to the first question: is education the privelegy for the noble and rich people or is it the human right as for example similarly discussed medicine rights. I suppose that answer to this question is only one, but unfortunately there are two radical different positions.\n",
            "The idea that education, including schools and universities, needs to be free and debates around this idea are very old, and it is impossible to discuss without answer to the first question: is education the privelegy for the noble and rich people or is it the human right as for example similarly discussed medicine rights? I suppose that answer to this question is only one, but unfortunately there are two radical different positions.\n",
            "\n",
            "\n",
            ".\n",
            ",\n",
            "Firstly. equality which is the core of getting same quality of education to everyone.\n",
            "Firstly, equality which is the core of getting same quality of education to everyone.\n",
            "\n",
            "\n",
            ".\n",
            "\n",
            "Why did authors selected Germany instead of,. for instance, France?\n",
            "Why did authors selected Germany instead of,  for instance, France?\n",
            "\n",
            "\n",
            ".\n",
            ",\n",
            "Each of us have a birth in a different families. countries and often in families without a lot of money.\n",
            "Each of us have a birth in a different families, countries and often in families without a lot of money.\n",
            "\n",
            "\n",
            ".\n",
            ",\n",
            "276 To sum up, I still consider that education should be free for all students. but u can apply for University just if you are clever enough.\n",
            "276 To sum up, I still consider that education should be free for all students, but u can apply for University just if you are clever enough.\n",
            "\n",
            "\n",
            ".\n",
            "\n",
            "The line graph depicts the differnce of income between men and women in five countries (Japan, United States of America, United Kingdom,. Germany and New Zealand) between 1980 and 1995 in percents.\n",
            "The line graph depicts the differnce of income between men and women in five countries (Japan, United States of America, United Kingdom,  Germany and New Zealand) between 1980 and 1995 in percents.\n",
            "\n",
            "\n",
            ".\n",
            "?\n",
            "And if one person has right to study, why the other person can get it. The biggest problem in this case is money.\n",
            "And if one person has right to study, why the other person can get it? The biggest problem in this case is money.\n",
            "\n",
            "\n",
            ".\n",
            ",\n",
            "All in all, education the most part of education should be provided for everyone. because this is our basic human right.\n",
            "All in all, education the most part of education should be provided for everyone, because this is our basic human right.\n",
            "\n",
            "\n",
            ".\n",
            "?\n",
            "What for to take in university student who do n't wish about education. But you will say: how about people that want to study a lot and to graduate the university, but they do n't have money at all.\n",
            "What for to take in university student who do n't wish about education? But you will say: how about people that want to study a lot and to graduate the university, but they do n't have money at all.\n",
            "\n",
            "\n",
            ".\n",
            "?\n",
            "But you will say: how about people that want to study a lot and to graduate the university, but they do n't have money at all. I think the decision is finding a way for it.\n",
            "But you will say: how about people that want to study a lot and to graduate the university, but they do n't have money at all? I think the decision is finding a way for it.\n",
            "\n",
            "\n",
            ". Because\n",
            "because\n",
            "37 On the one hand, there will be difficult for government to pay for all schools and universities in their countries. Because if the education is free it must be paid from the government.\n",
            "37 On the one hand, there will be difficult for government to pay for all schools and universities in their countries because if the education is free it must be paid from the government.\n",
            "\n",
            "\n",
            ". Because,\n",
            ", because\n",
            "First, a free education have to be given those which passed their exams in school well. Because, some of students do not want to study at all, and for governments it will be just a waste of money that can be treated for something usefull.\n",
            "First, a free education have to be given those which passed their exams in school well, because some of students do not want to study at all, and for governments it will be just a waste of money that can be treated for something usefull.\n",
            "\n",
            "\n",
            ". But\n",
            ",\n",
            "Although there are some people who justify education being expensive explaining that it is done so the schools, colleges and universities are not overcrowded and there will be enough space for the ones who are willing to study. But I assume this is completely wrong and this is a duty of government to establish favourable conditions for everyone and to provide enough places and opportunities for people to be educated.\n",
            "Although there are some people who justify education being expensive explaining that it is done so the schools, colleges and universities are not overcrowded and there will be enough space for the ones who are willing to study, I assume this is completely wrong and this is a duty of government to establish favourable conditions for everyone and to provide enough places and opportunities for people to be educated.\n",
            "\n",
            "\n",
            ". Gradual\n",
            "gradual\n",
            "Japan is also a bit different from the other countries, since their decrees has remained. Gradual throughout the whole given period, as other countries had a slightly more slow change after 1990 almost becoming stabilized.\n",
            "Japan is also a bit different from the other countries, since their decrees has remained gradual throughout the whole given period, as other countries had a slightly more slow change after 1990 almost becoming stabilized.\n",
            "\n",
            "\n",
            ". i\n",
            ", I\n",
            "In conclusion. i want to say that in today's world of equar rights, everyone should receive education.\n",
            "In conclusion, I want to say that in today's world of equar rights, everyone should receive education.\n",
            "\n",
            "\n",
            ". Their\n",
            "э=-@ their\n",
            "One the other hand, there are some basic thing that every people must know. Their language, bare phisics low, math, cultural features and more.\n",
            "One the other hand, there are some basic thing that every people must know э=-@ their language, bare phisics low, math, cultural features and more.\n",
            "\n",
            "\n",
            "; he\n",
            ". He\n",
            "For instance, there was one of the most famous Russian scientists Michail Lomonosov; he was from average family. But he was very talented and the government invested in his skills.\n",
            "For instance, there was one of the most famous Russian scientists Michail Lomonosov. He was from average family. But he was very talented and the government invested in his skills.\n",
            "\n",
            "\n",
            "Also\n",
            "Also, a\n",
            "Also system of grants is usefull because poor but talanted students can get their chance and make the world only better by their more cualified job. But this system should be precise enough to not drop the value of high education.\n",
            "Also, a system of grants is usefull because poor but talanted students can get their chance and make the world only better by their more cualified job. But this system should be precise enough to not drop the value of high education.\n",
            "\n",
            "\n",
            "countries\n",
            "countries, it\n",
            "Looking at other countries can be seen that there is no high peaks of getting income. For instance at the beginning UK had around 36 percent but what happens further, it goes down fairly fast and in 1995 they had less than 25 percent.\n",
            "Looking at other countries, it can be seen that there is no high peaks of getting income. For instance at the beginning UK had around 36 percent but what happens further, it goes down fairly fast and in 1995 they had less than 25 percent.\n",
            "\n",
            "\n",
            "countries\n",
            "countries, it\n",
            "In the other countries was the opposite situation. The earning in New Zealand, the Usa, the UK and Germany stood at 20, 30, approximately 32.5 and 10 percent respectively.\n",
            "In the other countries, it was the opposite situation. The earning in New Zealand, the Usa, the UK and Germany stood at 20, 30, approximately 32.5 and 10 percent respectively.\n",
            "\n",
            "\n",
            "countries\n",
            "countries, a\n",
            "In other countries similar effect did not appeared. In 1995 Germany and New Zealand had almost the same rate between 10 and 15 percent which is minimal among all countries.\n",
            "In other countries, a similar effect did not appeared. In 1995 Germany and New Zealand had almost the same rate between 10 and 15 percent which is minimal among all countries.\n",
            "\n",
            "\n",
            "education\n",
            "education, a\n",
            "Without education person can not develop himself in different ways and areas. What is more, such person will never become useful to humanity due to the fact that he will not be able to leave something memorable after himself.\n",
            "Without education, a person can not develop himself in different ways and areas. What is more, such person will never become useful to humanity due to the fact that he will not be able to leave something memorable after himself.\n",
            "\n",
            "\n",
            "education\n",
            "education, a\n",
            "Secondly, without education person can not be a scientist, a researcher or doing other complicated intellectual job, because it demands from a person a knowledge background. For example, if someone has a strong predisposition to the science but do not have enough money to go to college or university- the world can loose the famous scientist and probably a lot of useful discoveries.\n",
            "Secondly, without education, a person can not be a scientist, a researcher or doing other complicated intellectual job, because it demands from a person a knowledge background. For example, if someone has a strong predisposition to the science but do not have enough money to go to college or university- the world can loose the famous scientist and probably a lot of useful discoveries.\n",
            "\n",
            "\n",
            "education\n",
            "education, a\n",
            "Without education person can not get a job and therefor earn money for a living. In the other hand, some people think defferentaly and can not agree with my opinion.\n",
            "Without education, a person can not get a job and therefor earn money for a living. In the other hand, some people think defferentaly and can not agree with my opinion.\n",
            "\n",
            "\n",
            "essay\n",
            "essay, I\n",
            "So in this essay will be demonstrates both points of view. One of the positions is that education is a basic human right.\n",
            "So in this essay, I will be demonstrates both points of view. One of the positions is that education is a basic human right.\n",
            "\n",
            "\n",
            "example\n",
            "example, a\n",
            "For example skater or surfer learns everything from experience. Moreover, I do not agree with most people's opinion that education is not expensive.\n",
            "For example, a skater or surfer learns everything from experience. Moreover, I do not agree with most people's opinion that education is not expensive.\n",
            "\n",
            "\n",
            "for getting\n",
            ", to get\n",
            "Firstly, it is impossible to exept liers and kind of bad behaviour, for example, giving money to teachers, tutors or professors for getting better place in better school or university and in the future for getting the highest marks. The problem refers to luxurity again, because the man who has more money will be more sucsessfull.\n",
            "Firstly, it is impossible to exept liers and kind of bad behaviour, for example, giving money to teachers, tutors or professors for getting better place in better school or university and in the future, to get the highest marks. The problem refers to luxurity again, because the man who has more money will be more sucsessfull.\n",
            "\n",
            "\n",
            "foremost\n",
            "foremost, a\n",
            "First and foremost well-working and permamently developing society is impossible to make without educated people, who will have all the skills and knowledge to manage the development. That means that without widely spread educational system the government will not be able to raise such people- the good members of the society.\n",
            "First and foremost, a well-working and permamently developing society is impossible to make without educated people, who will have all the skills and knowledge to manage the development. That means that without widely spread educational system the government will not be able to raise such people- the good members of the society.\n",
            "\n",
            "\n",
            "free\n",
            "free, but\n",
            "In my opinion, education should be free not for everyone. There are a lot of advantages for paid education for part of people.\n",
            "In my opinion, education should be free, but not for everyone. There are a lot of advantages for paid education for part of people.\n",
            "\n",
            "\n",
            "free\n",
            "free, a\n",
            "If every university is free person can choose despite on his or her earnings. It will result in increasing the number of educated people.\n",
            "If every university is free, a person can choose despite on his or her earnings. It will result in increasing the number of educated people.\n",
            "\n",
            "\n",
            "free\n",
            "free, a\n",
            "So, if education is free person will try himself in many professions and find the best for him. Overall, people will be more free in choosing unversaty and it will make education more producti.e.\n",
            "So, if education is free, a person will try himself in many professions and find the best for him. Overall, people will be more free in choosing unversaty and it will make education more producti.e.\n",
            "\n",
            "\n",
            "knowledge\n",
            "knowledge, a\n",
            "Studying some sphere of knowledge human can understand that it is not for him or her and try to do his or her best in the other sphere. For example, my friend Nikita went to the college after finishing the school.\n",
            "Studying some sphere of knowledge, a human can understand that it is not for him or her and try to do his or her best in the other sphere. For example, my friend Nikita went to the college after finishing the school.\n",
            "\n",
            "\n",
            "new zealand an uk.\n",
            "New Zealand and the UK,\n",
            "the impressive results among others show countries like new zealand an uk. where the diffetnece in earnig between men and women is lower than other developed countries.\n",
            "the impressive results among others show countries like New Zealand and the UK, where the diffetnece in earnig between men and women is lower than other developed countries.\n",
            "\n",
            "\n",
            "study\n",
            "study. It\n",
            "From my humble opinion, every person should have the same abilities to study only depends on who they are want to be in the future, and hot how much money their family ready to spend it on. Moreover, i truly believe, that free education gives opportunity to people, who can not afford paying for studying, but who is truly in getting knowledge and dreaming to become the best.\n",
            "From my humble opinion, every person should have the same abilities to study. It only depends on who they are want to be in the future, and hot how much money their family ready to spend it on. Moreover, i truly believe, that free education gives opportunity to people, who can not afford paying for studying, but who is truly in getting knowledge and dreaming to become the best.\n",
            "\n",
            "\n",
            "think,\n",
            "thinks\n",
            "One part think, that education should be provided for small elite gropus, but another one, that include me, think, that it must be provided for everyone. Personally I am convieced, that the earlier steps of education is essential right for every person.\n",
            "One part thinks that education should be provided for small elite gropus, but another one, that include me, think, that it must be provided for everyone. Personally I am convieced, that the earlier steps of education is essential right for every person.\n",
            "\n",
            "\n",
            "think,\n",
            "thinks\n",
            "One part think, that education should be provided for small elite gropus, but another one, that include me, think, that it must be provided for everyone. Personally I am convieced, that the earlier steps of education is essential right for every person.\n",
            "One part think, that education should be provided for small elite gropus, but another one, that include me, thinks that it must be provided for everyone. Personally I am convieced, that the earlier steps of education is essential right for every person.\n",
            "\n",
            "\n",
            "UK\n",
            "UK, it\n",
            "In Japan, USA and UK did not exceeded 8%.\n",
            "In Japan, USA and UK, it did not exceeded 8%.\n",
            "\n",
            "\n",
            "up\n",
            "up, a\n",
            "To sum up common trend should be observed. This trend is negative.\n",
            "To sum up, a common trend should be observed. This trend is negative.\n",
            "\n",
            "\n",
            "USA\n",
            "USA, and\n",
            "For example, this difference reduced in a quater form 40 per cent to 30, in the USA plummeted, reducing in a half, starting from around 40 per cent in 1980 and achieving 20 per cent in 1995. In Germany this figure grew from 10 per cent in 1990 to around 13 per cent in 1995.\n",
            "For example, this difference reduced in a quater form 40 per cent to 30, in the USA, and plummeted, reducing in a half, starting from around 40 per cent in 1980 and achieving 20 per cent in 1995. In Germany this figure grew from 10 per cent in 1990 to around 13 per cent in 1995.\n",
            "\n",
            "\n",
            "work\n",
            "work, a\n",
            "Even for manual work person needs to sign some kind of contract for the work simply to be paid, so if he is not educated enough to understand the aspects of the contract that might lead to some bad consequences. Education is also important for our brain development too because especially in childhood people are able to memorize enormously huge amounts of information and can be taught really fast.\n",
            "Even for manual work, a person needs to sign some kind of contract for the work simply to be paid, so if he is not educated enough to understand the aspects of the contract that might lead to some bad consequences. Education is also important for our brain development too because especially in childhood people are able to memorize enormously huge amounts of information and can be taught really fast.\n",
            "\n",
            "\n",
            "world\n",
            "world, a\n",
            "In modern world human should be able to do some basic tasks and without these skills he will not be able to survive. Only education can give these skills and thus to help to grow up self-independent person.\n",
            "In modern world, a human should be able to do some basic tasks and without these skills he will not be able to survive. Only education can give these skills and thus to help to grow up self-independent person.\n",
            "\n",
            "\n",
            "world\n",
            "world, and\n",
            "The technologies and innovations rule the world so does the intellectual industries such as IT-companies, medicine with biotechnologies and so on. All of them demands developed and high-qualified workers who are ready for hard challenges so the education is a way for a better work.\n",
            "The technologies and innovations rule the world, and so does the intellectual industries such as IT-companies, medicine with biotechnologies and so on. All of them demands developed and high-qualified workers who are ready for hard challenges so the education is a way for a better work.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wj7-fNn6guV4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}